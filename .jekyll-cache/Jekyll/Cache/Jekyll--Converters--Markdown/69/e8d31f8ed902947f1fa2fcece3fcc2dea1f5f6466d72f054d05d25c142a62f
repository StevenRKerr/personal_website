I"<p>29 Aug 2023</p>

<h3>  What is linear regression? </h3>

<p>Itâ€™s one of the first major topics you learn about when taking statistics class. In essence, it allows you to model the mean of a response variable \( y_i \)  as depending on some explanatory variables \( X_i \), where the subscript \( i = 1â€¦n \) labels different observations. More concretely,</p>

\[y = X \beta + \epsilon, \tag{1}\]

<p>where \( y \) is an \( n \)-dimensional vector whose elements are \( y_i \), and \( X \) is an \( n \times k \) matrix whose rows are \( X_i \), and \( \beta \) is a \( k \)-dimensional vector of parameters to be estimated. If we assume that</p>

\[\mathbb{E}[\epsilon \;|\; X] = 0, \tag{2}\]

<p>then</p>

\[\mathbb{E}[y \;|\; X] = X \beta, \tag{3} \label{ols}\]

<p>Multiplying both sides by the <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse" target="_blank">Moore-Penrose pseudoinverse</a> of \( X \) gives</p>

\[\beta = (X^T X)^{-1} X^T \; \mathbb{E}[y \;|\; X]. \tag{4}\]

<p>Finally, replacing \( X^T\; \mathbb{E}[y \; | \; X] \) with the estimator \( X^T \; y \) gives the famous ordinary least squares formula</p>

\[\beta = (X^T X)^{-1} X^T y. \tag{5}\]

<p>This is the most simple and instructive derivation of this equation youâ€™ll ever say if I do say so myself. And I do, in fact, say so myself.</p>

<h3>  What if your dependant variable is restricted to some subset of the real numbers?</h3>

<p>So, armed with the above formula, you can go about modelling all sorts of stuff. The technical assumptions being made are fairly unrestrictive. There is no assumption that \( \epsilon \) is normally distributed, as I have discovered is depressingly commonly believed even amongst professional statisticians (but thatâ€™s a topic for another day).</p>

<p>Ah but what if I want to model a process where, say, the dependent variable \( y_i \) is binary taking only values \( 0 \) or \( 1\)? The conventional wisdom is that you should not use linear regression, but a different model e.g. logistic regression. Why?</p>

<h3>  Objection 1: Values out of bounds</h3>

<p>You may have noticed that in equation \( \ref{ols} \), there is nothing stopping the modelled mean from being less than \( 0 \) or greater than \( 1 \) -  \( X \) and \( \beta \) might just conspire to make it so. Clearly that cannot actually be, given that \( y_i \in \{ 0, 1 \} \). This problem cannot occur in logistic regression, because the functional dependence of \( \mathbb{E}[y \; | \; X] \) on \( X \) and \( \beta \) ensures that \( y_i \) is always in  \( {0, 1} \).</p>

<p>If you wish to read more about this, including more mathematical detail than you probably want, you should check out my recent paper <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4469869" target="_blank">here</a>.</p>

:ET