I"Í<p>30 Dec 2022</p>

<h3> What is effective altruism? </h3>

<p>It is a social movement that broadly aims to do the â€˜most goodâ€™ in the world with limited resources. The effective altruism movement advocates for making career choices and charitable donations that seek to maximise altruistic impact. Their activities include analysing effectiveness of charitable organisations, giving career advice, promoting pledges to donate a proportion of oneâ€™s income to charity, and writing academic papers on ethics. The main individuals who are credited with starting the movement are Will MacAskill and Toby Ord, both at the University of Oxford, and Australian moral philospher Peter Singer, at Princeton university.</p>

<h3> What is longtermism? </h3>

<p>Longtermism has become a signature idea within the effective altruism community. It broadly argues for greater emphasis to be placed on the long-term future in ethical decisions. The basic gist is that future humans/sentient beings who have not yet come into existence should be given the same â€˜moral weightâ€™ as those alive now. This tends to be coupled with the idea that the number of beings alive right now could be completely dwindled by the number of future humans, especially if humanity survives for millions/billions of years into the future and colonises other planets.</p>

<h3> Sounds dandy. Ok, and what is the last thing you mentioned? </h3>

<p>Pascalâ€™s mugging.</p>

<p>Letâ€™s say youâ€™re walking along the street one day and youâ€™re accosted by a mugger. However, he is a very incompetent mugger who has forgotten his weapon. So, he instead makes the following proposal: give me all the money in your wallet, and tomorrow I will give you back twice that amount of money. You, being sensible, immediately dismiss his offer - what are the chances that he would actually follow through? The mugger, sensing he is losing you, continues to increase his offer. He says that if there is even the tiniest non-zero probability that he will honour the deal, then you should accept. Eventually he wins you over by offering a bajillion dollars on the morrow sharp if you agree, and otherwise he will kick a gazillion puppies right in the face.</p>

<h3> Ok, you have a problem, don't you Steven. Why don't you tell me about it. </h3>

<p>Yeah I do have a problem. Well-intuited, me.</p>

<p>The idea that we should give the same â€˜moral weightâ€™ to future individuals that are not yet in existence as we do to individuals alive now, comes from a little mish-mash of undergraduate economics and philosophy. In economics 101, you learn that the default setting for economic models is to discount future utility. That means that we prefer a bowl of ice cream now compared to a bowl of ice cream tomorrow, or a year from now. This is expressed mathematically as follows:</p>

<p>\(U = \sum_{t=1}^{T} \beta^t u(x_t)\).</p>

<p>Here, \( x_t \) is some amount of a good received at time \( t \) (e.g. a bowl of ice cream), \( u \) is a utility function which measures the benefit that you get from said good, and \( \beta \) is a number between 0 and 1 that we call the utility discount factor. The utility discount factor captures how much we prefer benefits now compared to benefits at a later date. For example, letâ€™s say units of time are measured in days and \( \beta = 0.5 \). That would mean that two bowls of ice cream tomorrow would give you the same benefit as one bowl of ice cream today.</p>

<p>Now, the fine upstanding philospher sees this and is frankly outraged. So outraged, he almost smashes his monocle. What does this presume to say - that the you of tomorrow has only half the moral worth of todayâ€™s you? What egregious claptrap is this!</p>

<p>I jest, but only a little. But thatâ€™s basically the longtermists; they think that \( \beta \) should be equal to 1, for reasons.</p>

<p>Of course what they completely ignore is that the utility discount factor is really just a parsimonious way of capturing the fact that everything is a lottery, without actually having to model everything as a lottery, because frankly thatâ€™s a pain in the arse. The choice is not between a bowl of ice cream now or tomorrow. It is a choice between a bowl of ice cream now, and a bowl of ice cream tomorrow plus a small probability that you meet your tragic and untimely demise in an unforseeable bovine trebuchet incident before tomorrow, thus depriving you of said bowl of ice cream.</p>

<p>Ok I jest again, maybe a little more this time.</p>

<p>The point is there is a chance you may not be around to get that bowl of ice cream tomorrow, or you might lose your taste buds, or whatever else. Of course thereâ€™s also a chance that a magical fairy will wave her wand and make it so that tomorrow you get twice the enjoyment from that bowl of ice cream that you otherwise would have. But basically, thereâ€™s a lottery of different events that could happen, and the end result of that lottery is that humans tend to prefer sure things today rather than near-sure things tomorrow, or not-so-sure things a year from now. As a quick fix, we set \( \beta \) a little less than 1.</p>

<p>It is not difficult to figure this out. Many people know it, and have pointed it out. This has not stopped the longtermists from building a utopian (read: probably dystopian) moral philosphy on it though.</p>

<p>So yeah, letâ€™s say we take the longtermist idea serious - there will be no utility discounting, no siree. On top of that, letâ€™s presume that there is a non-zero chance that there will be an arbitrarily large number of human beings with wonderful lives in the future. Does this remind you of a certain unarmed robbery of a certain historical Frenchman yet?</p>

<p>Armed with this ideology, you could come to some rather dubious conclusions, to say the least. For example, you could argue that instead of using our resources to provide food for millions of people on the brink of famine today, we should instead give that money to our well-to-do academics and intellectual superiors at Oxbridge and the Ivy league so they can do urgently-needed research on where to best expend our resources.</p>

<p>Yeah, right.</p>

<p>In fact, you could justify inflicting any amount of suffering on people who are alive today, in order to serve the interests of hypothetical future people. After all, whatâ€™s worse: the suffering of a few million people, or the suffering of <b>ONE ZILLION SQUILLION</b> future people, who letâ€™s face it are, for all ethical intents and purposes, really alive today.</p>

<p>This thinking is the product of average academic minds furiously scurrying to justify their existence. They need some new, counterintuitive take that they can write papers about and use to position themselves as the thought-leaders of a burgeoning social movement and get tenured positions.</p>

<p>Itâ€™s easy not to take this too seriously. Surely they are on the fringe.</p>

<p>Then you realise that that the effective altruism movement may have control over several billions of dollars.</p>

<p>Then you find out that a prominent effective altruist, by the name of Sam Bankman-Fried, was (allegedly!) running a cryptocurrency Ponzi scheme, which roughly amounted to stealing peopleâ€™s life savings and donating it to his preferred political party.</p>

<p>Very effective. Much altruism.</p>

<p>Really what this is is some kids with a god complex and a utopian ideology that, like so many before it, would almost certainly lead to misery on the grandest scale imaginable. The only thing they need to make that a reality is a little more power.</p>
:ET