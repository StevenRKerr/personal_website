I"Ë<p>29 Aug 2023</p>

<h3>  What is linear regression? </h3>

<p>Itâ€™s one of the first major topics you learn about when taking statistics class. In essence, it allows you to model the mean of a response variable \( y_i \) as depending on some explanatory variables \( x_i \), where the subscript \( i = 1â€¦n \) labels different observations. More concretely,</p>

\[y = X \beta + \epsilon, \tag{1}\]

<p>where \( y \) is an \( n \)-dimensional vector whose elements are \( y_i \), \( X \) is an \( n \times k \) matrix whose rows are \( x_i \), and \( \beta \) is a \( k \)-dimensional vector of parameters to be estimated. If we assume that</p>

\[\mathbb{E}[\epsilon \;|\; X] = 0, \tag{2}\]

<p>then</p>

\[\mathbb{E}[y \;|\; X] = \mathbb{E}[X \beta \; | \; X], \tag{3} \label{ols}\]

<p>Multiplying both sides by the <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse" target="_blank">Moore-Penrose pseudoinverse</a> of \( X \) gives</p>

\[\beta = \mathbb{E}[ (X^T X)^{-1} X^T y \;|\; X]. \tag{4}\]

<p>Finally, replacing the expected value with its sample counterpart gives the famous ordinary least squares formula</p>

\[\beta = (X^T X)^{-1} X^T y. \tag{5}\]

<p>This is the most simple and instructive derivation of this equation youâ€™ll ever say if I do say so myself. And I do, in fact, say so myself.</p>

<h3>  What if your dependant variable is restricted to some subset of the real numbers?</h3>

<p>So, armed with the above formula, you can go about modelling all sorts of stuff. The technical assumptions being made are fairly unrestrictive. There is no assumption that \( \epsilon \) is normally distributed, as I have discovered is depressingly commonly believed even amongst professional statisticians (but thatâ€™s a topic for another day).</p>

<p>Ah but what if I want to model a process where, say, the dependent variable \( y_i \) is binary taking only values \( 0 \) or \( 1\)? The conventional wisdom is that you should not use linear regression, but a different model e.g. logistic regression. Why?</p>

<h3>  Objection 1: Values out of bounds</h3>

<p>You may have noticed that in equation \( \ref{ols} \), there is nothing stopping the modelled mean from being less than \( 0 \) or greater than \( 1 \) -  \( X \) and \( \beta \) might just conspire to make it so. Clearly that cannot actually be, given that \( y_i \in \{ 0, 1 \} \). This problem cannot occur in logistic regression, because the functional dependence of \( \mathbb{E}[y \; | \; X] \) on \( X \) and \( \beta \) ensures that \( y_i \) is always in  \( \{0, 1\} \).</p>

<p>To which I respond: if your model predicts a mean that is impossible by definition, then either your model specification is bad, or your sample size is too small. If you didnâ€™t have either of those issues, then linear regression would be just dandy. That is, itâ€™s not necessarily the estimation method thatâ€™s at fault, itâ€™s you.</p>

<p>In fact, you might even consider this an asset - out of bounds values is a clear diagnostic tool you can use to detect issues with model specification and/or sample size.</p>

<h3>  Objection 2: Heteroscedasticity </h3>

<p>So usually we not only want to estimate \( \beta \), but also its variance. We want to have an idea of what level of confidence we can reasonably have in our estimates. The most straightforward way of doing this assumes that</p>

\[\mathrm{Var}[\epsilon_i \; \| \; X] = \sigma^2 \; \forall i. \tag{6}\]

<p>This assumption goes under the name â€˜homoscedasticityâ€™, and amounts to the variance of the residual being independent of \( X \). It turns out that if you fit the model using linear regression, this assumption cannot possible be satisifed. The reason is that you are modelling a Bernoulli variable with probability of \( 1 \) given by</p>

\[p_i = \mathbb{E}[y_i \; \| \; X_i]. \tag{7}\]

<p>The variance is \( p_i(1-p_i) \), which in general will depend on \( X \), thus violating the homoscedasticity assumption.</p>

<p>Fortunately, there is a simple estimator for the variance of \( \beta \) that is â€˜robustâ€™ to heteroscedasticity, that addresses this issue.</p>

<h3>  What's the real reason you're not using linear regression? </h3>

<p>I am of course, not the first person to point any of this out. Otter Hellevik put forward the case nicely in this <a href="https://link.springer.com/article/10.1007/s11135-007-9077-3" target="_blank">paper</a> from 2009, and others even before him. There are also a few advantages that linear regression has over e.g. logistic regression. Interpretability for one - the coefficient in a linear regression will tell you the number of percentage points your risk for the outcome goes up if you are in a given risk group, whereas the coefficient in logistic regression will tell you an odds ratio, which is a lot less intuituive.</p>

<p>Given all this, why does it remain more or less unthinkable in many fields for reserachers to use linear regression when the dependant variable is limited to a discrete subset of the reals?</p>

<p>I think one reason is that itâ€™s a â€˜zero-thoughtâ€™ method. If you use linear regression when the dependant variable is binary, it just gives one more way that your modelling can definitively be said to have failed. No such thing will happen if you fit a logistic regression.</p>

<p>It probably also has something to do with the vagaries of history. Maximum likelihood methods are newer than linear regression, and were all the rage at the time. Plus computers have gotten a lot more powerful in recent decades, and often there is very little difference to the end user in fitting these models despite the fact that maximum likelihood models are generally more demanding.</p>

<p>I should also mention that similar arguments work for Poisson regression. There you are just modelling a rate, which is the expectation of a discrete random variable divided by time - something that ordinary least squares is also perfectly suited for.</p>

<p>One reason this may again become quite relevant, certainly in health data science, is if <a href="https://en.wikipedia.org/wiki/Secure_multi-party_computation" target="_blank">secure multi-party computation</a> are adopted and become commonplace. These techniques allow a function to be calculated across multiple private datasets, with zero information sharing. However, nothing is free, and this comes with a significant additional burden in computational complexity. That might make linear regression start to look very attractive again compared to methds that require iterative optimisation methods.</p>

:ET