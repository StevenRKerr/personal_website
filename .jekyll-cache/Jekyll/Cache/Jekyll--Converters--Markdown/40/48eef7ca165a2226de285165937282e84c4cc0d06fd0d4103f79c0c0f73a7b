I"j<p>29 Aug 2023</p>

<h3>  What is linear regression? </h3>

<p>Itâ€™s one of the first major topics you learn about when taking statistics class. In essence, it allows you to model the mean of a response variable \( y_i \)  as depending on some explanatory variables \( X_i \), where the subscript \( i = 1â€¦n \) labels different observations. More concretely,</p>

\[y = X \beta + \epsilon, \tag{1}\]

<p>where \( y \) is an \( n \)-dimensional vector whose elements are \( y_i \), and \( X \) is an \( n \times k \) matrix whose rows are \( X_i \), and \( \beta \) is a \( k \)-dimensional vector of parameters to be estimated. If we assume that</p>

\[\mathbb{E}[\epsilon \;|\; X] = 0, \tag{2}\]

<p>then</p>

\[\mathbb{E}[y \;|\; X] = X \beta, \tag{3} \label{ols}\]

<p>Multiplying both sides by the <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse" target="_blank">Moore-Penrose pseudoinverse</a> of \( X \) gives</p>

\[\beta = (X^T X)^{-1} X^T \; \mathbb{E}[y \;|\; X]. \tag{4}\]

<p>Finally, replacing \( X^T\; \mathbb{E}[y \; | \; X] \) with the estimator \( X^T \; y \) gives the famous ordinary least squares formula</p>

\[\beta = (X^T X)^{-1} X^T y. \tag{5}\]

<p>This is the most simple and instructive derivation of this equation youâ€™ll ever say if I do say so myself. And I do, in fact, say so myself.</p>

<h3>  What if your dependant variable is restricted to some subset of the real numbers?</h3>

<p>So, armed with the above formula, you can go about modelling all sorts of stuff. The technical assumptions being made are fairly unrestrictive. There is no assumption that \( \epsilon \) is normally distributed, as I have discovered is depressingly commonly believed even amongst professional statisticians (but thatâ€™s a topic for another day).</p>

<p>Ah but what if I want to model a process where, say, the dependent variable \( y_i \) is binary taking only values \( 0 \) or \( 1\)? The conventional wisdom is that you should not use linear regression, but a different model e.g. logistic regression. Why?</p>

<h3>  Objection 1: Values out of bounds</h3>

<p>You may have noticed that in equation \( \ref{ols} \), there is nothing stopping the modelled mean from being less than \( 0 \) or greater than \( 1 \) -  \( X \) and \( \beta \) might just conspire to make it so. Clearly that cannot actually be, given that \( y_i \in \{ 0, 1 \} \). This problem cannot occur in logistic regression, because the functional dependence of \( \mathbb{E}[y \; | \; X] \) on \( X \) and \( \beta \) ensures that \( y_i \) is always in  \( \{0, 1\} \).</p>

<p>To which I respond: if your model predicts a mean that is impossible by definition, then either your model specification is bad, or your sample size is too small. If you didnâ€™t have either of those issues, then linear regression would be just dandy. That is, itâ€™s not necessarily the estimation method thatâ€™s at fault, itâ€™s you.</p>

<p>In fact, you might even consider this an asset - out of bounds values is a clear diagnostic tool you can use to detect issues with model specification and/or sample size.</p>

<h3>  Objection 2: Heteroskedasticity </h3>

<p>So usually we not only want to estimate \( \beta \), but also its variance. We want to have an idea of what level of confidence we can reasonably have in our estimates. The most straightforward way of doing this assumes that \( \mathrm{Var}[\epsilon_i] = \sigma^2 \; \forall i \).</p>

<p>If you wish to read more about this, including more mathematical detail than you probably want, you should check out my recent paper <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4469869" target="_blank">here</a>.</p>

:ET