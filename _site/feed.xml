<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-07-24T11:39:14+01:00</updated><id>/feed.xml</id><entry><title type="html">The solution to everything</title><link href="/2022/07/22/The-solution-to-everything.html" rel="alternate" type="text/html" title="The solution to everything" /><published>2022-07-22T00:00:00+01:00</published><updated>2022-07-22T00:00:00+01:00</updated><id>/2022/07/22/The%20solution%20to%20everything</id><content type="html" xml:base="/2022/07/22/The-solution-to-everything.html"><![CDATA[<p>22 Jul 2022</p>

<p>What if I told you there was a well-known, 100-year old theory of gravity that reproduces all empirical successes of Einstein’s general relativity, and may solve a who’s who of major oustanding problems in theoretical physics?</p>

<p>Its called <a href="https://en.wikipedia.org/wiki/Einstein%E2%80%93Cartan_theory" target="_blank"> Einstein-Cartan theory</a>, or sometimes Einstein-Cartan-Sciama-Kibble (ECSK) theory. By the way, the eponymous Kibble is my “physics grandfather” - my PhD supervisor’s PhD supervisor.</p>

<p>ECSK theory is a minor generalisation of Einstein’s general relativity that does not require the <a href="https://en.wikipedia.org/wiki/Torsion_tensor" target="_blank"> torsion </a> to be zero. It turns out that the torsion tensor does not have any propagating degrees of freedom, with the consequence that the only place that ECSK theory and general relavitity differ is at extremely high matter densities.</p>

<p>First, let’s do a quick tour of major outstanding problems that ECSK theory might solve.</p>

<h3> Gravitational singularities </h3>

<p>In a nutshell, Einstein’s general relativity posits that space and time are curved, and that their curvature is determined by the distribution of energy/matter within the spacetime. So, if you plonk a mass down somewhere, it causes space and time to bend in way that we experience as gravity.</p>

<p>One of the problems with the theory is that if the mass/energy gets sufficiently dense, the curvature of spacetime becomes infinite. This is known as a gravitational singularity, and in general relativity every black hole comes with one. Many physicists consider it to be an unsatisfactory state of affairs that the theory predicts some physical quantities are infinite, although currently this does not conflict with any experimental evidence because the singularity is inevitably ‘hidden’ behind an <a href="https://en.wikipedia.org/wiki/Event_horizon" target="_blank"> event horizon</a>, which stops us from knowing much about what happens on the other side.</p>

<p>Physicist Nikodem Poplawski has suggested that <a href="https://arxiv.org/abs/0910.1181" target="_blank">gravitational singularities may be avoided in ECSK theory</a>, due to torsion manifesting as a repulsive force that prevents fermions collapsing to infinite density. The big bang is then replaced by a big bounce, and our universe is the other side of a black hole in another universe.</p>

<h3> The horizon problem </h3>

<p>The universe appears to be relatively samey everywhere we look. In particular, the temperature of the <a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background" target="_blank"> cosmic background radiation</a>, a remanant from the early universe, is extremely uniform. Turns out that’s a big problem.</p>

<p>The issue is that when the cosmic background radiation was formed, the universe was about 300,000 years old, and it is possible to show that cosmic background radiation separated by an angle of incidence on Earth of any more than about 2 degrees cannot have been been in causal contact at that time. In other words, there is no reason to think that the radiation coming from any two such patches of the sky were in thermal equilibrium at the time they were produced, and they have not been in thermal contact since - so why is it the same temperature?</p>

<p>To put it more simply, let’s say I have a glass of hot water and a glass of cold water. If they are not in thermal contact with each other, or their surroundings, they will remain at different temperatures. If I pour them both into a container, they rapidly come into thermal equilibrium at an intermediate temperature. Cosmic background radiation from two slightly separated patches of the sky are like separate glasses of water - there’s no reason why they should have been in thermal equilibrium when they were created, and have not been in thermal contact since. So why is it that in every direction we look, the temperature is the same?</p>

<p>In the ECSK theory, the torsion generates <a href="https://en.wikipedia.org/wiki/Inflation_(cosmology)" target="_blank"> cosmic inflation</a>, which<a href="https://arxiv.org/abs/1410.3881" target="_blank"> causes the early universe to be homogeneous</a>, leading to uniformity in the temperature of the cosmic background radiation.</p>

<h3> The black hole information paradox </h3>

<p>In quantum mechanics/quantum field theory, it is taken as an axiom that time evolution is <a href="https://en.wikipedia.org/wiki/Unitarity_(physics)" target="_blank"> unitary</a>. What this means simply is that the value of the wavefunction at one time determines a unique value at all times. The main reason this is important is because it is a sufficient condition for ‘probability to be conserved’. That is, if the probability of finding the system in some state \( \ket{ \psi(0)} \) at time \(0\) is \(p\), and it evolves to \( \ket{ \psi(t)} \) at time \(t\) under the Hamiltonian, then the probability of \( \ket{ \psi(t)} \) is also \(p\).</p>

<p>Note however, as Roger Penrose reminds us, that there are standard interpretations of quantum mechanics in which unitarity is routinely violated (by measurement), and this does not appear to pose any insurmountable difficulties.</p>

<p>In the 1970s, Stephen Hawking found that combining quantum field theory and general relativity could result in a loss of unitarity. In particular, black holes emit <a href="https://en.wikipedia.org/wiki/Hawking_radiation" target="_blank"> Hawking radiation</a> which should eventually cause black holes to evaporate. The resulting thermal radiation would not contain any information about the initial state of the black hole. In other words, multiple different initial states could end in the same final state, and therefore a value for the wavefunction at a later time does not uniquely determine its value at all other times. Information has somehow been destroyed.</p>

<p>Thus it appears that physicists have to give up at least one of their cherished principles. Either time evolution in the Schrodinger equation is not unitary, or general relativity is wrong, or some other aspect of quantum field theory is wrong.</p>

<p>It has been suggested the ECSK theory could resolve the black hole information paradox by avoiding the gravitational singularity in black holes. Then every black hole is actually a one-way membrane leading to a new universe that retains all the information from the initial state.</p>

<h3> Matter-antimatter asymmetry </h3>

<p>In the laws of physics as they are best known today, there is nothing that favours the production of matter over antimatter. On the other hand, the <a href="https://en.wikipedia.org/wiki/Observable_universe" target="_blank"> observable universe</a> is heavily dominated by matter, which immediately raises the question “Why?”.</p>

<p>ECSK theory coupled to fermions generates a term that is cubic in the spinors and thus not invariant under charge conjugation, giving a <a href="https://arxiv.org/abs/1101.4012" target="_blank"> possible explanation for matter-antimatter asymmetry</a>.</p>

<h3> Ultraviolet divergences </h3>

<p>When you naively try to calculate physically measurable quantities in interacting quantum field theories, you tend to get an infinite answer. That’s a problem.</p>

<p>The basic issue is that fields are operator-valued distributions, for which multiplication is not well-defined outside of some trivial cases. If you plough ahead anyway pretending this problem doesn’t exist, aforementioned infinities rear their ugly head. Garbage in, garbage out, as they say.</p>

<p>To deal with this, physicsts play a regularisation game that involves artificially adding terms to the theory to cancel out the infinities, in a process that Richad Feynman famously described as ‘dippy’. This is not mathematically kosher, but it does allow us to extract sensible predictions from the theory, so the technical concerns tend to get brushed under the carpet to a degree.</p>

<p>The infinities that occur are broadly of two kinds: ultraviolet (high energy/short distance) and infrared (low energy/large distance). The infrared divergences are typically regarded as less troubling, and one reason is because they go away if you think the universe is finite because that introduces a maximum possible length scale. The ultravolet divergences tend to cause more consternation because there is less sympathy for the idea of a universal minimum length scale.</p>

<p>It has been suggested by Nikodem Poplawski that Einstein-Cartan theory <a href="https://arxiv.org/abs/1712.09997" target="_blank"> resolves the ultraviolet divergences</a> because torsion causes momentum operators to be non-commutative.</p>

<h3> Ok, if ECSK theory is so awesome, why isn't it more popular? </h3>

<p>Great question, me.</p>

<p>So I should first say that the solutions that ECSK theory proposes to the above problems are mostly only conjecture at the moment. We have suggestive evidence only.</p>

<p>On the other hand, it is a relatively modest generalisation of Einstein’s general relativity that may solve just about every major problem going in physics today. Why isn’t it famous?</p>

<p>One reason is that ECSK theory is more complicated than Einstein’s general relativity. Introducing torsion makes it significantly more difficult to do calculations and extract predictions from the theory. Because there is no currently feasible experiment whose result would distinguish between Einstein’s general relativity and ECSK theory, people have tended to go for the simpler theory.</p>

<p>That said, the additional theoretical complications that ECSK theory brings does not, to me, seem to justify how little attention it receives. It introduces non-linearity in the Dirac equation, but since when did a little non-linearity scare us physicists? General relativity and Quantum Chromodynamics are both non-linear and we’re quite happy with those. On the other hand, there is an entire industry dedicated to the extremely speculative idea of string theory, which is mind-bogglingly complicated and it is not yet clear whether it solves anything despite decades of dedicated work by many very smart people. Surely ECSK theory could use a little love too?</p>

<p>Ultimately I think the disparity has its explanation in historical coincidence and herding behaviour. Which I understand, but I also think we need to #GiveECSKTheoryAChance.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[22 Jul 2022]]></summary></entry><entry><title type="html">On one-use code</title><link href="/2022/06/05/On-one-use-code.html" rel="alternate" type="text/html" title="On one-use code" /><published>2022-06-05T00:00:00+01:00</published><updated>2022-06-05T00:00:00+01:00</updated><id>/2022/06/05/On%20one%20use%20code</id><content type="html" xml:base="/2022/06/05/On-one-use-code.html"><![CDATA[<p>05 Jun 2022</p>

<p>Academics who code are, I think, somewhat known for their less than stellar creations. I have certainly written code that I would prefer didn’t see the light of day. A recent example that attracted a great deal of attention is <a href="https://en.wikipedia.org/wiki/Neil_Ferguson_(epidemiologist)" target="_blank"> Professor Neil Ferguson’s</a> <a href="https://en.wikipedia.org/wiki/CovidSim" target="_blank"> CovidSim</a>. As the name suggests, it is a simulator of COVID-19 transmission that works by creating artificial agents representing people and environments that they interact with, in much the same way as the SimCity series of games but without the funky graphics. It was the basis of a <a href="https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf" target="_blank"> paper</a> that is credited with fundamentally altering the course of the UK’s COVID-19 policy. The paper predicted that even under the optimal ‘mitigation’ strategy that was considered, the peak surge capacity of ICU beds in the UK would be exceeded 8-times over due to the pandemic. As of the date of writing, it has 3,910 citations.</p>

<p>In May 2020, The code behind the simulation was released to GitHub, which is a ubiquitously-used online tool for software development. Before it was released to the public, it apparently consisted of a single 15,000 line file written in C. Having one file of source code that long is already a cardinal sin in coding, and there were many other failures to live up to standard software development practices.</p>

<h3> Ok, but it worked, didn't it? </h3>

<p>An independent research group was able to reproduce the published results of CovidSim by running it themselves. So, while CovidSim may have been less than desirable from a coding point of view, it did its job as intended. Does it really matter that it wasn’t that pretty?</p>

<p>I have a lot of sympathy for the team that worked on this code. Academic coding often doesn’t live up to industry software development standards, for a multitude of reasons. Code that is used to carry out analysis for research is typically ‘one-use’, or close to it. It is not, for example, google.com, which is used by god knows how many millions of people every day and has a large team of people continuously maintaining it. Code written for research is typically intended to be used by a small number of people who authored it, to obtain a specific set of outputs just once.</p>

<p>In addition to that, the PhD students, postdocs and other research staff who typically write the code for such projects are usually themselves not trained in industry standard software development pratices. Many of them will come from disciplines that aren’t primarily coding-centred, and will have to pick it up along the way with little or no oversight. That’s a tough position to be in.</p>

<p>Third, academia is often a race to publish results, that can end up being a very much winner-take-all proposition. Release your paper a week too late and posterity will not look kindly upon you. These are not circumstances that are conducive to producing nice code.</p>

<p>Lastly, an adage comes to mind that goes something like this: “Feel free to break the rules once you know why they exist”. It captures the idea that once you are at a sufficiently high level in a given skill, a lot of what you do consists of knowing when exactly the rules can be bent/broken. Great chess grandmasters, for example, often play with flagrant disregard for well-established principles of the game. They have been at it for long enough that they can often get a competitive edge by going beyond the rules, in a way that is informed by years of study and expertise. Likewise, there are circumstances where it is ok to write fairly ‘terrible’ code, that breaks all the rules - sometimes you just need a result quickly, and you can focus on cleaning it up/optimising later, or not at all. Why, for example, spend hours beautifying a piece of code when you have a deadline looming that depends crucially on the output of said code? Do your due diligence in making sure it is correct, and hit enter.</p>

<p>That said, it is not exactly confidence-inspiring that this simulation that changed the course of a nation was lacking in robustness. One reason we like pretty code is that it tends to minimise the chances of an error. The stakes could not have been higher in this case - an error would have had massive consequences. Also, the fact that the code has been around in one form or another since 2005, and has been adapted repeatedly for modelling various epidemics, makes me a little less sympathetic. That is not exactly what one would call one-use code.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[05 Jun 2022]]></summary></entry><entry><title type="html">The F-twist</title><link href="/2022/05/07/The-F-twist.html" rel="alternate" type="text/html" title="The F-twist" /><published>2022-05-07T00:00:00+01:00</published><updated>2022-05-07T00:00:00+01:00</updated><id>/2022/05/07/The%20F-twist</id><content type="html" xml:base="/2022/05/07/The-F-twist.html"><![CDATA[<p>07 May 2022</p>

<h3> What is the F-twist? </h3>

<p>Milton Friedman was a Nobel prize-winning economist whose ideas became deeply influential both inside and outside academia.
He broadly advocated for free markets with minimal government intervention. For example, he opposed the military draft,
 minimum wage laws and <a href="https://en.wikipedia.org/wiki/Occupational_licensing" target="_blank"> occupational licensing</a>,
and supported <a href="https://en.wikipedia.org/wiki/School_voucher" target="_blank"> school vouchers</a>
and legalisation of drugs and prostitution. He was an advisor to Ronald Reagan
and Margaret Thatcher, and he is widely thought of as the intellectual father of the ideology of 
<a href="https://en.wikipedia.org/wiki/Neoliberalism" target="_blank"> neoliberalism</a>.</p>

<p>In his 1953 book Essays in positive economics, he introduced a principle that has later become known as the “F-twist”, famously 
embodied in the following sentence:</p>

<blockquote>
Truly important and significant hypotheses will be found to have "assumptions" that are wildly inaccurate descriptive 
representations of reality, and, in general, the more significant the theory, the more unrealistic the assumptions (in this sense).
</blockquote>

<p>The name “F-twist” was dubbed by Friedman’s fellow Nobel economist Paul Samuelson, who reportedly chose not to name it after Friedman
directly out of “courtesy”.</p>

<p>It’s a pretty outrageous idea, seemingly suggesting that we should limit our search for new theories to those whose axioms are
“wildly inaccurate”. Nonetheless, it has been widely adopted in mainstream academic economics. That’s a fascinating state of affairs
to me.</p>

<h3> Doesn't it contradict pretty much everything we know about science and the scientific method? </h3>

<p>In a word: yes.</p>

<p>The core of the scientific method is</p>

<ol>
  <li>Generate a hypothesis</li>
  <li>Figure out what observable consequences of the hypothesis are</li>
  <li>Do experiments to see if those observable consequences are in accordance with reality</li>
</ol>

<p>There’s a few thorny issues to take care of there. But the underlying philosophy is that the proper aim of science is to understand/predict
the behaviour of the universe and its various constitutents, and the ultimate arbiter of truth is reality itself - all ideas
must be subject to rigorous empirical testing.</p>

<p>As an example, consider Einstein’s special theory of relativity. It starts with two axioms:</p>

<ol>
  <li>The laws of physics are the same in all inertial frames of reference</li>
  <li>The speed of light in a vacuum is that same for all observers</li>
</ol>

<p>Starting from just this, one can figure out (if you’re a genius) that measurements of length and time are “subjective” - two 
observers with clocks and rulers can disagree on the length of an object, or the number of seconds that elapses between
two events, depending on their state of motion. In particular, an observer travelling at high speed will experience time
passing more slowly, and lengths as shorter, than a stationary observer.</p>

<p>One of the great strengths of this theory is that it made clear, novel predictions, and the axioms can be directly subjected 
to experimental test. As it happened, it passed with flying colours.</p>

<p>Of note here is that the axioms of a theory are trivially also predictions of that theory, that should be tested against reality. 
There is nothing special about the axioms as opposed to observable consequences that are derived from those axioms; they all 
must be scrutinised to determine if they are in concordance with reality.</p>

<p>The extraordinary thing about the F-twist is that it completely upends this bedrock of science. It arbitrarily asks us to give the axioms of 
a theory an easy ride. In fact, further; it asks us to look for axioms that are wildly inconsistent with observed reality, for
this is surely where all the most fruitful and significant ideas lie. If taken seriously, it would revolutionise the 
way we do science. It’s not often in history that paradigm shifts like that occur.</p>

<p>Of course, it needn’t be a revolution that’s actually any good. Imagine physicists and chemists and biologists rejecting new
ideas out of hand because they are simply in excesively good agreement with reality. Everyone knows those theories are a dead-end!</p>

<p>I have to call out the F-twist for what it is: nonsense.</p>

<h3> Oh yeah, if it'so obviously wrong, then why has it been bought wholesale by academic economists? </h3>

<p>Oh that’s easy. It’s because they need to rationalise what they spend their professsional lives doing.</p>

<p>Economists in general are trying to answer extraordinarily complex questions. Questions so difficult that they may be
permanently beyond the ability of humans to answer. The people who end up devoting their professional lives to this pursuit
are inevitably going to be the ones who can convince themselves, by hook or by crook, that what they are doing is useful and important.
 The people who don’t do that drop out of the profession relatively quickly.</p>

<p>What we are left with is a lot of theorising that is ultimately fairly useless. The metric of success is decoupled from 
the normal scientific standards of standing up to empirical scrutiny, and instead focuses on how much other equally 
useless research it inspires/influences. Friedman was quite expicit about this in his essays in positive economics; 
theories are to be judged according to fruitfulness in the precision and scope of their predictions and <i>ability to generate 
additional research lines</i>. Theoretical economics becomes a hierarchy that is predicated more upon ability to convince
one’s colleagues how pleasing one’s ideas are, than ability to understand and predict economic systems.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[07 May 2022]]></summary></entry><entry><title type="html">Model complexity</title><link href="/2022/04/21/Model-complexity.html" rel="alternate" type="text/html" title="Model complexity" /><published>2022-04-21T00:00:00+01:00</published><updated>2022-04-21T00:00:00+01:00</updated><id>/2022/04/21/Model%20complexity</id><content type="html" xml:base="/2022/04/21/Model-complexity.html"><![CDATA[<p>21 Apr 2022</p>

<p>There are models for just about anything you can think of. Fundamental physics, economic markets, epidemics, brain function, climate, click-through rates, animal behavior, election outcomes, to name a few. There’s a universe of possibilities for ways to model any given phenomenon, and only a few small hidden oases in an otherwise barren desert. What’s a model-builder to do?</p>

<h3> How complex should my model be? </h3>

<p>So, a model should certainly not be as “complex” as the pheonomenon that it attempts to describe is, at least at face value. The whole point of a model is to take something that is too complex for our brains to process by brute force, and find some underlying structure that makes it easier for us to understand. ‘The map is not the territory’, as they say.</p>

<p>As an example, <a href="https://en.wikipedia.org/wiki/Standard_Model" target="_blank"> The Standard Model</a> of particle physics contains precisely 19 free parameters that need to be determined by experiment. In exchange for that, we get a description of all known matter, that is accurate all the way down to the subatomic scale. Quite extraordinary.</p>

<p>That still leaves plenty of rope with which to hang oneself though. In particular, I think the academy has a tendency to needlessly over-complicate. For one thing, more free parameters allows more impressive results to be obtained - at least when limiting to the data that the model is trained with. This is known as <a href="https://en.wikipedia.org/wiki/Overfitting" target="_blank"> overfitting</a>, and it’s everywhere. It gives the model-builder more levers and buttons that they can use in order to manufacture the exact results they want. Cranking up model complexity also allows academics to go down an intellectual rabbit hole where they can write lots of papers, as is necessary to advance one’s career. And finally, it allows them to obfuscate their overfitting - the model gets so complex that people mostly can’t be bothered investing weeks/months of their lives in order to figure out the exact trickery that is afoot.</p>

<h3> Model complexity should be proportional to experimental power </h3>

<p>The better the experiments we can do, the more detailed the hypotheses they can distinguish between. So for example, good physics models can be quite complex because we can often do excellent experiments that allow us to declare a clear victor amongst the competitors. We can build big machines like the <a href="https://en.wikipedia.org/wiki/Large_Hadron_Collider" target="_blank"> Large Hadron Collider</a> and smash beams of particles together in precisely controlled ways for years, and see which theory best predicts what comes out the other end.</p>

<p>By way of contrast, in e.g. economics we generally have poor experimental power. We can’t run repeated controlled experiments that involve tweaking the economies of large countries and see what the results are. This is why economic modelling should for the most part be quite humble in its aspirations. It should not, for example, have any illusions about its ability to provide adequate <a href="https://en.wikipedia.org/wiki/Microfoundations" target="_blank"> microfoundations</a> for macroeconomic models. It’s just not going to happen, at least anytime soon.</p>

<h3> Parameters required:Parameters used </h3>

<p>I have a somewhat loosely formulated idea for an initial sweep at sorting the wheat from the chaff.</p>

<p>First, look at the quantities that the model purports to accurately predict. What is the smallest number of parameters that would be required to get a prediction at least as good? For example, if its a macroeconomic model whose only accurate prediction a straight line trend for GDP, this would require precisely two parameters: an intercept and a slope.</p>

<p>Now look at the number of free parameters the model has. If it has at least two, then I suggest the model prima facie has no predictive value beyond a simple line of best fit. It hasn’t simplified anything, and the free parameters  give the model-builder enough space to actually get whatever straight line they wanted.</p>

<p>This is definitely a vague idea at the moment, but maybe it has some legs.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[21 Apr 2022]]></summary></entry><entry><title type="html">God</title><link href="/2022/03/27/God.html" rel="alternate" type="text/html" title="God" /><published>2022-03-27T00:00:00+00:00</published><updated>2022-03-27T00:00:00+00:00</updated><id>/2022/03/27/God</id><content type="html" xml:base="/2022/03/27/God.html"><![CDATA[<p>27 Mar 2022</p>

<p>I have decided to take a philosophical turn with this blog, because why the Dickens not. Philosphers love doing useless crap (just kidding, I heart you guys), so in that spirit I have converged upon the ultimate philosophical waste of time: Does god exist?</p>

<h3> Whence god? </h3>

<p>Right so as a good proto-philsopher, it’s important to start with definitions in order to elucidate the subsequent discussion. What do we mean by ‘god’?</p>

<p>I distinguish between two broad formulations.</p>

<h4> Formulation 1: The man in the sky </h4>

<p>This notion of god would have him as a <em>physical</em> being, residing in a <em>physical</em> place in the universe (in our exemplar, the sky). So basically, a bloke a lot like you or I, except holding some mysterious interest in/sway over mundane human matters.</p>

<p>Now I think we are quite justified in concluding with an <em>extremely high</em> level of confidence that this god does not exist. First of all, you’d think we would have located him by now. And don’t give me that <a href="https://en.wikipedia.org/wiki/Evidence_of_absence" target="_blank"> absence of evidence is not evidence of absence</a> claptrap - if I look extensively in the place where the thing would be if it existed and turn up nothing, then that <em>is</em> evidence that the thing ain’t there. But more than that, this hypothetical being would, according to most ideas about god, have various supernatural powers/properties that are contrary to basically everything that we know and believe about anything. If we ever found him, this lad would have <em>a lot</em> of explaining to do. The physics textbooks would have to be re-written for a start.</p>

<p>Alright so let’s move on to the next door.</p>

<h4> Formulation 2: The whatcha-ma-callit  in the thing-ma-bob </h4>

<p>Admittedly, this one is a bit more abstract. But I think it is the concept of god that most believers have, whether they acknowledge it or not. Namely; god as a mysterious, supernatural and completely intangible entity. There is no experiment that we could do, <em>not even in principle</em>, that would allow us to reasonably adjust our degree of belief in his existence one way or the other. God transcends all that pedestrian stuff.</p>

<p>At this point my reply is pretty simple: I consider this to be the definition of something that does not exist. Like, if someone tells me there’s a gnome on my shoulder, but this gnome does not interact with our universe in any intelligble way and has a presence that is theoretically undetectable, then I am pretty uninterested in talking about the gnome. That is as long as I’m on a fact-finding mission about the universe. This gnome contains no information about the universe, and so frankly can take a running jump. Just another <a href="https://en.wikipedia.org/wiki/Russell%27s_teapot" target="_blank">Russell’s teapot</a>.</p>

<p>I think this is a reasonable definition of what it means for something to not exist, and I’m willing to engage in trial by combat with anyone who thinks otherwise.</p>

<h3> Hang on, but isn't the idea of god interesting/useful? </h3>

<p>For sure. And this is quite separate from the idea of whether god exists. Humans are quite capable of entertaining/believing in all manner of fantastical, self-inconsistent and just plain wrong things.</p>

<p>As a storytelling device god is absolutely unparalleled. I think the most compelling stories that have ever been told have all in one way or another been about god. As an organising idea for human behaviour and by extension a force guiding the course of human history, you’ll struggle to find anything more significant - religious stories have had as deep an impact on humanity as you care to realise. For my money, I think that god and religion provide a sort of familial <a href="https://en.wikipedia.org/wiki/Superstructure" target="_blank">superstructure</a> to society - in Christianity it is the brotherhood of man under the paternal bond of god. I don’t think we stop having a deep yearning for a father figure as soon as we become adults ourselves. That’s our so-called god-shaped-hole, and it needs filled with something, gosh darnit. Indeed, I am semi-convinced that such superstructures are necessary/responsible for the emergence of humans from the wretched <a href="https://en.wikipedia.org/wiki/State_of_nature" target="_blank">state of nature</a>. And I have had my mind changed in a fairly major way in recent years on the merits of religion and the contributions that it has made to humanity. In short, I think it’s likely a net positive. But that’s a subject for another day.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[27 Mar 2022]]></summary></entry><entry><title type="html">The weakness of the continuum</title><link href="/2022/03/08/The-weakness-of-the-continuum.html" rel="alternate" type="text/html" title="The weakness of the continuum" /><published>2022-03-08T00:00:00+00:00</published><updated>2022-03-08T00:00:00+00:00</updated><id>/2022/03/08/The%20weakness%20of%20the%20continuum</id><content type="html" xml:base="/2022/03/08/The-weakness-of-the-continuum.html"><![CDATA[<p>08 Mar 2022</p>

<p>Ever since Newton/Leibniz invented the infinitesimal calculus in the latter half of the 1600s, it seems like the world has been enthralled by beauty and possibility of the continuum. This event marked the beginning of modern physics and mathematics, and there is virtually no topic in either of these fields that is not deeply interwoven with calculus.</p>

<p>Sometimes, though, I think they may have been a little too successful.</p>

<h3> Before we get started, kindly allow me to explain my super clever title pun </h3>

<p>It turns out that there are different sizes of infinity. So, for example, the entire set of counting numbers \( \{ 1,2,3 …\} \) is smaller than the entire set of real numbers. What this means is that you can pair up every counting number to a real number, and still have lots of real numbers left over. 
Sets that are of the same ‘size’ as the real numbers are sometimes said to have the ‘power of the continuum’ - and this is where we lay the scene of the eponymous wordplay.</p>

<h3> What's weak about the continuum? </h3>

<p>No one can doubt the real numbers are pretty and endlessly fascinating. But have we sometimes been a little too seduced/beguiled by them?</p>

<h4> Exhibit A: Quantum field theory </h4>

<p>Quantum field theory is at the heart of our current best theory of the fundamental workings of the universe, <a href="https://en.wikipedia.org/wiki/Standard_Model" target="_blank">The Standard Model</a>. However, somewhat amusingly, <a href="https://en.wikipedia.org/wiki/Wightman_axioms#Existence_of_theories_which_satisfy_the_axioms" target="_blank">we do not yet know if the standard model ‘exists’ mathematically</a>. When it comes to calculating physical quantities from the standard model, we use various types of fudges/approximations, all of which rely in one way or another on a ‘cutoff’. The cutoff is a distance scale below which, for practical purposes, we declare that we are not interested in what is physically happening - much like when we round off numbers at say, 2 decimal places, because precision beyond that is not useful for the purpose at hand. This cutoff effectively introduces some ‘discreteness’ into the theory, and that allows us to do sensible calculations.</p>

<p>However, the full theory is meant to be set in a continuous spacetime. When you try to define this mathematically, you unfortunately run into all sorts of problems. In particular, the fields are operator-valued distributions, for which multiplication is not well-defined outside of some very restricted subsets. This has the consequence that there are basically no known, interacting quantum field theories that have local degrees of freedom - this is the kind of quantum field theory that is required to describe our universe. Indeed there is a <a href="https://en.wikipedia.org/wiki/Yang%E2%80%93Mills_existence_and_mass_gap" target="_blank">million dollar Millenium prize</a> waiting for anyone who can provide an example. It has also led physicists to spend a huge amount of time and effort on very complex, speculative, and issue-ridden ideas like string theory.</p>

<p>All of this kerfuffle can be avoided if you’re just willing to take the cutoff seriously and put the theory on a discrete spacetime lattice. Everything is mathematically well-defined, and any physical quantity the theory predicts can be calculated to any desired degree of precision by adjusting the cutoff as necessary.</p>

<p>Physicists almost unanimously reject this, though. The continuum is just too pretty to be abandoned, despite all the issues it causes.</p>

<h4> Exhibit B: Financial markets </h4>

<p>Let’s say that asset prices are buffeted by mysterious, difficult-to-comprehend forces that we believe are roughly ‘independent’ from each other at different time steps. Let’s say we also believe that time in financial markets is a continuum. Then the central limit theorem implies that movements in asset prices are normally distributed at all times - it’s inescapable.</p>

<p>Of course, movements in asset prices are known to exhibit ‘fat tails’ - in reality there are far more extreme events than we would expect from models that use normal distributions. Indeed some people have at least partly attributed the 2008 financial crisis, as well as various other financial disasters, to widespread use of modelling strategies in which asset returns are normally distributed.</p>

<p>It is pretty clear that the assumptions above are wrong. An asset price does not undergo an infinite number of random kicks between 09:00:00 AM and 09:00:01 AM on a Monday morning. But the underlying mathematics sure is pretty if you assume it does.</p>

<h3> Conclusion </h3>

<p>I’m sure there are other good examples we could talk about. My point is that I think people often want so much for the maths to be pretty that they get led astray from reality.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[08 Mar 2022]]></summary></entry><entry><title type="html">Vaccines and blood clots</title><link href="/2022/02/27/Vaccines-and-blood-clots.html" rel="alternate" type="text/html" title="Vaccines and blood clots" /><published>2022-02-27T00:00:00+00:00</published><updated>2022-02-27T00:00:00+00:00</updated><id>/2022/02/27/Vaccines%20and%20blood%20clots</id><content type="html" xml:base="/2022/02/27/Vaccines-and-blood-clots.html"><![CDATA[<p>27 Feb 2022</p>

<p>On February 22nd, PLOS medicine published a <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003927" target="_blank">paper</a> on cerebral venous sinus thrombosis (CVST) following vaccination on which I was the lead author. CVST is a type of blood clot in the brain that can lead to stroke and death. In this paper, we carried out a pooled analyis of 11.6 million people in England, Scotland and Wales, comparing the rate of CVST events in individuals before and after receiving the vaccine. We found that in the 4 weeks following vaccination with Oxford-AstraZeneca, the risk of CVST events approximately doubled. We did not see any increase in risk of CVST events following Pfizer.</p>

<p>This paper was released at the same time as a <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003926" target="_blank">second paper</a> headed by William Whiteley, also at the University of Edinburgh, that studied 46 million people in England and had strikingly similar findings. It is reassuring when independent research efforts come to similar conclusions.</p>

<p>CVST is a very rare event, occurring at a rate of perhaps 3-4 per million people per year. Assuming this as a background rate, our findings would imply one additional CVST event for every 4 million doses of Oxford-AstraZeneca administered. This has to be weighed up against the benefits of vaccination. Due to concerns over safety, Oxford-AstraZeneca was suspended in those aged under 40, and was not routinely used for booster doses in the UK. The risk/benefit analysis for whom should be vaccinated, with which vaccine, and when, is extremely complicated and depends on background rates of infection, availability of vaccines, vaccine waning, characteristics of future variants and robustness of naturally-acquired immunity versus vaccine-induced immunity, among others.</p>

<p>Our paper included a small methodological novelty that allowed us to pool data across several nations without individual-level data having to be shared between them. Each country stores data in a trusted resesarch environment (TRE), which is just a highly secure server that contains anonymised data. Sharing individual-level data is forbidden, but we worked around this by carrying out a Poisson regression in which only count data is required. We are aiming to use a similar method to carry out further pooled studies in future.</p>

<p>PLOS medicine published an author interview with me, which can be found <a href="https://speakingofmedicine.plos.org/2022/02/22/plos-medicine-author-interview-steven-kerr-phd/" target="_blank">here</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[27 Feb 2022]]></summary></entry><entry><title type="html">Lockdowns and mortality</title><link href="/2022/02/09/Lockdowns-and-mortality.html" rel="alternate" type="text/html" title="Lockdowns and mortality" /><published>2022-02-09T00:00:00+00:00</published><updated>2022-02-09T00:00:00+00:00</updated><id>/2022/02/09/Lockdowns%20and%20mortality</id><content type="html" xml:base="/2022/02/09/Lockdowns-and-mortality.html"><![CDATA[<p>09 Feb 2022</p>

<p>A <a href="https://sites.krieger.jhu.edu/iae/files/2022/01/A-Literature-Review-and-Meta-Analysis-of-the-Effects-of-Lockdowns-on-COVID-19-Mortality.pdf" target="_blank">paper</a> has recently come out claiming that lockdowns had minimal effects on COVID mortality, causing quite a stir. The headline figure is that in Europe and the USA, lockdowns reduced mortality by 0.2%. Fifty words in and the authors are already dropping bombs like this:</p>

<p><em>“While this meta-analysis concludes that lockdowns have had little to no public health effects,
they have imposed enormous economic and social costs where they have been adopted. In
consequence, lockdown policies are ill-founded and should be rejected as a pandemic policy
instrument.”</em></p>

<p>You’ve gotta admire that moxie.</p>

<p>Here’s my review of this paper.</p>

<h3>  What did they do? </h3>

<p>The authors have done a meta-analysis of studies seeking to establish the effect that non-pharmaceutical interventions (NPIs, referred to loosely as `lockdowns’ by the authors) have on COVID mortality.</p>

<ul>
  <li>
    <p>Their initial search turned up 18,590 papers.</p>
  </li>
  <li>
    <p>1,048 remained after screening based on the title of the paper.</p>
  </li>
  <li>
    <p>117 remained after excluding studies that were not empirical, or did not study COVID mortality.</p>
  </li>
  <li>
    <p>34 remained after filtering according to their eligibility criteria.</p>
  </li>
</ul>

<h3>  What eligibility criteria? </h3>

<p>Great question. Most of the substantive criticisms of this paper focus on whether their selection conditions were reasonable. The authors used the following criteria:</p>

<ul>
  <li>
    <p><strong>Studies that used simulations to predict mortality in the counterfactual were excluded</strong> <br />
Counterfactuals are scenarios that didn’t happen, that one typically uses as a basis for comparison. In this context, the counterfactual is what would have unfolded without lockdowns. The authors only admit studies that used real data on mortality from countries that implemented different policies, rather than simulated data. <br /><br />I find myself in agreement with this as an exclusion criterion, given how stunningly bad various predictions for the epidemic have been, and how much freedom simulations give researchers to get the answer that they want.</p>
  </li>
  <li>
    <p><strong>Studies that used synthetic controls were excluded</strong> <br />
Synthetic control methods work as follows. Ideally, what one would like to do is to take two identical populations, existing in identical circumstances, apply a lockdown to one (the ‘treatment’ group) and not the other (the control group), and then measure the difference in mortality. Any difference can then unambiguously be attributed to lockdown, since everything else is identical.<br /><br />However, that’s obviously not practically feasible. Synthetic control studies attempt to get around this by artifiically creating a control group. So, perhaps there exists a population that were not subjected to lockdown that might be used as a control group, except they are quite different to the treatment group. Synthetic control studies statistically re-weight this population so that they look more like the treatment group, and then examine the differences in mortality compared to the treatment group <br /><br />While there is definitely a lot of room for this technique to be abused, I think the authors were perhaps a little too trigger-happy using it as grounds for exclusion.</p>
  </li>
  <li>
    <p><strong>Interrupted time-series studies were excluded</strong> <br />
Interrupted time series studies are fairly straightforward, and in this context consist of fitting a curve to deaths both before and after the imposition of lockdown. One hopes that all other variables that might affect COVID mortality do not change much during the study period, so that causality can be assigned to the lockdown.<br /><br />I think the authors were on solid ground excluding studies of this type. There are typically just too many other variables that are changing in the study period to sensibly assign causation.</p>
  </li>
  <li>
    <p><strong>Both published and working papers were included</strong> <br />
This means that the paper did not have to be published in a peer-reviewed journal to make the final cut.<br /><br />I think this is fair enough. In my experience, peer-review rarely results in major changes to the results of a paper, and there are many high quality papers that are yet to be published.</p>
  </li>
  <li>
    <p><strong>Papers that focus on comparing imposition of lockdowns at different times excluded</strong> <br />
The main rationale behind this is that the authors were seeking to compare the lockdowns that were actually imposed with a counterfactual in which the lockdowns were not imposed, not a counterfactual in which they were imposed at a different time.<br /><br />You might say, however, that it is still worth doing the latter comparison because maybe it will tell us whether lockdowns are worthwhile if we were just able to get the timing right. However, I have pretty much zero confidence in our ability to precisely time the imposition of lockdowns, and I think I the worldwide track record in the last two years backs me up on that.</p>
  </li>
</ul>

<h3>  Quality measures </h3>
<p>Ok, so that’s how the authors narrowed it down to their chosen 34. That’s not the end of the story though. They also rated each of these studies according to the following four measures of quality:</p>

<ul>
  <li>
    <p><strong>Published versus working papers</strong> <br />
You didn’t think they were going to ignore peer-review completely, did you? <br /><br />I do think it is reasonable to put more trust in papers that have been peer-reviewed. So no qualms with this one.</p>
  </li>
  <li>
    <p><strong>Long verus short term</strong> <br />
Papers that have data that extends beyond 31st May 2021 get an extra merit point from the authors. The rationale here is that if lockdowns ‘flattern the curve’ but do not prevent deaths, then studies that are cut short may conclude that lockdowns are effective against death when in fact all they may be doing is slightly prolonging death.<br /><br />I also think this is reasonable.</p>
  </li>
  <li>
    <p><strong>Studies that have an effect on mortality sooner than 14 days after the intervention</strong> <br />
The idea here is that lockdowns presumably affect mortality by curbing transmission. Since it typically takes at least a few weeks between infection and death for those who die from COVID, any study that sees an early effect on mortality is likely to be flawed.<br /><br />I’m on board with this.</p>
  </li>
  <li>
    <p><strong>Social sciences versus other sciences</strong> <br />
Ok this is where it gets a bit sketchy. The authors’ rationale is that social scientists have greater expertise in evaluating policy interventions compared to those in the natural sciences. What this effectively means is that studies carried out by economists get a merit point, and studies carried out by e.g. epidemiologists don’t.<br /><br />I think this is questionable. While I recognise there certainly are biases in different fields, this just doesnt sit well with me.</p>
  </li>
</ul>

<h3>  The results </h3>
<p>These measures of quality were used to do stratified analyses. So, they carried out their analysis on papers that scored 4 out of 4 on the above criteria, then a separate analysis for the papers that scored at least 3, etc.<br /><br />The authors carry out an inverse-variance weighted meta-analysis, which is just a fancy type of weighted average of the results in the selected papers. It means that studies that were able to more precisely estimate the effect of lockdown on mortality get given more weight. Their main results are as follows:</p>

<ul>
  <li>
    <p><strong>Stringency-based studies</strong> <br />
These are studies that used the <a href="https://ourworldindata.org/" target="_blank">our world in data</a> <a href="https://ourworldindata.org/metrics-explained-covid19-stringency-index" target="_blank">stringency index</a> to measure how strict lockdown measures were. In their meta-analysis of these studies, the authors find that lockdowns resulted in a 0.2% reduction in mortality in Europe and the United States. There were only seven papers in this analysis, and one of them receives almost all the weighting.</p>
  </li>
  <li>
    <p><strong>Shelter-in-place-order studies</strong> <br />
The authors found that shelter-in-place orders reduced mortality by 2.9% in Europe and the United States. Again most of the weight goes to the result from one paper.</p>
  </li>
  <li>
    <p><strong>Specific NPIs</strong> <br />
Amongst papers that scored a perfect 4 on the quality criteria, the authors found a 34% reduction in mortality for the use of face masks, and 2.9% reduction for business closures. Once again, most of weight appears to come from single studies.</p>
  </li>
</ul>

<h3>  The verdict </h3>

<p>Once can find a selection of papers in the literature on lockdowns that support pretty much any inclusion one wishes to come to. Thus most criticism of this meta-analysis has rightly focused on whether the selection criteria employed by the authors are reasonable.</p>

<p>I think they are mostly kosher. The one that I take greatest issue with is the exclusion of papers written by people from the natural sciences. It is a little difficult though, because I do think there is a prevailing orthodoxy amongst epidemiologists that errs strongly on the side of interventions that seek to prevent deaths whose proximate cause is COVID, while neglecting any other side effects those interventions may have, no matter how catastrophic. Economists, on the other hand, tend to orient themselves towards broader, utilitarian measures of societal welfare.</p>

<p>At the intutitive level, I would not be surprised if lockdowns had minimal effect on COVID mortality. The mechanism through which they were meant to work was ‘flattening the curve’, thus preventing healthcare services from being overwhelmed. That didn’t happen, and looking at other countries that didn’t have as stringent lockdowns, it seems like we wouldn’t even have come particularly close.</p>

<p>Another interesting point that the furore over this paper illustrates is the huge schism between economists and public health professionals. It’s quite amazing how two well-developed wings of the academy can be so deeply in disagreement over some rather fundamental items. It’s reminiscent of the chasm that exists between biologists and the humanities on ideas like the <a href="https://en.wikipedia.org/wiki/Tabula_rasa#Science" target="_blank">tabula rasa</a> theory of human nature.</p>

<p>One thing is for sure; if the economists were in charge of the governmental response to the pandemic, things would have looked <em>a lot</em> different.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[09 Feb 2022]]></summary></entry><entry><title type="html">What is vaccine effectiveness?</title><link href="/2022/01/17/What-is-vaccine-effectiveness.html" rel="alternate" type="text/html" title="What is vaccine effectiveness?" /><published>2022-01-17T00:00:00+00:00</published><updated>2022-01-17T00:00:00+00:00</updated><id>/2022/01/17/What%20is%20vaccine%20effectiveness</id><content type="html" xml:base="/2022/01/17/What-is-vaccine-effectiveness.html"><![CDATA[<p>17 Jan 2022</p>

<p>There have been interesting reports from Public Health Scotland that COVID infection rates amongst the unvaccinated have been consistently lower compared to those who have had one or two doses of vaccine since early December 2021, though higher compared to those with a booster dose. See a news story <a href="https://www.heraldscotland.com/news/19843315.covid-scotland-case-rates-lowest-unvaccinated-double-jabbed-elderly-drive-rise-hospital-admissions/" target="_blank">here</a>, based on the original report <a href="https://publichealthscotland.scot/media/11089/22-01-12-covid19-winter_publication_report.pdf" target="_blank">here</a>. I believe this has led many people to question vaccine effectiveness.</p>

<h3>  What is vaccine effectiveness? </h3>

<p>If you’re maths averse, you might want to scroll straight down to the “What does it mean?” section. It’s not that bad though, I promise.</p>

<p>There are many quantities that get called “vaccine effectiveness” that are mathematically and conceptually distinct. This is a little bit unfortunate, because it makes it diffucult to compare them. Throughout this article, I will assume we are talking about vaccine effectiveness against infection, but we may also be interested in effectiveness against hospitalistion, death and other outcomes.</p>

<h4>  Risk ratio </h4>

<p>Risk ratios are perhaps the most intuitive way  of defining vaccine effectiveness. It really is just the probability of a vaccinated person being infected with COVID, divided by the probability of an unvaccinated person being infected with COVID. A risk ratio less than one indicates some vaccine effectiveness against infection. This motivates our first definition of vaccine effectiveness,</p>

\[\mathrm{VE} = 1-  \mathrm{Risk \; ratio}.\]

<h4>  Rate ratio </h4>

<p>The rate at which vaccinated/unvaccinated people get infected can be estimated using <a href="https://en.wikipedia.org/wiki/Poisson_regression" target="_blank">Poisson regression</a>. Dividing the rate for vaccinated people by the rate for unvaccinated people gives the rate ratio, which leads to our second definition of vaccine effectiveness,</p>

\[\mathrm{VE} = 1-  \mathrm{Rate \; ratio}.\]

<p>Rate ratios and risk ratios should be approximately equivalent over the same time horizon.</p>

<h4>  Hazard ratio </h4>

<p>The hazard ratio is similar to the rate ratio, except that it takes into account the number of people who are susceptible to an event at any given time. So the rate ratio just compares the event rate in all the vaccinated and unvaccinated, whereas the hazard ratio compares the event rate amongst only those who are susceptible to an event in the vaccinated and unvaccinated. A key difference is that, for example, people who die during the observation period are removed from the calculation of hazard ratios, but not rate ratios. Hazard ratios can be estimated using <a href="https://en.wikipedia.org/wiki/Proportional_hazards_model" target="_blank">proportional hazards models</a>. This gives our third definition of vaccine effectiveness,</p>

\[\mathrm{VE} = 1-  \mathrm{Hazard \; ratio}.\]

<p>Hazard ratios and risk ratios should be aproximately equivalent if there aren’t many people who cease being susceptible to an event in the observation period.</p>

<h4>  Odds ratio </h4>

<p>The odds of an event is the probability of it occurring \( p \), divided by the probability of it not occurring \(1- p \),</p>

\[\mathrm{Odds} = \frac{p}{1-p}.\]

<p>An odds ratio is dividing the odds of one event, by the odds of a second event,</p>

\[\mathrm{OR} = \frac{ \mathrm{Odds}_1 }{ \mathrm{Odds}_2 }.\]

<p>So, we might divide the odds of being infected with COVID if you have recived the vaccine, by the odds of being infected with COVID if you haven’t received the vaccine. If this quantity is less than one, it indicates that those who received the vaccine are less likely to be infected than those who didn’t. Odds ratios can be calculated using <a href="https://en.wikipedia.org/wiki/Logistic_regression" target="_blank">logistic regression</a>. This gives our fourth definition of vaccine effectiveness,</p>

\[\mathrm{VE} = 1- \mathrm{OR}.\]

<p>Odds ratios are approximately equal to risk ratios for low probability events. This follows from the first order Taylor expansion of the odds,</p>

\[\mathrm{Odds} = \frac{p}{1-p} \simeq p.\]

<h3>  Why so many different measures of vaccine effectiveness? </h3>

<p>At first glance, it might seem odd that we use so many different measures of vaccine effectivenss, especially given that under many circumstances they are approximately equal. Why not just one? The answer that there are a number of statistical models/techniques that can be used to try and estimate vaccine effectiveness, and they each have various merits depending on the assumptions one thinks are reasonable to make, and the data that one has available.</p>

<h3>  What does it mean? </h3>

<p>That’s a fine question. I believe that many people, when reading a headline of e.g. 90% effectiveness, will take that to mean something like “there is a 90% chance the vaccine will prevent me from getting COVID”. Which is of course, not at all what it means.</p>

<p>A more sophisticated take would be “The vaccine will reduce my chances of catching COVID by a factor of 20”. This is better, but still not quite right. The main issue is that it lacks a time horizon. What period of time does this statement apply to?</p>

<p>News reports on studies have vaccine effectiveness have tended to report peak effectiveness over a certain unit of time. For example, in our <a href="https://doi.org/10.1016/S0140-6736(21)00677-2 " target="_blank">study of vaccine effectiveness</a>, we calculated vaccine effectiveness by week, and the media widely reported the maximum value. This can cause confusion.</p>

<h3>  An example </h3>

<p>Let’s assume that the probability of an unvaccinated person being infected with COVID in a given week is 0.1 and constant. Let’s assume vaccine effectiveness stays at roughly the peak level that we found in our paper of 90%.  What is the probability of being infected with COVID relative to an unvaccinated person over, say, a 12 week period?</p>

<p>This is like flipping a biased coin each week, and calculating the probability of flipping 12 “not infected” in a row. Doing this calculation (I’ll spare you the details), you get a reduction in risk of \( 84 \% \).</p>

<p>Pretty good. However, we know that COVID vaccines wane in effectiveness relatively quickly. What happens if we take that into account?</p>

<p>Let’s assume that vaccine effectiveness starts out at 90% and decreases by 5% each week (this is a decent approximation to what we actually see with COVID vaccines). Then you get a reduction in risk of \(49 \% \). Doing the same calculation over 16 weeks, you get a reduction in risk of \(33 \% \).</p>

<p>This is very much a ‘back of the envelope’ calculation, and does not take account of the fact that background infection rates are dynamic and dependent upon levels of immunity amongst the population. Nonetheless I do think it hints at some interesting questions about what the proper aim of vaccination is in the midst of a pandemic.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[17 Jan 2022]]></summary></entry><entry><title type="html">Counting COVID-19 deaths</title><link href="/2022/01/06/Counting-COVID-deaths.html" rel="alternate" type="text/html" title="Counting COVID-19 deaths" /><published>2022-01-06T00:00:00+00:00</published><updated>2022-01-06T00:00:00+00:00</updated><id>/2022/01/06/Counting%20COVID%20deaths</id><content type="html" xml:base="/2022/01/06/Counting-COVID-deaths.html"><![CDATA[<p>06 Jan 2022</p>

<p>There has been a great deal of controversy over the way COVID-19 deaths are calculated. Initially in the UK, the count that was most widely reported consisted of all who died within 60 days of a positive PCR test, or who died more than 60 days after a positive test but had COVID-19 listed as a cause of death on their death certificate.</p>

<p>The main objection to this is that, for example, someone could test positive for COVID-19, recover fully, then die in e.g. a car crash and still be counted as a COVID-19 death.</p>

<h3> What are we trying to do when we count COVID-19 deaths anyway? </h3>

<p>It’s important to be clear what we’re trying to achieve with the COVID-19 death count. I think most people agree that we should be aiming to count the number of deaths caused by COVID-19. But what does that mean exactly? In the Scottish data, the average number of comorbidities in people who died from COVID-19 is upwards of 3. How do we assign causation when there are multiple contributing factors?</p>

<p>Ideally, what we would like to do is a controlled experiment. We would ‘run’ a parallel version of the world that is completely identical, except SARS-CoV-2 is non-existent. And when I say identical, I really mean it. Everyone believes just as much in the reality of SARS-CoV-2 as they do now, people are furloughed from work en masse, governments impose social distancing and lockdown measures etc. Everything the same, it’s just that there ain’t no such thing as SARS-CoV-2.</p>

<p>The point is that we only changed one thing in our experiment. Therefore any difference in outcome can definitively be attributed to that one change.</p>

<p>Of course this wouldn’t really be feasible (not to mention ethical), but at least now we know what we’re aiming for.</p>

<h3> Hasn't someone spent a lot of time thinking about this problem before? </h3>

<p>Yes. There is a large literature on, for example, how to define deaths due to influenza. The proximate cause of death in most people who die as a result of influenza is pneumonia. But they also often have several comorbidities. One might typically see a flu death defined as death after a hospital admission with flu recorded as the reason for admission. However, the question is far from being settled.</p>

<h3> What should we do with COVID-19 then? </h3>

<p>On August 12 2020, <a href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/916035/RA_Technical_Summary_-_PHE_Data_Series_COVID_19_Deaths_20200812.pdf" target="_blank">Public Health England changed their main definition of a COVID-19 death to be a death within 28 days of a PCR positive test.</a> This led to the primary COVID-19 death count dropping by about 13% overnight. Still, it remains possible for car crash victims to be counted as COVID-19 deaths, and there is the unfortunate fact that the rate of COVID-19 deaths increases as we do more testing, even if the rate of deaths actually caused by COVID-19 remains constant.</p>

<p>This is also quite different from how a flu death is usually defined. Perhaps there’s a defensible rationale for that; COVID-19 and flu are different diseases, and there is a world of difference in how we have reacted to them. For example, it might be reasonable not to require a positive PCR test in the definition of a flu death because PCR testing is quite rare in a typical flu season, which would likely lead to a significant under-count. On the other hand, in the UK we have ramped up PCR testing for COVID extraordinarily rapidly, <a href="https://coronavirus.data.gov.uk/details/testing" target="_blank"> from 10s of thousands per day in April 2020, to well over half a million per day at the time of writing</a>. In my eyes, the more pertinent danger with our current definition of a COVID-19 death is over-counting. I think there is a reasonable case for further modifying the defintion of a COVID-19 death, to be death within 28 days of a postive PCR test <strong>and </strong> COVID-19 listed as the main cause of death on the death certificate, or something similar.</p>

<p>I really wouldn’t want to be the PR person who deals with the reaction to that, though.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[06 Jan 2022]]></summary></entry></feed>