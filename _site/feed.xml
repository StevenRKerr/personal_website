<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-11-23T15:09:04+00:00</updated><id>/feed.xml</id><entry><title type="html">Negative vaccine effectiveness</title><link href="/2022/11/20/Negative-vaccine-effectiveness.html" rel="alternate" type="text/html" title="Negative vaccine effectiveness" /><published>2022-11-20T00:00:00+00:00</published><updated>2022-11-20T00:00:00+00:00</updated><id>/2022/11/20/Negative%20vaccine%20effectiveness</id><content type="html" xml:base="/2022/11/20/Negative-vaccine-effectiveness.html"><![CDATA[<p>20 Nov 2022</p>

<p>Myself and a team of collaborators including Stuart Bedston at the University of Swansea, Declan Bradley at Queen’s University Belfast and Mark Joy at the University of Oxford recently published a <a href="https://doi.org/10.1093/ije/dyac199" target="_blank"> paper</a> in the International Journal of Epidemiology studying waning of COVID-19 vaccine effectiveness. One of the major findings of the paper is that for first and second dose of AstraZeneca and first dose of Pfizer, effectiveness of the vaccine against COVID-19 hospitalisation or death reached zero by approximately day 60-80 after receiving the vaccine, and then went negative.</p>

<p>That is quite a striking result. This post is going to dig into possible explanations of this phenomenon and its implications.</p>

<h3> First of all, what did you actually do? </h3>

<p>We carried out a type of study called a ‘target trial’. This attempts to find a ‘naturally occurring’ clinical trial, by matching people in a population who received a given treatment to similar people who didn’t receive the treatment. You can then compare the rate of adverse events in these retrospectively generated, artificial treatment and control groups.</p>

<p>The data that we used to do this study are highly confidential, and stored securely on different servers in each nation of the UK. It is not permitted to have any statistically disclosive, individual-level information leave those servers. However, we introduced an interesting methodological novelty that allowed us to do a pooled study despite these restrictions. This means that the final result that we ended up with is identical to what we would have got if the data all started out in the same location. We were able to do this because of the target trial study design that we used. So, amazingly, it is possible to do a pooled analysis across multiple separate datasets that controls very carefully for individual level characteristics, without actually having to share any individual-level information!</p>

<h3> What could be causing negative vaccine effectiveness? </h3>

<p>One possibilty is that it is behavioural. So people who get vaccinated might be more likely to behave in ways that put them at greater risk of infection. For example, at various points during the pandemic, vaccinated people have been subject to fewer restrictions on movement than unvaccinated people. On the other hand, it could just as well go the other way - unvaccinated people may be less risk averse when it comes to the possibility of being infected, and thus may have behaved in ways that put them at greater danger of contracting COVID-19.</p>

<p>It is also possible that naturally-acquired immunity is more robust than vaccine-induced immunity. A <a href="https://doi.org/10.1093/ije/dyac199" target="_blank"> study</a> from Israel found that vaccinated individuals had a 13-fold increased risk of infection compared to previously infected individuals.</p>

<p>A final possibility is that this is all just a consequence of the fact that the protection offered by the vaccine is relatively short-lasting. For example, imagine that I had two cohorts of similar individuals, and vaccination offered 100% protection against infection for a short period of time. The first cohort receives a dose of vaccine on day 0, while the second cohort receives a dose of vaccine on day 30. One would intuitively expect that the first cohort would initially have fewer severe COVID-19 outcomes, but then at some point the second cohort would do better. Now instead of vaccinating the second cohort, let’s have them contract COVID-19. And instead of it happening on day 30, let’s sprinkle it around randomly in time, but initially occurring at a higher rate than in the vaccinated group. Finally, let’s assume that the vaccine is effective but not perfect for a short period of time, and naturally-acquired innoculation is as immunologically effective (or better) than the vaccine. One would still expect something qualitatively similar; the first cohort would have fewer events intially, then subsequently the second cohort would do better because those people who contract and recover from COVID-19 are naturally innoculated.</p>

<p>By the way I should say that our study is not the only one to find negative vaccine effectiveness - our result is unlikely to be an anomaly.</p>

<h3> What are the implications? </h3>

<p>If negative vaccine effectiveness is largely explained by behavioural factors, then it might be possible to curtail it with a public information campaign aimed at making sure vaccinated individuals continue to observe precautions against COVID-19. On the other hand, I do think that one of the major incentives to get the vaccine is so that one can return to normal behaviour. I personally am not convinced that the explanation for negative vaccine effectiveness is behavioural, and even if it was I am skeptical about our ability to do anything about it.</p>

<p>If negative vaccine effectiveness cannot be significantly curtailed, then it raises some very interesting policy questions. Vaccines still have utility, but what is their proper role in world where COVID-19 is endemic? For example, if vaccine doses could be timed so that the period where negative vaccine effectiveness would kick in happens towards the end of a wave of infections, it may be be possible to provide significant long-term protection by offering a precisley-timed, recurring regimen of COVID-19 vaccines. However, I don’t really have much confidence in our ability to sufficiently accurately forecast future waves of infections in a way that would allow us to usefully implement such a scheme.</p>

<p>The other main utility offered by COVID-19 vaccines is ‘flattening the curve’. That is, spreading the caseload over time so that our healthcare services are not overwhelmed and can offer better treatment. This could be worthwhile, though again it would require forecasting ability that so far has proven elusive to us.</p>

<p>The final question is whether all of this is worth it. Mass COVID-19 vaccinations are expensive, and it is not clear how beneficial it is. It seems quite possible that there are far more cost-effective ways to preserve human life. In particular, I think in our revolutionary fervour for vaccines, we may have neglected a cheaper and easier win in the form of anti-viral and monoclonal antibody treatments.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[20 Nov 2022]]></summary></entry><entry><title type="html">Modern monetary theory is the road to hell</title><link href="/2022/08/28/Modern-monetary-theory-is-the-road-to-hell.html" rel="alternate" type="text/html" title="Modern monetary theory is the road to hell" /><published>2022-08-28T00:00:00+01:00</published><updated>2022-08-28T00:00:00+01:00</updated><id>/2022/08/28/Modern%20monetary%20theory%20is%20the%20road%20to%20hell</id><content type="html" xml:base="/2022/08/28/Modern-monetary-theory-is-the-road-to-hell.html"><![CDATA[<p>28 Aug 2022</p>

<h3> What is modern monetary theory (MMT)? </h3>

<p>It’s an economic theory which, in a nutshell, says that full employment and funding of various other government programmes can be guaranteed by printing money, and the inflation that would normally ensue can be kept under control by taxation.</p>

<h3> Woah. That sounds pretty wild. </h3>

<p>Indeed.</p>

<p>The first thing that should set off alarm bells is that MMT appears to advocate for governmental control over monetary policy. That’s a <i>really</i> bad idea. This is a lesson that we have historically learned the hard way. Governments simply cannot be trusted with the keys to the printing press. The temptation for them to print money in order to deliver on some promise, at least in the short term, is just too great. That’s how you get hyperinflation. There is a reason we insist on independence of central banks and government.</p>

<p>Second is that, of course if you’re willing to fire up the printing press you can achieve full employment. You can pay people to twiddle their thumbs all day if you want. Much more difficult is ensuring that people are employed <i>usefully</i>. Forgive me if I don’t have a great deal of confidence in the ability of governments to do this. It would mean that they would have to step in and effectively deploy a significant and variable fraction of the country’s labour force across different sectors, depending on the prevailing economic conditions. Yeah, right.</p>

<p>Finally, how exactly is it that taxation is supposed to control inflation? I mean, in principle it could be done if the government increased taxes and then threw the revenue thus collected into the sea. And I mean that literally, not figuratively, i.e. if they actually destroyed money, or at least removed it from circulation for an extended period of time. That would effectively make money more valuable which could reduce inflation. The problem is again, do we really think governments can be trusted to do this rather than spend that money? Especially a government that has already demonstrated itself to be quite fond of liberal use of the printing press to meet its obligations?</p>

<p>The whole thing is a nonsense. And a very dangerous nonsense, at that.</p>

<h3> What's really going on here? </h3>

<p>When I see people advocating for ideologies that are so obviously lacking, instead of trying square their circle I ask what purpose does the ideology serve for them.</p>

<p>As an example, think about Marxism. There are many ideas under that umbrella that fail to survive the gentlest scrutiny. Take the <a href="https://en.wikipedia.org/wiki/Labor_theory_of_value" target="_blank"> labour theory of value</a>, which underpins the whole edifice. It holds, roughly speaking, that the economic value of an object is determined by the amount of labour required to produce it. This has the immediate consequence that some extraordinarily useless objects have arbitarily high value. Like a <a href="https://www.youtube.com/watch?v=EZ73Q4DwrGM" target="_blank"> sarcasm detector</a> or <a href="https://youtu.be/dJNd_HtZH2g?t=30" target="_blank"> The Homer</a>. I bet it takes <i>a lot</i> of effort to make them. Ok, my examples may be taken from the Simpsons, but the Simpsons does have a stunning ability to reflect real life.</p>

<p>Anyway, the point is that the entire foundation of Marxism is based on a nonsense. Why then did it become so popular? I would say that no small part of the explanation lies in the fact that it gave the working classes, who at the time genuinely were exploited and treated horrendously, intellectual justification for their revolutionary fervour. It was political discontent in search of an academic seal of approval.</p>

<p>And so it is with MMT.</p>

<p>What MMT advocates ultimately want is to radically increase government spending and have the rich pay for it via progressive taxation. That <i>may</i> be an admirable goal, but what if the process by which it is achieved is far more harmful than the end is good?</p>

<p>The MMT crowd have figured out that increasing taxation is difficult and generally requires a long legislative process that tends to disfavour radical changes. On the other hand, if the government just got its hands on the printing press, it could spend as much as it wants.</p>

<p>The only problem is that causes inflation, and inflation is effectively a tax levied on all those who hold the nation’s currency. As prices rise, the notes in your wallet have less and less purchasing power - you are losing real wealth. And it is a deeply regressive tax at that - poorer people who can’t put their money into assets will be hit harder than the rich.</p>

<p>What the MMT advocates wish to do is to use that to put a gun to government’s head. They fire up the printing press and spend to their heart’s content, and that threatens to cause inflation with  truly terrible consequences. At this point they offer up their snake oil solution. There is only one way out: force rich people to bail us out.</p>

<p>It’s a truly terrible ideology. And there are some indications that it’s becoming more mainstream. For example, here is Joe Biden <a href="https://twitter.com/joebiden/status/1525234935346483210?lang=en" target="_blank"> advocating for a core tenet of MMT</a>.</p>

<p>This is very worrying.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[28 Aug 2022]]></summary></entry><entry><title type="html">The solution to everything</title><link href="/2022/07/22/The-solution-to-everything.html" rel="alternate" type="text/html" title="The solution to everything" /><published>2022-07-22T00:00:00+01:00</published><updated>2022-07-22T00:00:00+01:00</updated><id>/2022/07/22/The%20solution%20to%20everything</id><content type="html" xml:base="/2022/07/22/The-solution-to-everything.html"><![CDATA[<p>22 Jul 2022</p>

<p>What if I told you there was a well-known, 100-year old theory of gravity that reproduces all empirical successes of Einstein’s general relativity, and may solve a who’s who of major oustanding problems in theoretical physics?</p>

<p>Its called <a href="https://en.wikipedia.org/wiki/Einstein%E2%80%93Cartan_theory" target="_blank"> Einstein-Cartan theory</a>, or sometimes Einstein-Cartan-Sciama-Kibble (ECSK) theory. By the way, the eponymous Kibble is my “physics grandfather” - my PhD supervisor’s PhD supervisor.</p>

<p>ECSK theory is a minor generalisation of Einstein’s general relativity that does not require the <a href="https://en.wikipedia.org/wiki/Torsion_tensor" target="_blank"> torsion </a> to be zero. It turns out that the torsion tensor does not have any propagating degrees of freedom, with the consequence that the only place that ECSK theory and general relavitity differ is at extremely high matter densities.</p>

<p>First, let’s do a quick tour of major outstanding problems that ECSK theory might solve.</p>

<h3> Gravitational singularities </h3>

<p>In a nutshell, Einstein’s general relativity posits that space and time are curved, and that their curvature is determined by the distribution of energy/matter within the spacetime. So, if you plonk a mass down somewhere, it causes space and time to bend in way that we experience as gravity.</p>

<p>One of the problems with the theory is that if the mass/energy gets sufficiently dense, the curvature of spacetime becomes infinite. This is known as a gravitational singularity, and in general relativity every black hole comes with one. Many physicists consider it to be an unsatisfactory state of affairs that the theory predicts some physical quantities are infinite, although currently this does not conflict with any experimental evidence because the singularity is inevitably ‘hidden’ behind an <a href="https://en.wikipedia.org/wiki/Event_horizon" target="_blank"> event horizon</a>, which stops us from knowing much about what happens on the other side.</p>

<p>Physicist Nikodem Poplawski has suggested that <a href="https://arxiv.org/abs/0910.1181" target="_blank">gravitational singularities may be avoided in ECSK theory</a>, due to torsion manifesting as a repulsive force that prevents fermions collapsing to infinite density. The big bang is then replaced by a big bounce, and our universe is the other side of a black hole in another universe.</p>

<h3> The horizon problem </h3>

<p>The universe appears to be relatively samey everywhere we look. In particular, the temperature of the <a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background" target="_blank"> cosmic background radiation</a>, a remanant from the early universe, is extremely uniform. Turns out that’s a big problem.</p>

<p>The issue is that when the cosmic background radiation was formed, the universe was about 300,000 years old, and it is possible to show that cosmic background radiation separated by an angle of incidence on Earth of any more than about 2 degrees cannot have been been in causal contact at that time. In other words, there is no reason to think that the radiation coming from any two such patches of the sky were in thermal equilibrium at the time they were produced, and they have not been in thermal contact since - so why is it the same temperature?</p>

<p>To put it more simply, let’s say I have a glass of hot water and a glass of cold water. If they are not in thermal contact with each other, or their surroundings, they will remain at different temperatures. If I pour them both into a container, they rapidly come into thermal equilibrium at an intermediate temperature. Cosmic background radiation from two slightly separated patches of the sky are like separate glasses of water - there’s no reason why they should have been in thermal equilibrium when they were created, and have not been in thermal contact since. So why is it that in every direction we look, the temperature is the same?</p>

<p>In the ECSK theory, the torsion generates <a href="https://en.wikipedia.org/wiki/Inflation_(cosmology)" target="_blank"> cosmic inflation</a>, which<a href="https://arxiv.org/abs/1410.3881" target="_blank"> causes the early universe to be homogeneous</a>, leading to uniformity in the temperature of the cosmic background radiation.</p>

<h3> The black hole information paradox </h3>

<p>In quantum mechanics/quantum field theory, it is taken as an axiom that time evolution is <a href="https://en.wikipedia.org/wiki/Unitarity_(physics)" target="_blank"> unitary</a>. What this means simply is that the value of the wavefunction at one time determines a unique value at all times. The main reason this is important is because it is a sufficient condition for ‘probability to be conserved’. That is, if the probability of finding the system in some state \( \ket{ \psi(0)} \) at time \(0\) is \(p\), and it evolves to \( \ket{ \psi(t)} \) at time \(t\) under the Hamiltonian, then the probability of \( \ket{ \psi(t)} \) is also \(p\).</p>

<p>Note however, as Roger Penrose reminds us, that there are standard interpretations of quantum mechanics in which unitarity is routinely violated (by measurement), and this does not appear to pose any insurmountable difficulties.</p>

<p>In the 1970s, Stephen Hawking found that combining quantum field theory and general relativity could result in a loss of unitarity. In particular, black holes emit <a href="https://en.wikipedia.org/wiki/Hawking_radiation" target="_blank"> Hawking radiation</a> which should eventually cause black holes to evaporate. The resulting thermal radiation would not contain any information about the initial state of the black hole. In other words, multiple different initial states could end in the same final state, and therefore a value for the wavefunction at a later time does not uniquely determine its value at all other times. Information has somehow been destroyed.</p>

<p>Thus it appears that physicists have to give up at least one of their cherished principles. Either time evolution in the Schrodinger equation is not unitary, or general relativity is wrong, or some other aspect of quantum field theory is wrong.</p>

<p>It has been suggested the ECSK theory could resolve the black hole information paradox by avoiding the gravitational singularity in black holes. Then every black hole is actually a one-way membrane leading to a new universe that retains all the information from the initial state.</p>

<h3> Matter-antimatter asymmetry </h3>

<p>In the laws of physics as they are best known today, there is nothing that favours the production of matter over antimatter. On the other hand, the <a href="https://en.wikipedia.org/wiki/Observable_universe" target="_blank"> observable universe</a> is heavily dominated by matter, which immediately raises the question “Why?”.</p>

<p>ECSK theory coupled to fermions generates a term that is cubic in the spinors and thus not invariant under charge conjugation, giving a <a href="https://arxiv.org/abs/1101.4012" target="_blank"> possible explanation for matter-antimatter asymmetry</a>.</p>

<h3> Ultraviolet divergences </h3>

<p>When you naively try to calculate physically measurable quantities in interacting quantum field theories, you tend to get an infinite answer. That’s a problem.</p>

<p>The basic issue is that fields are operator-valued distributions, for which multiplication is not well-defined outside of some trivial cases. If you plough ahead anyway pretending this problem doesn’t exist, aforementioned infinities rear their ugly head. Garbage in, garbage out, as they say.</p>

<p>To deal with this, physicsts play a regularisation game that involves artificially adding terms to the theory to cancel out the infinities, in a process that Richad Feynman famously described as ‘dippy’. This is not mathematically kosher, but it does allow us to extract sensible predictions from the theory, so the technical concerns tend to get brushed under the carpet to a degree.</p>

<p>The infinities that occur are broadly of two kinds: ultraviolet (high energy/short distance) and infrared (low energy/large distance). The infrared divergences are typically regarded as less troubling, and one reason is because they go away if you think the universe is finite because that introduces a maximum possible length scale. The ultravolet divergences tend to cause more consternation because there is less sympathy for the idea of a universal minimum length scale.</p>

<p>It has been suggested by Nikodem Poplawski that Einstein-Cartan theory <a href="https://arxiv.org/abs/1712.09997" target="_blank"> resolves the ultraviolet divergences</a> because torsion causes momentum operators to be non-commutative.</p>

<h3> Ok, if ECSK theory is so awesome, why isn't it more popular? </h3>

<p>Great question, me.</p>

<p>So I should first say that the solutions that ECSK theory proposes to the above problems are mostly only conjecture at the moment. We have suggestive evidence only.</p>

<p>On the other hand, it is a relatively modest generalisation of Einstein’s general relativity that may solve just about every major problem going in physics today. Why isn’t it famous?</p>

<p>One reason is that ECSK theory is more complicated than Einstein’s general relativity. Introducing torsion makes it significantly more difficult to do calculations and extract predictions from the theory. Because there is no currently feasible experiment whose result would distinguish between Einstein’s general relativity and ECSK theory, people have tended to go for the simpler theory.</p>

<p>That said, the additional theoretical complications that ECSK theory brings does not, to me, seem to justify how little attention it receives. It introduces non-linearity in the Dirac equation, but since when did a little non-linearity scare us physicists? General relativity and Quantum Chromodynamics are both non-linear and we’re quite happy with those. On the other hand, there is an entire industry dedicated to the extremely speculative idea of string theory, which is mind-bogglingly complicated and it is not yet clear whether it solves anything despite decades of dedicated work by many very smart people. Surely ECSK theory could use a little love too?</p>

<p>Ultimately I think the disparity has its explanation in historical coincidence and herding behaviour. Which I understand, but I also think we need to #GiveECSKTheoryAChance.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[22 Jul 2022]]></summary></entry><entry><title type="html">On one-use code</title><link href="/2022/06/05/On-one-use-code.html" rel="alternate" type="text/html" title="On one-use code" /><published>2022-06-05T00:00:00+01:00</published><updated>2022-06-05T00:00:00+01:00</updated><id>/2022/06/05/On%20one%20use%20code</id><content type="html" xml:base="/2022/06/05/On-one-use-code.html"><![CDATA[<p>05 Jun 2022</p>

<p>Academics who code are, I think, somewhat known for their less than stellar creations. I have certainly written code that I would prefer didn’t see the light of day. A recent example that attracted a great deal of attention is <a href="https://en.wikipedia.org/wiki/Neil_Ferguson_(epidemiologist)" target="_blank"> Professor Neil Ferguson’s</a> <a href="https://en.wikipedia.org/wiki/CovidSim" target="_blank"> CovidSim</a>. As the name suggests, it is a simulator of COVID-19 transmission that works by creating artificial agents representing people and environments that they interact with, in much the same way as the SimCity series of games but without the funky graphics. It was the basis of a <a href="https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf" target="_blank"> paper</a> that is credited with fundamentally altering the course of the UK’s COVID-19 policy. The paper predicted that even under the optimal ‘mitigation’ strategy that was considered, the peak surge capacity of ICU beds in the UK would be exceeded 8-times over due to the pandemic. As of the date of writing, it has 3,910 citations.</p>

<p>In May 2020, The code behind the simulation was released to GitHub, which is a ubiquitously-used online tool for software development. Before it was released to the public, it apparently consisted of a single 15,000 line file written in C. Having one file of source code that long is already a cardinal sin in coding, and there were many other failures to live up to standard software development practices.</p>

<h3> Ok, but it worked, didn't it? </h3>

<p>An independent research group was able to reproduce the published results of CovidSim by running it themselves. So, while CovidSim may have been less than desirable from a coding point of view, it did its job as intended. Does it really matter that it wasn’t that pretty?</p>

<p>I have a lot of sympathy for the team that worked on this code. Academic coding often doesn’t live up to industry software development standards, for a multitude of reasons. Code that is used to carry out analysis for research is typically ‘one-use’, or close to it. It is not, for example, google.com, which is used by god knows how many millions of people every day and has a large team of people continuously maintaining it. Code written for research is typically intended to be used by a small number of people who authored it, to obtain a specific set of outputs just once.</p>

<p>In addition to that, the PhD students, postdocs and other research staff who typically write the code for such projects are usually themselves not trained in industry standard software development pratices. Many of them will come from disciplines that aren’t primarily coding-centred, and will have to pick it up along the way with little or no oversight. That’s a tough position to be in.</p>

<p>Third, academia is often a race to publish results, that can end up being a very much winner-take-all proposition. Release your paper a week too late and posterity will not look kindly upon you. These are not circumstances that are conducive to producing nice code.</p>

<p>Lastly, an adage comes to mind that goes something like this: “Feel free to break the rules once you know why they exist”. It captures the idea that once you are at a sufficiently high level in a given skill, a lot of what you do consists of knowing when exactly the rules can be bent/broken. Great chess grandmasters, for example, often play with flagrant disregard for well-established principles of the game. They have been at it for long enough that they can often get a competitive edge by going beyond the rules, in a way that is informed by years of study and expertise. Likewise, there are circumstances where it is ok to write fairly ‘terrible’ code, that breaks all the rules - sometimes you just need a result quickly, and you can focus on cleaning it up/optimising later, or not at all. Why, for example, spend hours beautifying a piece of code when you have a deadline looming that depends crucially on the output of said code? Do your due diligence in making sure it is correct, and hit enter.</p>

<p>That said, it is not exactly confidence-inspiring that this simulation that changed the course of a nation was lacking in robustness. One reason we like pretty code is that it tends to minimise the chances of an error. The stakes could not have been higher in this case - an error would have had massive consequences. Also, the fact that the code has been around in one form or another since 2005, and has been adapted repeatedly for modelling various epidemics, makes me a little less sympathetic. That is not exactly what one would call one-use code.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[05 Jun 2022]]></summary></entry><entry><title type="html">The F-twist</title><link href="/2022/05/07/The-F-twist.html" rel="alternate" type="text/html" title="The F-twist" /><published>2022-05-07T00:00:00+01:00</published><updated>2022-05-07T00:00:00+01:00</updated><id>/2022/05/07/The%20F-twist</id><content type="html" xml:base="/2022/05/07/The-F-twist.html"><![CDATA[<p>07 May 2022</p>

<h3> What is the F-twist? </h3>

<p>Milton Friedman was a Nobel prize-winning economist whose ideas became deeply influential both inside and outside academia.
He broadly advocated for free markets with minimal government intervention. For example, he opposed the military draft,
 minimum wage laws and <a href="https://en.wikipedia.org/wiki/Occupational_licensing" target="_blank"> occupational licensing</a>,
and supported <a href="https://en.wikipedia.org/wiki/School_voucher" target="_blank"> school vouchers</a>
and legalisation of drugs and prostitution. He was an advisor to Ronald Reagan
and Margaret Thatcher, and he is widely thought of as the intellectual father of the ideology of 
<a href="https://en.wikipedia.org/wiki/Neoliberalism" target="_blank"> neoliberalism</a>.</p>

<p>In his 1953 book Essays in positive economics, he introduced a principle that has later become known as the “F-twist”, famously 
embodied in the following sentence:</p>

<blockquote>
Truly important and significant hypotheses will be found to have "assumptions" that are wildly inaccurate descriptive 
representations of reality, and, in general, the more significant the theory, the more unrealistic the assumptions (in this sense).
</blockquote>

<p>The name “F-twist” was dubbed by Friedman’s fellow Nobel economist Paul Samuelson, who reportedly chose not to name it after Friedman
directly out of “courtesy”.</p>

<p>It’s a pretty outrageous idea, seemingly suggesting that we should limit our search for new theories to those whose axioms are
“wildly inaccurate”. Nonetheless, it has been widely adopted in mainstream academic economics. That’s a fascinating state of affairs
to me.</p>

<h3> Doesn't it contradict pretty much everything we know about science and the scientific method? </h3>

<p>In a word: yes.</p>

<p>The core of the scientific method is</p>

<ol>
  <li>Generate a hypothesis</li>
  <li>Figure out what observable consequences of the hypothesis are</li>
  <li>Do experiments to see if those observable consequences are in accordance with reality</li>
</ol>

<p>There’s a few thorny issues to take care of there. But the underlying philosophy is that the proper aim of science is to understand/predict
the behaviour of the universe and its various constitutents, and the ultimate arbiter of truth is reality itself - all ideas
must be subject to rigorous empirical testing.</p>

<p>As an example, consider Einstein’s special theory of relativity. It starts with two axioms:</p>

<ol>
  <li>The laws of physics are the same in all inertial frames of reference</li>
  <li>The speed of light in a vacuum is that same for all observers</li>
</ol>

<p>Starting from just this, one can figure out (if you’re a genius) that measurements of length and time are “subjective” - two 
observers with clocks and rulers can disagree on the length of an object, or the number of seconds that elapses between
two events, depending on their state of motion. In particular, an observer travelling at high speed will experience time
passing more slowly, and lengths as shorter, than a stationary observer.</p>

<p>One of the great strengths of this theory is that it made clear, novel predictions, and the axioms can be directly subjected 
to experimental test. As it happened, it passed with flying colours.</p>

<p>Of note here is that the axioms of a theory are trivially also predictions of that theory, that should be tested against reality. 
There is nothing special about the axioms as opposed to observable consequences that are derived from those axioms; they all 
must be scrutinised to determine if they are in concordance with reality.</p>

<p>The extraordinary thing about the F-twist is that it completely upends this bedrock of science. It arbitrarily asks us to give the axioms of 
a theory an easy ride. In fact, further; it asks us to look for axioms that are wildly inconsistent with observed reality, for
this is surely where all the most fruitful and significant ideas lie. If taken seriously, it would revolutionise the 
way we do science. It’s not often in history that paradigm shifts like that occur.</p>

<p>Of course, it needn’t be a revolution that’s actually any good. Imagine physicists and chemists and biologists rejecting new
ideas out of hand because they are simply in excessively good agreement with reality. Everyone knows those theories are a dead-end!</p>

<p>I have to call out the F-twist for what it is: nonsense.</p>

<h3> Oh yeah, if it'so obviously wrong, then why has it been bought wholesale by academic economists? </h3>

<p>Oh that’s easy. It’s because they need to rationalise what they spend their professsional lives doing.</p>

<p>Economists in general are trying to answer extraordinarily complex questions. Questions so difficult that they may be
permanently beyond the ability of humans to answer. The people who end up devoting their professional lives to this pursuit
are inevitably going to be the ones who can convince themselves, by hook or by crook, that what they are doing is useful and important.
 The people who don’t do that drop out of the profession relatively quickly.</p>

<p>What we are left with is a lot of theorising that is ultimately fairly useless. The metric of success is decoupled from 
the normal scientific standards of standing up to empirical scrutiny, and instead focuses on how much other equally 
useless research it inspires/influences. Friedman was quite expicit about this in his essays in positive economics; 
theories are to be judged according to fruitfulness in the precision and scope of their predictions and <i>ability to generate 
additional research lines</i>. Theoretical economics becomes a hierarchy that is predicated more upon ability to convince
one’s colleagues how pleasing one’s ideas are, than ability to understand and predict economic systems.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[07 May 2022]]></summary></entry><entry><title type="html">Model complexity</title><link href="/2022/04/21/Model-complexity.html" rel="alternate" type="text/html" title="Model complexity" /><published>2022-04-21T00:00:00+01:00</published><updated>2022-04-21T00:00:00+01:00</updated><id>/2022/04/21/Model%20complexity</id><content type="html" xml:base="/2022/04/21/Model-complexity.html"><![CDATA[<p>21 Apr 2022</p>

<p>There are models for just about anything you can think of. Fundamental physics, economic markets, epidemics, brain function, climate, click-through rates, animal behavior, election outcomes, to name a few. There’s a universe of possibilities for ways to model any given phenomenon, and only a few small hidden oases in an otherwise barren desert. What’s a model-builder to do?</p>

<h3> How complex should my model be? </h3>

<p>So, a model should certainly not be as “complex” as the pheonomenon that it attempts to describe is, at least at face value. The whole point of a model is to take something that is too complex for our brains to process by brute force, and find some underlying structure that makes it easier for us to understand. ‘The map is not the territory’, as they say.</p>

<p>As an example, <a href="https://en.wikipedia.org/wiki/Standard_Model" target="_blank"> The Standard Model</a> of particle physics contains precisely 19 free parameters that need to be determined by experiment. In exchange for that, we get a description of all known matter, that is accurate all the way down to the subatomic scale. Quite extraordinary.</p>

<p>That still leaves plenty of rope with which to hang oneself though. In particular, I think the academy has a tendency to needlessly over-complicate. For one thing, more free parameters allows more impressive results to be obtained - at least when limiting to the data that the model is trained with. This is known as <a href="https://en.wikipedia.org/wiki/Overfitting" target="_blank"> overfitting</a>, and it’s everywhere. It gives the model-builder more levers and buttons that they can use in order to manufacture the exact results they want. Cranking up model complexity also allows academics to go down an intellectual rabbit hole where they can write lots of papers, as is necessary to advance one’s career. And finally, it allows them to obfuscate their overfitting - the model gets so complex that people mostly can’t be bothered investing weeks/months of their lives in order to figure out the exact trickery that is afoot.</p>

<h3> Model complexity should be proportional to experimental power </h3>

<p>The better the experiments we can do, the more detailed the hypotheses they can distinguish between. So for example, good physics models can be quite complex because we can often do excellent experiments that allow us to declare a clear victor amongst the competitors. We can build big machines like the <a href="https://en.wikipedia.org/wiki/Large_Hadron_Collider" target="_blank"> Large Hadron Collider</a> and smash beams of particles together in precisely controlled ways for years, and see which theory best predicts what comes out the other end.</p>

<p>By way of contrast, in e.g. economics we generally have poor experimental power. We can’t run repeated controlled experiments that involve tweaking the economies of large countries and see what the results are. This is why economic modelling should for the most part be quite humble in its aspirations. It should not, for example, have any illusions about its ability to provide adequate <a href="https://en.wikipedia.org/wiki/Microfoundations" target="_blank"> microfoundations</a> for macroeconomic models. It’s just not going to happen, at least anytime soon.</p>

<h3> Parameters required:Parameters used </h3>

<p>I have a somewhat loosely formulated idea for an initial sweep at sorting the wheat from the chaff.</p>

<p>First, look at the quantities that the model purports to accurately predict. What is the smallest number of parameters that would be required to get a prediction at least as good? For example, if its a macroeconomic model whose only accurate prediction a straight line trend for GDP, this would require precisely two parameters: an intercept and a slope.</p>

<p>Now look at the number of free parameters the model has. If it has at least two, then I suggest the model prima facie has no predictive value beyond a simple line of best fit. It hasn’t simplified anything, and the free parameters  give the model-builder enough space to actually get whatever straight line they wanted.</p>

<p>This is definitely a vague idea at the moment, but maybe it has some legs.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[21 Apr 2022]]></summary></entry><entry><title type="html">God</title><link href="/2022/03/27/God.html" rel="alternate" type="text/html" title="God" /><published>2022-03-27T00:00:00+00:00</published><updated>2022-03-27T00:00:00+00:00</updated><id>/2022/03/27/God</id><content type="html" xml:base="/2022/03/27/God.html"><![CDATA[<p>27 Mar 2022</p>

<p>I have decided to take a philosophical turn with this blog, because why the Dickens not. Philosphers love doing useless crap (just kidding, I heart you guys), so in that spirit I have converged upon the ultimate philosophical waste of time: Does god exist?</p>

<h3> Whence god? </h3>

<p>Right so as a good proto-philsopher, it’s important to start with definitions in order to elucidate the subsequent discussion. What do we mean by ‘god’?</p>

<p>I distinguish between two broad formulations.</p>

<h4> Formulation 1: The man in the sky </h4>

<p>This notion of god would have him as a <em>physical</em> being, residing in a <em>physical</em> place in the universe (in our exemplar, the sky). So basically, a bloke a lot like you or I, except holding some mysterious interest in/sway over mundane human matters.</p>

<p>Now I think we are quite justified in concluding with an <em>extremely high</em> level of confidence that this god does not exist. First of all, you’d think we would have located him by now. And don’t give me that <a href="https://en.wikipedia.org/wiki/Evidence_of_absence" target="_blank"> absence of evidence is not evidence of absence</a> claptrap - if I look extensively in the place where the thing would be if it existed and turn up nothing, then that <em>is</em> evidence that the thing ain’t there. But more than that, this hypothetical being would, according to most ideas about god, have various supernatural powers/properties that are contrary to basically everything that we know and believe about anything. If we ever found him, this lad would have <em>a lot</em> of explaining to do. The physics textbooks would have to be re-written for a start.</p>

<p>Alright so let’s move on to the next door.</p>

<h4> Formulation 2: The whatcha-ma-callit  in the thing-ma-bob </h4>

<p>Admittedly, this one is a bit more abstract. But I think it is the concept of god that most believers have, whether they acknowledge it or not. Namely; god as a mysterious, supernatural and completely intangible entity. There is no experiment that we could do, <em>not even in principle</em>, that would allow us to reasonably adjust our degree of belief in his existence one way or the other. God transcends all that pedestrian stuff.</p>

<p>At this point my reply is pretty simple: I consider this to be the definition of something that does not exist. Like, if someone tells me there’s a gnome on my shoulder, but this gnome does not interact with our universe in any intelligble way and has a presence that is theoretically undetectable, then I am pretty uninterested in talking about the gnome. That is as long as I’m on a fact-finding mission about the universe. This gnome contains no information about the universe, and so frankly can take a running jump. Just another <a href="https://en.wikipedia.org/wiki/Russell%27s_teapot" target="_blank">Russell’s teapot</a>.</p>

<p>I think this is a reasonable definition of what it means for something to not exist, and I’m willing to engage in trial by combat with anyone who thinks otherwise.</p>

<h3> Hang on, but isn't the idea of god interesting/useful? </h3>

<p>For sure. And this is quite separate from the idea of whether god exists. Humans are quite capable of entertaining/believing in all manner of fantastical, self-inconsistent and just plain wrong things.</p>

<p>As a storytelling device god is absolutely unparalleled. I think the most compelling stories that have ever been told have all in one way or another been about god. As an organising idea for human behaviour and by extension a force guiding the course of human history, you’ll struggle to find anything more significant - religious stories have had as deep an impact on humanity as you care to realise. For my money, I think that god and religion provide a sort of familial superstructure to society - in Christianity it is the brotherhood of man under the paternal bond of god. I don’t think we stop having a deep yearning for a father figure as soon as we become adults ourselves. That’s our so-called god-shaped-hole, and it needs filled with something, gosh darnit. Indeed, I am semi-convinced that such superstructures are necessary/responsible for the emergence of humans from the wretched <a href="https://en.wikipedia.org/wiki/State_of_nature" target="_blank">state of nature</a>. And I have had my mind changed in a fairly major way in recent years on the merits of religion and the contributions that it has made to humanity. In short, I think it’s likely a net positive. But that’s a subject for another day.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[27 Mar 2022]]></summary></entry><entry><title type="html">The weakness of the continuum</title><link href="/2022/03/08/The-weakness-of-the-continuum.html" rel="alternate" type="text/html" title="The weakness of the continuum" /><published>2022-03-08T00:00:00+00:00</published><updated>2022-03-08T00:00:00+00:00</updated><id>/2022/03/08/The%20weakness%20of%20the%20continuum</id><content type="html" xml:base="/2022/03/08/The-weakness-of-the-continuum.html"><![CDATA[<p>08 Mar 2022</p>

<p>Ever since Newton/Leibniz invented the infinitesimal calculus in the latter half of the 1600s, it seems like the world has been enthralled by beauty and possibility of the continuum. This event marked the beginning of modern physics and mathematics, and there is virtually no topic in either of these fields that is not deeply interwoven with calculus.</p>

<p>Sometimes, though, I think they may have been a little too successful.</p>

<h3> Before we get started, kindly allow me to explain my super clever title pun </h3>

<p>It turns out that there are different sizes of infinity. So, for example, the entire set of counting numbers \( \{ 1,2,3 …\} \) is smaller than the entire set of real numbers. What this means is that you can pair up every counting number to a real number, and still have lots of real numbers left over. 
Sets that are of the same ‘size’ as the real numbers are sometimes said to have the ‘power of the continuum’ - and this is where we lay the scene of the eponymous wordplay.</p>

<h3> What's weak about the continuum? </h3>

<p>No one can doubt the real numbers are pretty and endlessly fascinating. But have we sometimes been a little too seduced/beguiled by them?</p>

<h4> Exhibit A: Quantum field theory </h4>

<p>Quantum field theory is at the heart of our current best theory of the fundamental workings of the universe, <a href="https://en.wikipedia.org/wiki/Standard_Model" target="_blank">The Standard Model</a>. However, somewhat amusingly, <a href="https://en.wikipedia.org/wiki/Wightman_axioms#Existence_of_theories_which_satisfy_the_axioms" target="_blank">we do not yet know if the standard model ‘exists’ mathematically</a>. When it comes to calculating physical quantities from the standard model, we use various types of fudges/approximations, all of which rely in one way or another on a ‘cutoff’. The cutoff is a distance scale below which, for practical purposes, we declare that we are not interested in what is physically happening - much like when we round off numbers at say, 2 decimal places, because precision beyond that is not useful for the purpose at hand. This cutoff effectively introduces some ‘discreteness’ into the theory, and that allows us to do sensible calculations.</p>

<p>However, the full theory is meant to be set in a continuous spacetime. When you try to define this mathematically, you unfortunately run into all sorts of problems. In particular, the fields are operator-valued distributions, for which multiplication is not well-defined outside of some very restricted subsets. This has the consequence that there are basically no known, interacting quantum field theories that have local degrees of freedom - this is the kind of quantum field theory that is required to describe our universe. Indeed there is a <a href="https://en.wikipedia.org/wiki/Yang%E2%80%93Mills_existence_and_mass_gap" target="_blank">million dollar Millenium prize</a> waiting for anyone who can provide an example. It has also led physicists to spend a huge amount of time and effort on very complex, speculative, and issue-ridden ideas like string theory.</p>

<p>All of this kerfuffle can be avoided if you’re just willing to take the cutoff seriously and put the theory on a discrete spacetime lattice. Everything is mathematically well-defined, and any physical quantity the theory predicts can be calculated to any desired degree of precision by adjusting the cutoff as necessary.</p>

<p>Physicists almost unanimously reject this, though. The continuum is just too pretty to be abandoned, despite all the issues it causes.</p>

<h4> Exhibit B: Financial markets </h4>

<p>Let’s say that asset prices are buffeted by mysterious, difficult-to-comprehend forces that we believe are roughly ‘independent’ from each other at different time steps. Let’s say we also believe that time in financial markets is a continuum. Then the central limit theorem implies that movements in asset prices are normally distributed at all times - it’s inescapable.</p>

<p>Of course, movements in asset prices are known to exhibit ‘fat tails’ - in reality there are far more extreme events than we would expect from models that use normal distributions. Indeed some people have at least partly attributed the 2008 financial crisis, as well as various other financial disasters, to widespread use of modelling strategies in which asset returns are normally distributed.</p>

<p>It is pretty clear that the assumptions above are wrong. An asset price does not undergo an infinite number of random kicks between 09:00:00 AM and 09:00:01 AM on a Monday morning. But the underlying mathematics sure is pretty if you assume it does.</p>

<h3> Conclusion </h3>

<p>I’m sure there are other good examples we could talk about. My point is that I think people often want so much for the maths to be pretty that they get led astray from reality.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[08 Mar 2022]]></summary></entry><entry><title type="html">Vaccines and blood clots</title><link href="/2022/02/27/Vaccines-and-blood-clots.html" rel="alternate" type="text/html" title="Vaccines and blood clots" /><published>2022-02-27T00:00:00+00:00</published><updated>2022-02-27T00:00:00+00:00</updated><id>/2022/02/27/Vaccines%20and%20blood%20clots</id><content type="html" xml:base="/2022/02/27/Vaccines-and-blood-clots.html"><![CDATA[<p>27 Feb 2022</p>

<p>On February 22nd, PLOS medicine published a <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003927" target="_blank">paper</a> on cerebral venous sinus thrombosis (CVST) following vaccination on which I was the lead author. CVST is a type of blood clot in the brain that can lead to stroke and death. In this paper, we carried out a pooled analyis of 11.6 million people in England, Scotland and Wales, comparing the rate of CVST events in individuals before and after receiving the vaccine. We found that in the 4 weeks following vaccination with Oxford-AstraZeneca, the risk of CVST events approximately doubled. We did not see any increase in risk of CVST events following Pfizer.</p>

<p>This paper was released at the same time as a <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003926" target="_blank">second paper</a> headed by William Whiteley, also at the University of Edinburgh, that studied 46 million people in England and had strikingly similar findings. It is reassuring when independent research efforts come to similar conclusions.</p>

<p>CVST is a very rare event, occurring at a rate of perhaps 3-4 per million people per year. Assuming this as a background rate, our findings would imply one additional CVST event for every 4 million doses of Oxford-AstraZeneca administered. This has to be weighed up against the benefits of vaccination. Due to concerns over safety, Oxford-AstraZeneca was suspended in those aged under 40, and was not routinely used for booster doses in the UK. The risk/benefit analysis for whom should be vaccinated, with which vaccine, and when, is extremely complicated and depends on background rates of infection, availability of vaccines, vaccine waning, characteristics of future variants and robustness of naturally-acquired immunity versus vaccine-induced immunity, among others.</p>

<p>Our paper included a small methodological novelty that allowed us to pool data across several nations without individual-level data having to be shared between them. Each country stores data in a trusted resesarch environment (TRE), which is just a highly secure server that contains anonymised data. Sharing individual-level data is forbidden, but we worked around this by carrying out a Poisson regression in which only count data is required. We are aiming to use a similar method to carry out further pooled studies in future.</p>

<p>PLOS medicine published an author interview with me, which can be found <a href="https://speakingofmedicine.plos.org/2022/02/22/plos-medicine-author-interview-steven-kerr-phd/" target="_blank">here</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[27 Feb 2022]]></summary></entry><entry><title type="html">Lockdowns and mortality</title><link href="/2022/02/09/Lockdowns-and-mortality.html" rel="alternate" type="text/html" title="Lockdowns and mortality" /><published>2022-02-09T00:00:00+00:00</published><updated>2022-02-09T00:00:00+00:00</updated><id>/2022/02/09/Lockdowns%20and%20mortality</id><content type="html" xml:base="/2022/02/09/Lockdowns-and-mortality.html"><![CDATA[<p>09 Feb 2022</p>

<p>A <a href="https://sites.krieger.jhu.edu/iae/files/2022/01/A-Literature-Review-and-Meta-Analysis-of-the-Effects-of-Lockdowns-on-COVID-19-Mortality.pdf" target="_blank">paper</a> has recently come out claiming that lockdowns had minimal effects on COVID mortality, causing quite a stir. The headline figure is that in Europe and the USA, lockdowns reduced mortality by 0.2%. Fifty words in and the authors are already dropping bombs like this:</p>

<p><em>“While this meta-analysis concludes that lockdowns have had little to no public health effects,
they have imposed enormous economic and social costs where they have been adopted. In
consequence, lockdown policies are ill-founded and should be rejected as a pandemic policy
instrument.”</em></p>

<p>You’ve gotta admire that moxie.</p>

<p>Here’s my review of this paper.</p>

<h3>  What did they do? </h3>

<p>The authors have done a meta-analysis of studies seeking to establish the effect that non-pharmaceutical interventions (NPIs, referred to loosely as `lockdowns’ by the authors) have on COVID mortality.</p>

<ul>
  <li>
    <p>Their initial search turned up 18,590 papers.</p>
  </li>
  <li>
    <p>1,048 remained after screening based on the title of the paper.</p>
  </li>
  <li>
    <p>117 remained after excluding studies that were not empirical, or did not study COVID mortality.</p>
  </li>
  <li>
    <p>34 remained after filtering according to their eligibility criteria.</p>
  </li>
</ul>

<h3>  What eligibility criteria? </h3>

<p>Great question. Most of the substantive criticisms of this paper focus on whether their selection conditions were reasonable. The authors used the following criteria:</p>

<ul>
  <li>
    <p><strong>Studies that used simulations to predict mortality in the counterfactual were excluded</strong> <br />
Counterfactuals are scenarios that didn’t happen, that one typically uses as a basis for comparison. In this context, the counterfactual is what would have unfolded without lockdowns. The authors only admit studies that used real data on mortality from countries that implemented different policies, rather than simulated data. <br /><br />I find myself in agreement with this as an exclusion criterion, given how stunningly bad various predictions for the epidemic have been, and how much freedom simulations give researchers to get the answer that they want.</p>
  </li>
  <li>
    <p><strong>Studies that used synthetic controls were excluded</strong> <br />
Synthetic control methods work as follows. Ideally, what one would like to do is to take two identical populations, existing in identical circumstances, apply a lockdown to one (the ‘treatment’ group) and not the other (the control group), and then measure the difference in mortality. Any difference can then unambiguously be attributed to lockdown, since everything else is identical.<br /><br />However, that’s obviously not practically feasible. Synthetic control studies attempt to get around this by artifiically creating a control group. So, perhaps there exists a population that were not subjected to lockdown that might be used as a control group, except they are quite different to the treatment group. Synthetic control studies statistically re-weight this population so that they look more like the treatment group, and then examine the differences in mortality compared to the treatment group <br /><br />While there is definitely a lot of room for this technique to be abused, I think the authors were perhaps a little too trigger-happy using it as grounds for exclusion.</p>
  </li>
  <li>
    <p><strong>Interrupted time-series studies were excluded</strong> <br />
Interrupted time series studies are fairly straightforward, and in this context consist of fitting a curve to deaths both before and after the imposition of lockdown. One hopes that all other variables that might affect COVID mortality do not change much during the study period, so that causality can be assigned to the lockdown.<br /><br />I think the authors were on solid ground excluding studies of this type. There are typically just too many other variables that are changing in the study period to sensibly assign causation.</p>
  </li>
  <li>
    <p><strong>Both published and working papers were included</strong> <br />
This means that the paper did not have to be published in a peer-reviewed journal to make the final cut.<br /><br />I think this is fair enough. In my experience, peer-review rarely results in major changes to the results of a paper, and there are many high quality papers that are yet to be published.</p>
  </li>
  <li>
    <p><strong>Papers that focus on comparing imposition of lockdowns at different times excluded</strong> <br />
The main rationale behind this is that the authors were seeking to compare the lockdowns that were actually imposed with a counterfactual in which the lockdowns were not imposed, not a counterfactual in which they were imposed at a different time.<br /><br />You might say, however, that it is still worth doing the latter comparison because maybe it will tell us whether lockdowns are worthwhile if we were just able to get the timing right. However, I have pretty much zero confidence in our ability to precisely time the imposition of lockdowns, and I think I the worldwide track record in the last two years backs me up on that.</p>
  </li>
</ul>

<h3>  Quality measures </h3>
<p>Ok, so that’s how the authors narrowed it down to their chosen 34. That’s not the end of the story though. They also rated each of these studies according to the following four measures of quality:</p>

<ul>
  <li>
    <p><strong>Published versus working papers</strong> <br />
You didn’t think they were going to ignore peer-review completely, did you? <br /><br />I do think it is reasonable to put more trust in papers that have been peer-reviewed. So no qualms with this one.</p>
  </li>
  <li>
    <p><strong>Long verus short term</strong> <br />
Papers that have data that extends beyond 31st May 2021 get an extra merit point from the authors. The rationale here is that if lockdowns ‘flattern the curve’ but do not prevent deaths, then studies that are cut short may conclude that lockdowns are effective against death when in fact all they may be doing is slightly prolonging death.<br /><br />I also think this is reasonable.</p>
  </li>
  <li>
    <p><strong>Studies that have an effect on mortality sooner than 14 days after the intervention</strong> <br />
The idea here is that lockdowns presumably affect mortality by curbing transmission. Since it typically takes at least a few weeks between infection and death for those who die from COVID, any study that sees an early effect on mortality is likely to be flawed.<br /><br />I’m on board with this.</p>
  </li>
  <li>
    <p><strong>Social sciences versus other sciences</strong> <br />
Ok this is where it gets a bit sketchy. The authors’ rationale is that social scientists have greater expertise in evaluating policy interventions compared to those in the natural sciences. What this effectively means is that studies carried out by economists get a merit point, and studies carried out by e.g. epidemiologists don’t.<br /><br />I think this is questionable. While I recognise there certainly are biases in different fields, this just doesnt sit well with me.</p>
  </li>
</ul>

<h3>  The results </h3>
<p>These measures of quality were used to do stratified analyses. So, they carried out their analysis on papers that scored 4 out of 4 on the above criteria, then a separate analysis for the papers that scored at least 3, etc.<br /><br />The authors carry out an inverse-variance weighted meta-analysis, which is just a fancy type of weighted average of the results in the selected papers. It means that studies that were able to more precisely estimate the effect of lockdown on mortality get given more weight. Their main results are as follows:</p>

<ul>
  <li>
    <p><strong>Stringency-based studies</strong> <br />
These are studies that used the <a href="https://ourworldindata.org/" target="_blank">our world in data</a> <a href="https://ourworldindata.org/metrics-explained-covid19-stringency-index" target="_blank">stringency index</a> to measure how strict lockdown measures were. In their meta-analysis of these studies, the authors find that lockdowns resulted in a 0.2% reduction in mortality in Europe and the United States. There were only seven papers in this analysis, and one of them receives almost all the weighting.</p>
  </li>
  <li>
    <p><strong>Shelter-in-place-order studies</strong> <br />
The authors found that shelter-in-place orders reduced mortality by 2.9% in Europe and the United States. Again most of the weight goes to the result from one paper.</p>
  </li>
  <li>
    <p><strong>Specific NPIs</strong> <br />
Amongst papers that scored a perfect 4 on the quality criteria, the authors found a 34% reduction in mortality for the use of face masks, and 2.9% reduction for business closures. Once again, most of weight appears to come from single studies.</p>
  </li>
</ul>

<h3>  The verdict </h3>

<p>Once can find a selection of papers in the literature on lockdowns that support pretty much any inclusion one wishes to come to. Thus most criticism of this meta-analysis has rightly focused on whether the selection criteria employed by the authors are reasonable.</p>

<p>I think they are mostly kosher. The one that I take greatest issue with is the exclusion of papers written by people from the natural sciences. It is a little difficult though, because I do think there is a prevailing orthodoxy amongst epidemiologists that errs strongly on the side of interventions that seek to prevent deaths whose proximate cause is COVID, while neglecting any other side effects those interventions may have, no matter how catastrophic. Economists, on the other hand, tend to orient themselves towards broader, utilitarian measures of societal welfare.</p>

<p>At the intutitive level, I would not be surprised if lockdowns had minimal effect on COVID mortality. The mechanism through which they were meant to work was ‘flattening the curve’, thus preventing healthcare services from being overwhelmed. That didn’t happen, and looking at other countries that didn’t have as stringent lockdowns, it seems like we wouldn’t even have come particularly close.</p>

<p>Another interesting point that the furore over this paper illustrates is the huge schism between economists and public health professionals. It’s quite amazing how two well-developed wings of the academy can be so deeply in disagreement over some rather fundamental items. It’s reminiscent of the chasm that exists between biologists and the humanities on ideas like the <a href="https://en.wikipedia.org/wiki/Tabula_rasa#Science" target="_blank">tabula rasa</a> theory of human nature.</p>

<p>One thing is for sure; if the economists were in charge of the governmental response to the pandemic, things would have looked <em>a lot</em> different.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[09 Feb 2022]]></summary></entry></feed>