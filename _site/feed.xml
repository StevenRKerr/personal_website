<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2025-02-16T23:00:58+00:00</updated><id>/feed.xml</id><entry><title type="html">Does citation count measure how many friends you have?</title><link href="/2025/02/16/Does-citation-count-measure-how-many-friends-you-have.html" rel="alternate" type="text/html" title="Does citation count measure how many friends you have?" /><published>2025-02-16T00:00:00+00:00</published><updated>2025-02-16T00:00:00+00:00</updated><id>/2025/02/16/Does%20citation%20count%20measure%20how%20many%20friends%20you%20have</id><content type="html" xml:base="/2025/02/16/Does-citation-count-measure-how-many-friends-you-have.html"><![CDATA[<p>16 Feb 2025</p>

<p>Alright folks, I’ll fess up right off the bat; I may have fallen face first into <a href="https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines" target="_blank"> Betteridge’s law</a></p>

<blockquote>
Any headline that ends in a question mark can be answered by the word no.
</blockquote>
<p><br />
Leaving aside the empirical support for this adage, it is true that the answer to the eponymous question is ‘no’. But, I’m trying to provoke discussion here alright. Gimme a break.</p>

<p>The lackadaisicalness of paper authorship policies varies quite a bit across different fields of academia. I’ve worked in several, and I will at least say this. Disciplines that are 
 a little further along the spectrum tend to be much more solitary affairs. The number of authors on your typical physics paper is pretty low; on your typical life sciences
 paper, much higher. There’s a clear positive relationship between the number of authors on a paper and how long they will spend making eye contact with you in a 
 conversation.</p>

<p>It is, however, not uncommon across academia for authorship standards to be pretty much non-existent. For example, a small number of people will do all the work, and then
  send the paper to dozens of others for ‘comments and feedback’. It’s possible to get your name on a paper with just a few mins of light exertion. That’s less than your average 
  <a href="https://www.youtube.com/watch?v=o-50GjySwew" target="_blank"> prancercise workout</a>.</p>

<p>Closely related is the `reciprocal co-authorship’ arrangement: Person A lets person B be a co-author on their paper with minimal contributions, with an unspoken
understanding that the favour will be returned at some date in the future.</p>

<p>One consequence of this is that there are a non-trivial number of researchers whose paper and citation count is, at first sight, utterly baffling. In the field 
I currently work in - health data science - it typically takes at least a year and often several to write a paper. This includes going through the extremely tedious process of 
acquiring permissions to access data, writing protocols, obtaining ethics approvals, etc. Then you have to do the analysis, get feedback, tweak it, etc for multiple rounds. 
After that comes actually writing the paper, followed by more rounds of tweaking. Submit it, get rejected a few times before it finally gets accepted somewhere, but not before at least 
one round of peer review where you often have to basically write another paper defending the first paper.</p>

<p>To put it straightforwardly, there is simply no way one person can make really signficant contributions to this workload for a large number of papers. And yet, there are researchers 
whose yearly paper count is in the high 10s or even 100s.</p>

<p>There is only really one way that this can happen, excepting outright academic fraud. These are people who have lots of friends, and lots of mutual back-scratching arrangements.</p>

<p>It only makes sense. Academics live and die by their publications. If it is possible to get your name on a paper with a tiny fraction of the effort it would require to actually do 
the bulk of the work yourself, then people are going to take advantage of that, some in a really quite cynical way. Practices like those above are so commonplace, I would say that 
in some fields it is pretty much the norm for everyone except the first and last authors on a paper to be ignored. It is fairly widely accepted that anyone in the middle is a tag-along.</p>

<p>This brings me to my next adage, known as <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law" target="_blank"> Goodhart’s law</a>,</p>

<blockquote>
When a measure becomes a target, it ceases to be a good measure.
</blockquote>
<p><br />
This one will be all too familiar to many people in academia. Individuals whose citation count, in the 10s of thousands, or even in excess of 100,000, that massively exceeds their talent. The 
kind of people who are experts at ‘playing the game’. The ones who talk a lot during meetings without actually saying anything, who have a penchant for inserting themselves into other people’s projects. Hierarchy inversions that border on comical, where the lowly but honest stats/code monkeys have pitiful citation counts, and yet their knowledge and skill 
absolutely dwarfs a hot air bag with a monumental citation count.</p>

<p>It’s a little depressing, but I don’t have a good solution. Just a word of caution to be wary of citation count as a metric of academic contribution. Often it is measuring something
else entirely.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[16 Feb 2025]]></summary></entry><entry><title type="html">Academia fails the smell test</title><link href="/2025/01/07/Academia-fails-the-smell-test.html" rel="alternate" type="text/html" title="Academia fails the smell test" /><published>2025-01-07T00:00:00+00:00</published><updated>2025-01-07T00:00:00+00:00</updated><id>/2025/01/07/Academia%20fails%20the%20smell%20test</id><content type="html" xml:base="/2025/01/07/Academia-fails-the-smell-test.html"><![CDATA[<p>07 Jan 2025</p>

<p>I’m a bit behind the outrage news cycle here, but In November last year, a student at the University of Cambridge by the name of Amelia (Ally) Louks unintentionally created quite the stink over her PhD thesis entitled “The Politics of Smell in Modern and Contemporary Prose”. It all started with a seemingly innocuous  <a href="https://x.com/DrAllyLouks/status/1861872149373297078" target="_blank"> twitter post</a> celebrating successful completion of her degree, and quickly spiralled into a furore that “broke the internet”.</p>

<p>Why? To put it gently, her thesis looks like garbage. For example, the second sentence of the abstract lays out her aim to</p>

<blockquote>
...offer an intersectional and wide-ranging study of olfactory oppression by establishing the underlying logics that facilitate smell's application in creating and subverting gender, class, sexual, racial and species power structures.
</blockquote>
<p><br /></p>

<p>For most people that fails the sniff test. The stench of pseudointellectual equine fecal matter is frankly overpowering. Surely only a particularly niche brand of masochist would be interested in subjecting themselves to what is no doubt the ordeal of hundreds of pages of this.</p>

<p>Which leaves many people wondering; how on Earth can someone get a PhD from one of the pre-eminent higher education institutes in the world by producing what appears to be an elaborately curated, heaping monument to obscurity and the unmistakable scent of pasture?</p>

<h3> Fashionable nonsense </h3>

<p>I share the disheartenment of the internet. In a sane world, this document would have been ignored by everyone as the esoteric and pointless ramblings of someone who has the comfort and privilege of being able to waste their time on such trivialities. It should not result in a doctoral degree from one of the most prestigious institutions on the planet.</p>

<p>Unfortunately, Louks’ thesis is not some isolated incident. It is just the tip of the iceberg. There are mountains of garbage just like it in academia. Entire industries are dedicated to producing this garbage.</p>

<p>How did this happen? Part of it is an insistence in many quarters on treating humanities subjects e.g. English literature, anthropology, gender studies, art etc as if they are on a par with the natural sciences - physics, chemistry, biology etc. The problem is that they are fundamentally different. In the natural sciences, there are real discoveries to be made, and real research to be done. There is knowledge that require serious time and effort to master, and it can take the better part of a lifetime to make a notable contribution to these fields. And no one can argue about the results; the achievements of science are absolutely extraordinary.</p>

<p>In the humanities, there is not much of any of that. Nonetheless there is a hierarchy to be constructed - somebody must, after all, be the leading gender studies scholar in the world. In the absence of the potential for real scientific competence or truth-seeking, that hierarchy becomes predicated on something else entirely - one’s ability to dress up mundane or silly ideas in a way that superficially makes them sound just as impressive as actual science. Noam Chomsky nails it in <a href="https://youtu.be/OzrHwDOlTt8?t=343" target="_blank"> this clip</a>,</p>

<blockquote>
Suppose you're a literary scholar at some elite university, or anthropologist or whatever. If you do your work seriously, that's fine. But you don't get any prizes for it. On the other hand, you take a look at the rest of the university, and you've got these guys in the physics department, and the math department, and they have all kind of complicated theories, which of course we can't understand, but they seem to understand them, and they have principles and they deduce complicated things from the principles, and they do experiments and find either they work or they don't work. Thats really impressive stuff, so I want to be like that too. I want to have a theory. 
<br />
<br />
In the humanities, literary criticism, anthropology and so on, theres a field called theory. We're just like the physicists. They talk incomprehensively, we can talk incomprehensively. They have big words, we'll have big words. They draw far-reaching conclusions, we'll draw far-reaching conclusions. We're just as prestigious as they are. Now if they say look well look we're doing real science and you guys aren't, that's white, male, sexist, bourgeois, whatever the answer is, how are we any different from them? Ok that's appealing.
</blockquote>
<p><br /></p>

<p>It really is that simple. And depressing.</p>

<p>All of this was exposed quite brilliantly by Alan Sokal in 1996, who has been a professor of physics at New York University and University College London. He submitted a fake article to the journal Social Text entitled “Transgressing the Boundaries: Towards a Transformative Hermeneutics of Quantum Gravity”, arguing that quantum gravity is a social construct. Needless to say, the paper was filled with nonsense. It nonetheless got published. In 1998, he wrote the book 
<a href="https://en.wikipedia.org/wiki/Fashionable_Nonsense" target="_blank"> Fashionable nonsense</a> criticising the postmodern academic tradition that resulted in the unfortunate institutional circumstances that allow this sort of thing to happen.</p>

<p>This was followed up by the <a href="https://en.wikipedia.org/wiki/Grievance_studies_affair" target="_blank"> Grievance studies affair</a> in 2017 and 2018, by Peter Boghossian, James Lindsay and Helen Pluckrose. They wrote 20 articles filled with patently absurd nonsense and submitted them to various peer-reviewed journals. By the time the hoax was revealed in October 2018, 4 papers had been published, 3 were accepted but not published, 6 were rejected and 7 were under review. The published papers included claims that dogs participate in rape culture and that men can reduce their transphobia by using anal sex toys on themselves.</p>

<p>Further evidence of the rot comes from <a href="https://en.wikipedia.org/wiki/Judith_Butler" target="_blank"> Judith Butler</a>, who is the Maxine Elliot professor in the department of comparative literature and the program of critical theory at the University of California, Berkeley. That is a named professorship at one of the most elite universities in the world. In her most famous work, <a href="https://en.wikipedia.org/wiki/Gender_Trouble" target="_blank"> Gender trouble</a> she argues that gender is not innate, but a performance. Which, depending on how you define gender, is either a truism or an absurdity. Butler is famous for her crimes against the English language, illustrated nicely with this literal award-winningly awful sentence</p>

<blockquote>
The move from a structuralist account in which capital is understood to structure social relations in relatively homologous ways to a view of hegemony in which power relations are subject to repetition, convergence, and rearticulation brought the question of temporality into the thinking of structure, and marked a shift from a form of Althusserian theory that takes structural totalities as theoretical objects to one in which the insights into the contingent possibility of structure inaugurate a renewed conception of hegemony as bound up with the contingent sites and strategies of the rearticulation of power.
</blockquote>
<p><br /></p>

<p>This leads me to what I call <strong>The Butler Challenge</strong>™: Choose <strong>any</strong> book by Judith Butler, flip to <strong>any</strong> page, choose <strong>any</strong> paragraph, and find me a sentence that isn’t complete gibberish. I offer a handsome bounty to anyone who succeeds in this daring endeavour.</p>

<h3> State-sponsored ideological nonsense </h3>

<p>If this was all just blather done in the name of light entertainment that would be one thing. It is altogether another thing when the humanities departments at universities are mass indoctrinating students with a particular ideology and sending them out in the world with revolutionary intentions. And on taxpayers’ dime to boot. Tens of millions of pounds are spent funding anti-scientific “research” in univerities. This is stuff that is on a par with flat earth theory, except far more socially destructive. That is unacceptable.</p>

<p>Unfortunately this shows no signs of abating anytime soon.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[07 Jan 2025]]></summary></entry><entry><title type="html">The trouble with economics</title><link href="/2024/10/02/The-trouble-with-economics.html" rel="alternate" type="text/html" title="The trouble with economics" /><published>2024-10-02T00:00:00+01:00</published><updated>2024-10-02T00:00:00+01:00</updated><id>/2024/10/02/The%20trouble%20with%20economics</id><content type="html" xml:base="/2024/10/02/The-trouble-with-economics.html"><![CDATA[<p>02 Oct 2024</p>

<p>In this post, I’m going to give an overview of some major theoretical issues in modern economics.</p>

<h3> Supply and demand </h3>
<p>In a typical undergraduate economics degree, you spend a lot of time drawing pictures like this:</p>

<div style="text-align: center;">
    <img src="../../../images/supply_demand.png" width="500" height="500" class="center" />
</div>

<p>I don’t think its an exaggeration to say that a major chunk of the syllabus is pretending these diagrams are deeper and have far more explanatory power than they really do. It is, after all, just a graph with two lines.</p>

<p>The basic idea is that when prices go up, demand goes down and supply goes up. The two lines have to meet somewhere, and this is the price that clears the market. Under ‘normal’ circumstances, this is the price that actually prevails (or so the story goes).</p>

<h3> General equilibrium theory </h3>

<p>By the time you get round to doing a masters in economics, you learn the grown up version of supply and demand diagrams - it goes by the name general equilibrium theory. Here there are many goods that are traded concurrently. Each person has preferences and a budget. A major theoretical result in economics is the proof, under fairly general assumptions, that there is always at least one set of prices that clears the market.</p>

<p>General equilibrium theory is the theoretical foundation for modern macroeconomic modelling. Pretty much every mainstream macroeconomic model is either an example of a general equilibrium model, or it is derived from one. This includes models that are routinely used by major financial institutions like central and commercial banks, government treasuries etc for economic forecasting. For example, the ubiquitous <a href="https://en.wikipedia.org/wiki/Dynamic_stochastic_general_equilibrium" target="_blank"> ‘dynamic stochastic general equilibrium’</a> models.</p>

<p>The trouble with general equilibrium theory is that it has features baked in that are in spectacular disagreement with reality.</p>

<h4> No money </h4>

<p>In general equilibrium theory, there is no money. You might be puzzled by this - how can it be that the foundational macroeconomic modelling strategy of major financial institutions throughout the world does not have money? Is money not a central focus of the entire subject of economics?</p>

<p>Well yes, it is, but turns out it’s a lot easier to make models without it. First consider the useful role that money serves</p>

<ul>
  <li> <b>Medium of exchange </b>
    <p>This is perhaps the primary and most important function of money - facilitating the exchange of goods. Without money, we would have a bartering economy in which a great deal of time and effort would be spent engaging in chains of trades that have the desired outcome. For example, if I have a cow and I would like to get a sack of potatoes, I would either have to find someone who has potatoes and wants a cow, or find a series of similar trades that eventually ends with my desired result. </p>
  </li>
  <li> <b>Store of value </b>
    <p>Money provides a means through which you can e.g. provide labour now in exchange for consumption later. For example, you can go to work tilling the fields, get paid into your bank account, then later use that money to buy a four-slice toaster. </p>
  </li>
  <li> <b>Standard of deferred payment </b>
    <p>This is closely related to the function of money as a store of value, but in reverse. That is, you can get consumption now in exchange for work later. This is achieved by borrowing money to fuel today's consumption, and paying back at a future date.</p>
  </li>
  <li> <b>Unit of account </b>
   <p>Finally, money provides a measuring stick for value. If a good can be sold for £1, we have a numerical measure of its value relative to other goods.
</p>
  </li>
</ul>

<p>Any model that includes money must capture all of these functions.</p>

<p>In general equilibrium theory, there is a set of market-clearing prices that certainly serve as a unit of account. But none of the other much more important functions of money are captured at all.</p>

<p>In particular, money does not serve as a medium of exchange. Prices that clear the market are found, then all the goods are magically redistributed in such a way that the value of the bundle each person receives is exactly equal to the value of the bundle they give up. In the real world, this would obviously be accomplished by carrying out all transactions with bits of paper we call money, where the number of pieces of paper required to buy a good is  equal to its equilibrium price. This can happen because we have a society-wide agreement to treat the bits of paper as if they are valuable. This can never happen in general equilibrium theory - there is no room in the model for social constructs or government fiat. And in their absence, why would anyone accept useless bits of paper in exchange for real, intrinsically useful goods? This also implies the equilibrium price of money is zero, rendering it useless as a store of value or a standard of deferred payment.
The result is that general equilibrium theory is really a theory of frictionless, money-free bartering. Which, you may have noticed, is somewhat lacking in realism.</p>

<h4> No unemployment </h4>

<p>In general equilibrium theory, market-clearing prices always prevail. Excess supply or demand is assumed away - it simply cannot happen. That is rather unfortunate because unemployment, perhaps the central quantity of interest in macroeconomic forecasting and policymaking, is a failure of labour markets to clear. In general equilibrium theory, unemployment simply does not exist.</p>

<h4> No price-setting </h4>
<p>In general equilibrium theory, no one sets prices. They are simply sent down as manna from heaven. Everyone calculates what bundle they would buy at every possible set of prices, and divine intervention selects a set of prices that clears the market. At no point does it occur to anyone that they might be able to influence prices.</p>

<h4> No strategy</h4>
<p>General equilibrium theory is not a game. Agents in the model behave as if choosing from a menu of options in complete isolation from everyone else. There is no notion that anyone else’s behaviour might affect your payoff. This is obviously completely at odds with the real world, where firms ruthlessly strategise against each other to maximise their profits.</p>

<p>These are just the failings of general equilibrium theory. However, standard macroeconomic models are in many ways even worse, because they make simplifying assumptions that even more thoroughly divorce the model from reality. In particular, they typically model the economy of an entire country as if it contains exactly one person, one firm, and a handful of goods that are traded. In case that didn’t strike you as obviously doomed to failure, there are theorems indicating you just can’t do this (see the <a href="https://en.wikipedia.org/wiki/Gorman_polar_form" target="_blank"> Gorman aggregation theorem</a>, the <a href="https://en.wikipedia.org/wiki/Cambridge_capital_controversy" target="_blank"> Cambridge capital controversy</a>).</p>

<h3> To summarise</h3>
<p>Standard macroeconomic models that are routinely used by major financial institutions to model entire nations are typically based on frameworks that do not have money and operate through frictionless bartering, where there is no unemployment, sellers cannot choose what price they offer to sell at, there is no strategic considerations, there is precisely one consumer and one firm, and only a handful of goods are traded. And many economists seem to be pretty satisfied with this state of affairs.</p>

<p>Yes, I find it hard to believe too.</p>

<p>When I worked in economics, I was most certainly not happy with this state of affairs. I came up with an alternative framework that in principle fixes many or all of these problems. What I quickly discovered, though, is that for various reasons my work was virtually unpublishable in mainstream economics journals. That is, however, a story for another day.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[02 Oct 2024]]></summary></entry><entry><title type="html">I for one welcome our AI overboard</title><link href="/2024/07/11/I-for-one-welcome-our-AI-overboard.html" rel="alternate" type="text/html" title="I for one welcome our AI overboard" /><published>2024-07-11T00:00:00+01:00</published><updated>2024-07-11T00:00:00+01:00</updated><id>/2024/07/11/I%20for%20one%20welcome%20our%20AI%20overboard</id><content type="html" xml:base="/2024/07/11/I-for-one-welcome-our-AI-overboard.html"><![CDATA[<p>11 Jul 2024</p>

<p>I’ve been very reticent to comment on the current AI hype train, mostly because it’s very easy to be wrong in a bull market. But you will be relieved to know I am now ready to put my hat in the ring.</p>

<p>First I would like to say that, like everyone else, I’m deeply impressed by recent advances in AI, particularly large language models (LLMs). They can give you flawless explanations of topological quantum field theory in one breath, then seamlesly switch to a nuanced discussion of the history of cartography. Tailor-made answers to your burning questions on heraldry, hermeneutics, object-oriented programming, epistemology, and just about any other domain of expert human knowledge your heart desires. They’re really quite amazing. I would be extraordinarily proud if I had been part of the teams that brought LLMs to the world.</p>

<p>But in the wake of the current wave of exuberance, I feel compelled to point out the manifold ways in which LLMs are extremely dumb. Here is but a partial list of tasks that state-of-the-art LLMs have failed at spectacularly:</p>

<ul>
  <li>Basic arithmetic, e.g. multiplying together two 3-digit numbers.</li>
  <li>Understanding relations, e.g. if X is Y, then Y is X. If you ask ‘Who is Olaf Shhulz?’, you can correctly be told that he is the 9th Chancellor of Germany, and then immediately be given a different answer to the question ‘Who is the 9th Chancellor of Germany?’.</li>
  <li>Creating a 3x3 word search containing specified 3-letter words.</li>
  <li>Name three famous people who all share the exact same birth date and year.</li>
  <li>Counting e.g. the numbers of letters in a string, number of paragraphs in a document, etc.</li>
  <li>Simple logical problems, with uncommon variations. For example, <a href="https://en.wikipedia.org/wiki/River_crossing_puzzle" target="_blank"> river-crossing problems</a>, but with no restrictions on which which items can be taken or left together.</li>
  <li>Spatial reasoning. For example, ‘Four children - Alex, Bella, Charlie, and Dana - are sitting around a picnic table. Alex is facing Bella. Charlie is sitting to the right of Bella. Who is sitting to the left of Alex?’.</li>
</ul>

<p>You can find a more in depth exploration of various LLM fails in the appendix of <a href="https://arxiv.org/pdf/2405.19616" target="_blank"> this paper</a>.</p>

<p>What these examples indicate is that LLMs don’t have any understanding of the world that is in any way similar to what you or I have. For example, if you give me the word ‘chair’, it brings to my mind a lifetime’s worth of relevant physical, sensory information. I know what they look and feel like. I know the precise purpose they serve, having experienced the discomfort of having to stand for prolonged periods of time. I know that chairs can be used in all sorts of ways that have little or nothing to do with their intended purpose, because I have actually done it before (wink wink). LLMs have none of this. What they have is, loosely speaking, a bunch of words that they associate with chairs from being exposed to internet text in which the word chair appears.</p>

<p>What this means is that I understand the concept of a chair and its manifestation in the real world in a way that guarantees I’m not going to make obvious, stupid errors that LLMs sometimes make. I can also exhibit chair-related creativity, like finding novel uses that you won’t find written about anywhere on the internet (nudge nudge), because I have an understanding of the physical reality of a chair that is more than just a cloud of associated words.</p>

<p>In short, LLMs may very well be <a href="https://en.wikipedia.org/wiki/Stochastic_parrot" target="_blank"> stochastic parrots</a>. To the extent they give good answers, it is only because human experts have provided good answers to a similar question somewhere on the internet that the LLMs have cobbled together. In a very real sense, you might think of LLMs as souped-up versions of search engines that actually construct a tailor-made, friendly prose answer to your particular query instead of making you spend hours piecing it all together from a collection of top hits from stack exchange, academic papers and obscure blog posts. Or, as Noam Chomsky bluntly put it, ‘high-tech plagiarism’.</p>

<p>Another way to think of an LLM is a smart 10 year old that has read and memorised a textbook on quantum mechanics. If you ask them typical questions about the subject, they may be able to give you impressive-sounding answers. Hell, they might even be able to convince you they are a child prodigy. But they have zero understanding of the subject, a fact which will manifest itself as soon as you venture even a little bit outside of what can be constructed by placing roughly the right words in roughly the right order in a superficial act of mimicry.</p>

<p>In light of this, I find it very difficult to take seriously any notion that we are blindly speeding into an AI-driven apocalypse. I recall seeing an online exchange where one person was taking as a sure harbringer of our impending doom the idea that LLMs were on the brink of being able to consistently perform arithmetic. To which they received the rather acerbic reply <em>‘So can my fu**ing calculator’</em>. Well, quite. And the calculator doesn’t need billions of parameters, hundreds of talented young minds, the entire text of the internet and the power supply of a small country to create.</p>

<p>The most popular version of the AI apocalypse scenario goes something like: we will soon create <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" target="_blank"> artificial general intelligence (AGI)</a> that exceeds human capabilities across a wide range of cognitive tasks. This will kick-off a chain of recursively improving AIs, ultimately leading to a superintelligence that will be able to outwit and outperform all of humanity across the board, leaving us utterly at its mercy. If this superintelligence happens to have an objective that is not carefully aligned with the interests of humans, it could all go seriously tits up. Take the so-called paperclip maximiser, dreamt up by philosopher Nick Bostrom,</p>

<blockquote>
Suppose we have an AI whose only goal is to make as many paper clips as possible. The AI will realize quickly that it would be much better if there were no humans because humans might decide to switch it off. Because if humans do so, there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into paper clips. The future that the AI would be trying to gear towards would be one in which there were a lot of paper clips but no humans.
</blockquote>

<p><br />
Are you scared yet folks?</p>

<p>Well no, not really. To me, this stuff just sounds like the fevered, masturbatory fantasies of sex-starved sci-fi fanboi man-children. In fact, that description might be eerily close to the truth. Exhibit A: Bostrom himself. Exhibit B: Elizier Yudkowsky, leading AI-doomer, and a man who thinks it’s a positively spiffing idea for superpowers to <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/" target="_blank"> conduct pre-emptive military strikes on each others’ AI labs, running the risk of precipitating a nuclear holocaust, in order to prevent the development of AGI</a>. I suppose the hundreds of millions of lives that would be lost is a small sacrifice at the altar of his very serious sci-fi fantasies. Incidentally, he’s the author of <a href="https://en.wikipedia.org/wiki/Harry_Potter_and_the_Methods_of_Rationality" target="_blank"> Harry Potter and the Methods of Rationality</a>, which is basically a sci-fi reimagining of Harry Potter. These people are clearly drawing heavily on a literary tradition that has enthralled us for many decades to weave their apocalyptic tales.</p>

<p>Given that cutting edge LLMs are recommending people <a href="https://news.sky.com/story/glue-cheese-to-pizza-and-eat-rocks-says-googles-new-ai-feature-as-mistakes-flood-social-media-13142528" target="_blank"> eat rocks as part of a healthy diet and glue cheese to pizzas to prevent slippage</a>, the latter based purely on the shitposting of a presumed teenage internet edgelord, I think we can confidently say we are safe for the time being. More generally though, the AI technology singularity scenario requires a long chain of reasoning, each step of which is, to put it gently, <em>extremely</em> speculative. Maybe it will take 50 more quantum leaps, each of a similar magnitude to that which brought us LLMs, to get to AGI. And there is nothing inevitable about each step of the putative self-improving AI explosion. Who knows if any of this is even possible, or if it is, whether it will happen. Maybe there are hard limits to what can be done, not least of which are the tremendous amount of energy and data that appear to be required to train these AIs.</p>

<p>The whole thing sounds like a big <a href="https://en.wikipedia.org/wiki/Conjunction_fallacy" target="_blank"> conjunction fallacy</a> to me. That’s when a specific proposition is thought to be more likely than a more general proposition of which it is a special case. For example, consider the following experiment carried out by Amos Tversky and Daniel Kahnemann where they asked a group of policy experts to rate the probability of the following events:</p>

<ol>
  <li>Russia invades Poland</li>
  <li>Russia invades Poland and the US breaks diplomatic relations with them the following year.</li>
</ol>

<p>They collectively assigned a 1% chance to option 1, and a 4% chance to option 2. This, of course, cannot be the case since 2 is a special case of 1. What might be going on here is that the participants find it difficult to imagine a scenario in which Russia invades Poland and thus assign it lower probability than a more detailed scenario that is more representative of what a real scenario would look like.</p>

<p>And so it is with AI-doomer fantasies. Most people didn’t spend the requisite amount of time in their parents basement during adolescence to be able to magick up the paperclip maximiser from the depths of their imagination. But given a fantastical sci-fi inspired story explaining one way it <em>could</em> happen they start to wildly overestimate its probability.</p>

<p>I find this fixation all particularly hard to swallow given that at this particular moment in history, we may well be on the brink of global nuclear war. Nuclear weapons pose a far bigger and proven threat to humanity that we have basically been ignoring for decades. Anyone who thinks otherwise should read the story of <a href="https://en.wikipedia.org/wiki/Vasily_Arkhipov" target="_blank"> Vasily Arkhipov</a>, the razor-thin, one-man margin that stood between us and nuclear annihilation back in 1962. There have been several more close calls since then too. Let’s be clear about this: <strong>The only reason the nuclear armageddon hasn’t happened yet is pure luck</strong>. The USA may be about to lose its position as global hegemon, and shows no indication of going quietly or peacefully. Countries around the world are increasing military spending and reintroducing the draft. Sorry folks, but I have bigger fish to fry than a damn paperclip maximiser.</p>

<p>My prediction for AI is that it will lead to disruption in some industries, and some handsome payoffs in increased efficiency. The AI doomers will continue their fearmongering, and for them AGI will be like fruit above Tantalus - always within reach, never in our hands.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[11 Jul 2024]]></summary></entry><entry><title type="html">Understanding case control studies</title><link href="/2024/06/04/Understanding-case-control-studies.html" rel="alternate" type="text/html" title="Understanding case control studies" /><published>2024-06-04T00:00:00+01:00</published><updated>2024-06-04T00:00:00+01:00</updated><id>/2024/06/04/Understanding%20case%20control%20studies</id><content type="html" xml:base="/2024/06/04/Understanding-case-control-studies.html"><![CDATA[<p>04 Jun 2024</p>

<p>Case control studies are a staple in the epidemiologist’s toolbox. The basic idea idea is that instead of carrying out an analysis on a random sample of a population, you instead sample from the population conditional on occurrence of an outcome of interest. Typically this will involve selecting all individuals who have the outcome (the cases), and only some people who don’t have the outcome (the controls). The motivation for doing this is that when the outcome is rare, most of the precision in estimates is driven by the cases, and adding a large number of controls often has little effect. Case control studies can thus be more efficient in terms of data collection/processing compared to e.g. a cohort study.</p>

<p>Case control studies have been instrumental in establishing many useful research findings, such as the link between smoking tobacco and lung cancer. Despite this, the exact workings of case control studies are poorly understood by many professionals who use them routinely. I recently co-authored an article in the Journal of Global Health with world-leading statistician and epidemiologist <a href="https://en.wikipedia.org/wiki/Sander_Greenland" target="_blank"> Sander Greenland</a> that attempts to clear up the confusion that persists over this study design.<a href="#1">[1]</a></p>

<h3> Outcome and exposure odds ratios </h3>

<p>Consider the following table that is basically a stock character in epidemiology text books.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: center"><u><strong>Exposed </strong></u></th>
      <th style="text-align: center"><u><strong> Unexposed </strong></u></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right"><u><strong>Outcome event </strong></u></td>
      <td style="text-align: center">a</td>
      <td style="text-align: center">b</td>
    </tr>
    <tr>
      <td style="text-align: right"><u><strong>No outcome event </strong></u></td>
      <td style="text-align: center">c</td>
      <td style="text-align: center">d</td>
    </tr>
  </tbody>
</table>

<p>The entry in each cell denotes the number of people in each exposure/outcome category in a closed cohort. Using this table, we get</p>

\[\begin{align}
\mathrm{Odds}(O=1 | E=1) &amp;= \frac{ \left(\frac{a}{a+c} \right) }{ \left(\frac{c}{a+c} \right) } = \frac{a}{c} \label{odds_e} \tag{1}\\
&amp; \\
\mathrm{Odds}(O=1 | E=0) &amp;= \frac{ \left( \frac{b}{b+d} \right) }{ \left( \frac{d}{b+d}  \right) } = \frac{b}{d}, \label{odds_u} \tag{2}
\end{align}\]

<p>where \( O \) and \( E \) are binary outcome and exposure indicator variables respectively. This gives the outcome odds ratio as</p>

\[\begin{align}
OR  = \frac{\mathrm{Odds} (O=1 \| E=1)}{\mathrm{Odds} (O=1 \| E=0) } = \frac{ad}{bc} \tag{3}.
\end{align}\]

<p>A fact that is often made much of in epidemiology text books is that if you repeat this calculation but instead looking at the odds ratio of <em>exposure</em> in cases compared to controls, you get exactly the same answer, \( \frac{ad}{bc} \). In fact, I have sometimes seen a stronger claim that case control studies <em>cannot</em> directly estimate the outcome odds ratio, but only the exposure odds ratio.</p>

<p>The idea behind this claim appears to be that in a case-control study, the dataset typically contains all the cases, but only some of the controls. Hence we do not know \( c \) or \( d \), and cannot estimate \( \ref{odds_e} \) or \( \ref{odds_u} \). On the other hand, when calculating the odds ratio of exposure, the probability of exposure among those who had the event and those who did not have the event can be estimated.</p>

<p>This is, however, completely immaterial. The ratio \( \frac{d}{c} \) can be estimated by dividing the number of people who are unexposed by the number exposed among the controls. Therefore the quantity \( \frac{ad}{bc} \) can also be estimated from the case control sample even though we don’t know \( c \) or \( d \). It’s irrelevant whether you want to call it an outcome odds ratio or an exposure odds ratio.</p>

<p>Unfortunately, this has generated some confusion. While the outcome and exposure odds ratios are theoretically equal, one would typically estimate them using logistic models for \(  P(O = o \; | \; E=e, X=x) \) and \(  P(E = e \; | \; O=o, X=x) \). These are related by Bayes theorem,</p>

\[\begin{align}
P(O = o \; | \; E=e, X=x) = \frac{  P(E = e \; | \; O=o, X=x) P(O=o, X=x) }{ P(E=e, X=x) }.  \label{bayes} \tag{4}
\end{align}\]

<p>It is extremely unlikely that logistic models will be consistent with this relationship because, as we all know, <a href="https://en.wikipedia.org/wiki/All_models_are_wrong" target="_blank"> all models are wrong</a>. Thus outcome and exposure odds ratios estimated in this way will in general not be the same. The choice of whether to take outcome or exposure to be the dependent variable in principle should be done on the basic of which model is better. In practice, there is an established tradition of the outcome being the dependent variable.</p>

<h3> Odds ratios or rate ratios? </h3>

<p>The second major source of confusion in case control studies is whether one should report an odds ratio or a rate ratio. On the face of it, you would think an odds ratio since case control studies typically use logistic regression. However, this odds ratio is an estimator for the parameter of a data generating process that includes the sampling we carried out for cases and controls. In other words, it is not an estimator for the odds ratio in the population.</p>

<p>On the other hand, there are some circumstances under which the odds ratio in a case control study is equal to the rate ratio in the population. These conditions are laid out in our paper <a href="#1"> [1]</a>, and also in Sander’s paper from the early 80s <a href="#2"> [2]</a>. In particular, this conclusion holds if the analysis uses risk set sampling to match cases with controls, the model includes strata terms for matched cases and controls, and the instantaneous rate ratio is constant over the study period. Risk set sampling means controls are randomly sampled from everyone who has not had an event at the time the of outcome for the index case. Proof of this can be found in <a href="#2"> [2]</a>.</p>

<p>Despite the fact that this has been known for over 50 years, there is still a great deal of confusion. In particular, there are a few papers out there that incorrectly claim that risk set sampling alone is sufficent for an odds ratio in the data generating process of a case control study to be equal to a rate ratio in the population.</p>

<p>Ultimately, it is a matter of ‘taste’ whether to report an odds ratio or rate ratio. Typically the entire motivation for estimating an odds ratio in a case control study is because it is also an estimator of a population parameter of interest - the rate ratio. On the other hand, it is not incorrect to report an odds ratio. What must be understood is that the odds ratio applies to a data generating process that includes the case control sampling, whereas the rate ratio applies to a data generating process for the whole population.</p>

<h3> Re-using data points? </h3>

<p>A final confusion that arises is that in risk set sampling, you can end up having the same individual as both a case and a control. This can happen if they are matched to another case at a time before they have had the event themselves. Surely that’s not allowed?</p>

<p>This can be resolved by understanding that the unit of observation in a case-control study is not an indivdual, but an individual <em>at a particular instant in time</em>. In other words, it’s as if each individual has a row in the data for every instant of follow-up time. That will generally result in an infinite number of observations, but the case control study only ever uses a finite subset of them. If an individual ends up having several of their observations included in the case control study, these should be understood as independent data points - or at least data points that are ‘sufficiently independent’ to satisfy the modelling assumptions that are made.</p>

<h3>  References </h3>

<p><span id="1"> <b>1. </b> </span> Kerr S, Greenland S, Rudan I et al. Understanding and reporting odds ratios as rate ratio estimates in case-control studies. Hournal of global health, 2023.</p>

<p><span id="2"> <b>2. </b> </span> Greenland S, Thomas DC. On the need for the rare disease assumption in case-control studies. Am J Epidemiol. 1982;116:547-53. Erratum in: Am J Epidemiol 1990 Jun;131(6): 1102.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[04 Jun 2024]]></summary></entry><entry><title type="html">The St Petersburg paradox</title><link href="/2024/04/24/The-St-Petersburg-paradox.html" rel="alternate" type="text/html" title="The St Petersburg paradox" /><published>2024-04-24T00:00:00+01:00</published><updated>2024-04-24T00:00:00+01:00</updated><id>/2024/04/24/The%20St%20Petersburg%20paradox</id><content type="html" xml:base="/2024/04/24/The-St-Petersburg-paradox.html"><![CDATA[<p>24 Apr 2024</p>

<h3> The St Petersburg game </h3>
<p>Imagine I offer you the chance to play the following game. The pot starts at \( $ 2\). I flip a coin repeatedly. Each time it comes up tails, the pot doubles. The first time a heads appears, the game ends and you leave with the pot. How much would you be willing to pay to play this game?</p>

<p>If you’re anything like a reasonable person, or <a href="https://en.wikipedia.org/wiki/Daniel_Bernoulli" target="_blank"> Daniel Bernoulli</a>, you’d be unlikely to pay more than \( $ 20\). At that price, you’d need at least 4 consecutive tails to make a profit.</p>

<p>According to some people, this would put you at odds with 300 years of conventional wisdom regarding uncertain outcomes. ‘Rational’ individuals are supposed to maximise expected value, and the expected value of this bet goes to infinity as the number of potential coin flips becomes large,</p>

\[\sum_{i=1}^n = \frac{1}{2} 2 + \frac{1}{4} 4 + \frac{1}{8} 8 + ... \frac{1}{2^n} 2^n \label{st petersburg}\\
= n.\]

<p>You should therefore be willing to hand over all your earthly possessions for a single shot at this game.</p>

<p>The tension between these two prescriptions is known as the <a href="https://en.wikipedia.org/wiki/ADM_formalism](https://en.wikipedia.org/wiki/St._Petersburg_paradox)" target="_blank"> St Petersburg paradox</a>.</p>

<h3> What is rational, anyway? </h3>

<p>Perhaps the main reason why it is nowadays believed that a ‘rational’ decision maker must maximise expected value or utility is the von Neumann Morgenstern (hereafter vNM) utility theorem, which I have written about <a href="https://drstevenkerr.com/2023/10/03/The-church-of-expected-utility-theory.html" target="_blank"> before</a>. To recap; the theorem says that anyone who behaves in accordance with four seemingly quite reasonable axioms must be an expected utility maximiser. They are as follows:</p>

<p><b>1. Completeness.</b> For any two lotteries \( L \) and \( M \), we have either \( L \succeq M \) or \( M \succeq L \)</p>

<p><b>2. Transitivity.</b> If there are three lotteries \( L \), \( M \) and \( N \), with \( L \succeq M \succeq N \), then \( L \succeq N \).</p>

<p><b>3. Continuity.</b> For any three lotteries with \( L \succeq M \succeq N\), there is some probability \( p \) such that \( pL + (1-p)N \sim M \).</p>

<p><b>4. Independence.</b> For any lottery \( M \) and any probability \( p \), \( L \succeq N \) if and only if \( pL + (1-p)M \succeq  pN + (1-p)M \).</p>

<p>Inquiring minds might wonder what exactly a lottery is. In von Neumann and Morgenstern’s original formulation, it’s actually a little bit vague. Subsequent work however clarified that lotteries are elements of some set of probability measures over a measurable space.</p>

<p>Another important but subtle point to note is that it is only proved that there exists a function over lotteries with the following property</p>

\[U(pL + (1-p) M) = p U(L) + (1-p)U(M).\]

<p>This implies that, for example, if you start off with an assignment of utilities to sure outcomes, then this can only be extended to lotteries where a finite number of outcomes have non-zero probability (density). These are sometimes called simple probability measures/lotteries. Later work extended the original vNM theorem to more general spaces of lotteries, although additional axioms need to be introduced.</p>

<p>The final subtlety to notice is that the vNM theorem, as well as various extensions, only demonstrate that for any individual who behaves in accordance with the above axioms, there is a <em>bounded</em> utility function over sure outcomes such that the expected utility represents their preferences over lotteries. Notice my emphasis on the word bounded.</p>

<h3> Resolving the paradox </h3>

<p>Daniel Bernoulli resolved the original form of the paradox shortly after it was first posed by his brother Nicolaus in 1713, and long before anyone knew about the vNM theorem. He noticed that the unbounded quantities of money you might receive in the St Petersburg game do not give unbounded ‘happiness’, or utility. If you restrict to bounded utilities, the paradox evaporates.</p>

<p>In the early 1900s, a newer form of the paradox appeared in which the payoffs are instead taken to be ‘utilities’ rather than money. The idea is that ‘pleasure’ can be measured in units in much the same way as physical quantities like distance can be measured in metres, and we can simply take a modified version of the original St Petersburg game in which the payoffs are units of pleasure that can become arbitrarily large. In some circles, this newer form of the paradox is still considered unsolved.</p>

<h3> The St Petersburg paradox 2.0 </h3>

<p>I have to say I’m fairly unimpressed with the modern incaration of the St Petersburg paradox. A careful reading of the vNM theorem and extensions indicates that there is absolutely no obligation for an individual who acts in accordance with its axioms to assign infinite utility to the St Petersburg game. The theorems simply guarantee that there is a <em>bounded</em> utility function that represents their preferences over sure outcomes. That utility function needn’t have an interpretation as measuring units of pleasure - it simply represents an ordering of outcomes. So, for example, if outcome \( A \) is assigned the value \( 1 \) and outcome \( B \) is assigned the value \( 2 \), it does not mean that \( B \) gives ‘twice as much pleasure’ as \( A \). It simply means that \( B \) is preferred to \( A \).</p>

<p>If you insist on an unbounded utility function that has an interpretation in terms of ‘units of pleasure’, then you are outwith the scope of the vNM theorem. I would suggest you’re probably outwith the scope of reality too. If it is even possible to sensibly measure units of pleasure, I would hazard that it’s not possible for a person, or a finite collection of people, to experience an infinite number of them. In just the same way that it’s not possible for anyone to travel an infinite distance or live for an infinite time. Infinities like this don’t occur in nature. The revamped St Petersburg paradox is asking us to make sense of something that isn’t physically possible. At which point we are completely justified in ignoring it, for the same reason that I don’t spend much time working out how to consistently value fairy dust or unicorn horns.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[24 Apr 2024]]></summary></entry><entry><title type="html">Loop quantum gravity, or lattice quantum gravity?</title><link href="/2024/03/26/Loops-quantum-gravity,-or-lattice-quantum-gravity.html" rel="alternate" type="text/html" title="Loop quantum gravity, or lattice quantum gravity?" /><published>2024-03-26T00:00:00+00:00</published><updated>2024-03-26T00:00:00+00:00</updated><id>/2024/03/26/Loops%20quantum%20gravity,%20or%20lattice%20quantum%20gravity</id><content type="html" xml:base="/2024/03/26/Loops-quantum-gravity,-or-lattice-quantum-gravity.html"><![CDATA[<p>26 Mar 2024</p>

<p>In this post, I’m going to give a brief critical overview of loop quantum gravity, the field that I worked in during my time in physics. A fairly strong understanding of physics is assumed.</p>

<h3> The ADM formalism </h3>

<p>Loop quantum gravity is an attempt at canonical quantisation of Einstein’s general relativity. Thus the starting point is constructing a Hamiltonian. In a flat spacetime that’s easy, but on a general spacetime manifold it’s not so easy because it requires one to define a time coordinate. The <a href="https://en.wikipedia.org/wiki/ADM_formalism" target="_blank"> ADM formalism</a> addresses this by assuming that spacetime is <a href="https://en.wikipedia.org/wiki/Foliation" target="_blank"> foliated</a>. Roughly this means that our spacetime manifold \( M \) is decomposed into a family of spacelike hypersurfaces \( \Sigma_t \) labelled by a timelike coordinate \( t\). Spacelike means that any pair of points on \( \Sigma_t \) are spacelike separated - their spatial and temporal separation is such that light could not travel between them. There are in general many possible foliations of \( M \) that should all be ‘equally as good’ as each other.</p>

<p>With a time coordinate in hand, we can now define the Hamiltonian in terms of the metric \( g_{ij} \) on the 3-dimensional spacelike hypersurfaces and its conjugate momentum \( \pi_{ij} \). The Hamiltonian thus constructed consists of two parts: the Hamiltonian constraint, and the (spatial) diffeomorphism constraint.</p>

<h3> The Wheeler-DeWitt equation </h3>

<p>The next step in canonical quantisation is to replace the canonical variables with operators acting on a suitable space of functionals. Because we are working with a field theory, \( \hat{\pi}_{ij} \) will be a <a href="https://en.wikipedia.org/wiki/Functional_derivative" target="_blank"> functional derivative</a>. If \( F \) is a functional with argument \( f \), the functional differential can be defined as follows</p>

<p>\(\delta F[f][\phi] = \left[ \frac{d}{d \epsilon} F[f + \epsilon \phi]  \right]_{\epsilon = 0}\).</p>

<p>This is the functional analysis equivalent of \( dx \) in vanilla calculus. From here we define the functional derivative as</p>

<p>\(\frac{\delta F[f]}{f(y)} = \delta F[f][\delta_y]\),</p>

<p>where \( \delta_y(x) = \delta(x-y) \) is the <a href="https://en.wikipedia.org/wiki/Dirac_delta_function" target="_blank"> Dirac delta function</a>.</p>

<p>You will notice that every time we take a functional derivative, we get a factor of a delta function. This is a problem. The Hamiltonian is second order in the canonical momenta \( \hat{\pi}_{ij} \), which leads to singularities arising from the accumulated delta functions.</p>

<p>Really what’s happening here is that we started off doing something that isn’t mathematically kosher. When we promote fields to operators, really they are operator-valued <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)" target="_blank"> distributions</a>. It turns out that it is <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)#Operations_on_distributions" target="_blank"> not at all clear how to sensibly define multiplication for such objects</a>. And yet our Hamiltonian multiplies them footloose and fancy-free.</p>

<p>These are problems that are generically encountered when attempting to canonically quantise any non-trivial quantum field theory. It is the reason that <a href="https://en.wikipedia.org/wiki/Wightman_axioms#Existence_of_theories_that_satisfy_the_axioms" target="_blank"> there is no known example of a mathematically well-defined interacting quantum field theory in 4 dimensions</a>.</p>

<p>If one is willing to work on a spacetime lattice, then this issue can be avoided. Lattice quantum field theory is in fact a completely consistent, mathematically well-defined, non-perturbative quantisation method. But we’ll come back to that later.</p>

<h3> The Ashtekar variables </h3>

<p>Ok so let’s say we are really determined to ignore the fact that we are not doing meaningful maths anymore. It turns out that you can do some manipulations on the Wheeler-DeWitt equation that maybe looks like progress.</p>

<p>The <a href="https://en.wikipedia.org/wiki/Ashtekar_variables" target="_blank"> Ashtekar variables</a> consist of a complex-valued field that looks like an \( SU(2) \) connection. It turns out that in terms of these new variables, the Hamiltonian becomes (up to a factor we can ignore) polynomial in the canonical variables. Now, as per the previous section, polynomials involve multiplication which is still not generally well-defined for operator-valued distributions. However, at least the most egregiously non-polynomial factors like square roots of deteminants no longer make an appearance. This development sparked a wave of excitement which really marks the start of the entire loop quantum gravity research programme.</p>

<h3> Thiemann's trick </h3>

<p>Unfortunately, because the canonical variables are now complex-valued, the theory is a complex version of general relativity. Classically it is easy to impose reality conditions that recover real general relativity. However, in the quantum theory it’s not so easy and so far no one has been able to do it. This led to people giving up on the Ashtekar variables after a few years.</p>

<p>Enter Thomas Thiemann. He figured out a way of writing the Hamiltonian in terms of Poisson brackets involving the connection and the volume. With this trick, he did not have to resort to the problematic complex-valued variables, and once again it looked like maybe progress could be made.</p>

<h3> The loop representation </h3>

<p>Normally the operators defined from the canonical variables are taken to act on a space of wavefunctions that are functionals of the fields. It turns out that no matter if you use the Ashtekar variables or Thiemann’s trick, sensibly defining the Hamiltonian constraint on this space remains difficult and perhaps impossible. That’s probably because we chose to ignore the fact that we’re not really doing maths some time ago and ploughed ahead anyway.</p>

<p>In order to overcome this difficulty, the next ingredient in the loop quantum gravity recipe is the loop representation. Instead of working with connections, we work with <a href="https://en.wikipedia.org/wiki/Holonomy" target="_blank"> holonomies</a> associated with the connection, and their corresponding conjugate variable called ‘flux vectors’. Because the connection lives in \( SU(2) \), it turns out that the wavefunctions coincide with Penrose’s old idea of <a href="https://en.wikipedia.org/wiki/Spin_network" target="_blank"> spin networks</a>.</p>

<p>Now we need to define an inner product to get a proper Hilbert space. Spin networks are defined by graphs, and it turns out that in order to preserve diffeomorphism invariance, the inner product more or less has to be defined so that its action on two spin networks is zero unless their underlying graphs coincide. The Hilbert space thus constructed does not have a countable basis, which is a big departure from standard quantum theory. It is relatively straightforward to define the actions of holonomy and flux vector operators on states in this space.</p>

<h3> Imposing constraints </h3>

<p>We have to impose the spatial diffeomorphism constraint and the Hamiltonian constraint to get to the physical Hilbert space. It turns out that in our candidate Hilbert space above, the diffeomorphism constraint operator can’t be defined, and even if it could, it wouldn’t contain any diffeomorphism invariant state aside from the trivial wavefunction. This is because a diffeomorphism is equivalent to moving the spin network around on the manifold \( M \).</p>

<p>In order to get around this difficulty, the candidate Hilbert space has to be drastically enlarged to allow states to be constructed that are something like ‘averages’ of spin networks over states related by diffeomorphisms. If that sounds like it’s probably not well-defined, that’s because it probably isn’t. It appears that there is no real consensus on how to construct this space properly. It is also not known whether it will have a countable basis.</p>

<p>Ok so that’s the diffeomorphism constraint, how about the Hamiltonian constraint? First you have to define a suitable quantum operator, and it turns out that is a complete nightmare. I won’t go into the details, partly because I don’t entirely know them, but mostly because I don’t think it’s that interesting. If you’re up for it, <a href="#1"> [1]</a> gets into more of the nitty gritty. The upshot is that a large number of contrived choices need to be made in order to even make symbolic process. And even then, a large number of ambiguities remain. In particular, it appears necessary to restrict the space that the Hamiltonian constraint acts on to a particular ‘habitat’, for which there may be an infinite number of candidate choices. It turns out too that the Hamiltonian and diffeomorphism constraints don’t play well together. If you proceed ‘naively’ the action of the Hamiltonian constraint can map a diffeomorphism invariant state onto one that isn’t.</p>

<p>The whole thing ends in such a huge mess that not a single physical eigenstate of the Hamiltonian constraint is known. In addition, because of the extraordinary complexity of the Hamiltonian and diffeomorphism operators, it is not known whether the quantum constraint algebra closes. This is a basic requirement for any theory to be consistent.</p>

<h3> Spin foams </h3>

<p>As long ago as the early 2000’s, the situation on the canonical quantisation side seemed hopeless enough that many people in the loop quantum gravity community had instead turned to path integral approaches, hoping they might provide a way out. To this end, various attempts were made to define a partition function for quantum gravity, which all ultimately rest on introducing a discretisation of spacetime - usually a <a href="https://en.wikipedia.org/wiki/Triangulation_(geometry)" target="_blank"> triangulation</a>. If you start out with topological quantum field theory (TQFT), the partition function is naturally independent of the triangulation you choose, and people like that because it has diffeomorphism symmetry as required by general relativity. However, TQFTs inevitably do not have local degrees of freedom, because those degrees of freedom would certainly be sensitive to the local geometry of spacetime. On the other hand, general relativity does have local degrees of freedom. As soon as you allow for local degrees of freedom, the triangulation independence is lost, and you are left in a situation that is basically identical to lattice quantum field theory. That is, the theory depends on the particular spacetime discretisation you choose.</p>

<h3> Conclusion </h3>

<p>Let me try and sum up the history of loop quantum gravity. We started out with the Ashtekar variables, which seemed to make progress on making sense of the Wheeler-DeWitt equation. However, they were abandoned because reality conditions could not sensibly be imposed in the quantum theory. Our next attempt at making sense of the Wheeler-DeWitt equation was to use Thiemann’s trick. Seemingly the only way to make ‘progress’ in the resulting quantum theory is to switch to the loop representation. This leads to hideously complicated and perhaps ill-defined forms for the Hamiltonian and diffeomorphism constraints and the space that they are meant to act on. The theory is riddled with a large number of ambiguities and artifical choices, and thus far it has not yielded anything physically intelligible. From there, we moved on to the spin foam approach, which is functionally indistinguishable from lattice quantum field theory.</p>

<p>In the process, the following issues remain unsolved</p>
<ul>
  <li>It is not known whether general relativity is recovered in the classical limit of loop quantum gravity.</li>
  <li>It is not known whether a suitable quantum constraint algebra can be defined that is closed, and therefore whether the quantum theory will be independent of arbitrary choices of coordinates.</li>
  <li>The candidate operators and Hilbert spaces that do exist may not be well-defined, and even if they are, their definition is plagued by a large number of ad-hoc choices and ambiguities.</li>
  <li>It is not known whether there is any connection between loop quantum gravity and spin foam approaches.</li>
</ul>

<p>To me, loop quantum gravity looks like a long series of mashing square-shaped pegs into round holes and ignoring the wreckage that ensues. The entire thing is ugly, contrived and fails to make any tangible progress on the subject of quantum gravity. After 40 years of hammering away at this, it appears that the best the field has to offer is something that is really just a variant of lattice quantum field theory. But the whole motivation for doing any of this in the first place was to avoid perceived shortcomings of lattice quanum field theory!</p>

<p>Ok, it’s easy for me to get on my historically retrospective high horse. I will say this about loop quantum gravity; I think it was worth a shot. But I do more or less consider it a failed research programme. It seems like most of its biggest proponents have quietly moved on to other things. I found that the field also has an unfortunate tendency to oversell itself. You would think the above laundry list of outstanding issues would call for a touch of humility and self-awareness. I didn’t see much of that when I worked in the field. I get that the people who dedicate their professional lives to a particular theory are inevitably going to be its biggest cheerleaders. But for young graduate students, the mind-boggling complexity of high energy theoretical physics means that it’s almost impossible to judge the merits of a field before having worked in it for several years. And by that time you’ve probably got intellectual Stockholm syndrome anyway. Maybe the self-aggrandisement of loop quantum gravity is a counter-reaction to the ideological hegemony of string theory and the revolutionary fervour of its adherents. But if that’s the case, I just wish everyone would get a grip. I guess that woudn’t leave them with much content to write papers about though.</p>

<h3>  References </h3>

<p><span id="1"> <b>1. </b> </span> Nicolai H, Peeters K, Zamalkar M. <a href="https://arxiv.org/abs/hep-th/0501114" target="_blank"> Loop quantum gravity: an outside view</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[26 Mar 2024]]></summary></entry><entry><title type="html">Robust standard errors</title><link href="/2024/01/17/Robust-standard-errors.html" rel="alternate" type="text/html" title="Robust standard errors" /><published>2024-01-17T00:00:00+00:00</published><updated>2024-01-17T00:00:00+00:00</updated><id>/2024/01/17/Robust%20standard%20errors</id><content type="html" xml:base="/2024/01/17/Robust-standard-errors.html"><![CDATA[<p>17 Jan 2024</p>

<p>Robust standard errors are frequently used in statistics, often in an unthinking way. They are certainly valuable and useful in the context of linear regression; however, they do not serve a similar purpose in maximum likelihood estimation of non-linear models. The basic issue is that the kind of mis-specification that robust standard errors can address in linear regresion does not spoil consistency results for the parameter estimates. On the other hand, it does spoil those results in the context of maximum likelihood estimation of non-linear models. Thus at best you end up with a consistent estimator for the variance of a parameter estimate that is itself inconsistent, which isn’t really of interest except as a diagnostic tool for detecting bad models. I’ll explore this in more detail below.</p>

<h3>  Linear regression </h3>

<p>In my <a href="https://drstevenkerr.com/2024/01/03/Why-do-so-many-statisticians-think-a-normality-assumption-is-required-in-linear-regression.html" target="_blank"> last post</a>, I discussed some sufficient conditions for ‘model validity’ in linear regression. I will use the same notation defined there. Namely: \( y \) is an \( n \)-dimensional vector whose elements are \( y_i \), \( X \) is an \( n \times k \) matrix whose rows are \( x_i \), \( \beta \) is a \( k\)-dimensional vector of parameters to be estimated, and \( \epsilon \) is an \( n \)-dimensional vector of residuals whose elements are \( \epsilon_i \). One of the conditions was as follows:</p>

<p><b>Central limit theorem (CLT).</b> \(  \sqrt{n} \left( \frac{1}{n} \sum_{i} x_i \epsilon_i\right) \xrightarrow{d} N(0, \Sigma) \), where \( \xrightarrow{d} \) denotes <a href="https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_distribution" target="_blank"> convergence in distribution</a>.</p>

<p>If \( \Sigma \) is of the form \( \sigma^2 I \) where \( I \) is the \( n \times n \) identity matrix, then the errors are said to ‘spherical’. In this case, the conditional variance of  the ordinary least squares (OLS) estimator is</p>

\[\mathrm{Var}(\hat{\beta} \; \lvert \; X) = \frac{\epsilon^T \epsilon}{n-k} (X^T X)^{-1},\]

<p>where \( \hat{\epsilon} = y - X \hat{\beta} \).</p>

<p>It turns out that the spherical error requirement can be significantly weakened. In particular, there are several ‘robust’ variance estimators that are consistent even in the presence of heteroskedasticity. For example, the following estimator</p>

\[HC_0 :=  \frac{n}{n-k} (X^T X)^{-1} X^T \mathrm{diag}(\hat{\epsilon}) X (X^T X)^{-1},\]

<p>where \( \mathrm{diag}(\hat{\epsilon}) \) is the diagonal matrix with non-zero elements given by \( \hat{\epsilon}\).</p>

<p>Crucially, the spherical error assumption is <b>not needed</b> for the OLS estimator \( \hat{\beta} = (X^T X)^{-1} X^T y \) to be consistent. This means there is no conflict between consistency of \( \hat{\beta}\) and presence of heteroscedasticity. Heteroscedasticity robust standard errors are therefore very useful as a quantification of uncertainty of parameter estimates in many scenarios.</p>

<p>As we shall see, the analogous statement is not so much the case in maximum likelihood estimation.</p>

<h3>  Maximum likelihood estimation </h3>

<p>In the following, I will use \( \theta_0 \) to denote the ‘true’ parameter values of a data-generating process, \( \theta \) for a set of parameter values, and \( w_i \) for a vector  that is a single sample from the data-generating process. I will consider the particular case of M-estimators, which are estimators that maximise an objective function that takes the form of a sample average,</p>

\[Q_n(\theta) :=  \frac{1}{n} \sum_i m(w_i, \theta).\]

<p>Here, \( m \) is just some function of the data and the parameters. The typical case in maximum likelihood estimation is \( m(w_i, \theta) = \mathrm{ln}(f(w_i, \theta)) \), where \( f(w_i, \theta) \) is a probability density parametrised by \( \theta \) and evaluated at \( w_i \). In constructing this expression, it is typically assumed that the \( w_i \) are <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" target="_blank"> iid</a>.</p>

<p>Let’s look at a typical set of sufficient conditions for consistency of maximum likelihood parameter estimates.</p>

<p><span id="1.1"> <b>1.1.</b> The <a href="https://en.wikipedia.org/wiki/Stochastic_process" target="_blank"> stochastic pocess</a> \( w_i \) is <a href="https://en.wikipedia.org/wiki/Stationary_process" target="_blank"> stationary</a> and <a href="https://en.wikipedia.org/wiki/Ergodic_process" target="_blank"> ergodic</a>.</span></p>

<p><span id="1.2"> <b>1.2.</b> \( \theta_0 \in \Theta \) where \( \Theta \) is a convex parameter space.</span></p>

<p><span id="1.3"> <b>1.3.</b> \( m(w_i, \theta) \) is concave in \( \theta \; \forall \; w_i \).</span></p>

<p><span id="1.4"> <b>1.4.</b> \( m(w_i, \theta) \) is a <a href="https://en.wikipedia.org/wiki/Measurable_function" target="_blank"> measurable function</a> of \( w_i \; \forall \theta \in \Theta \).</span></p>

<p><span id="1.5"> <b>1.5.</b> \( \mathbb{E}[m(w_i, \theta)] \) is uniquely maximised on \( \Theta \) at \( \theta_0 \).</span></p>

<p><span id="1.6"> <b>1.6.</b> \( \mathbb{E}[m(w_i, \theta)] \) is finite \(\forall \; \theta \in \Theta \).</span></p>

<p>The logic behind these conditions is as follows. <a href="#1.1"> 1.1</a> and <a href="#1.6"> 1.6</a> guarantee that the objective function \( Q_n(\theta) \) converges in probability to \( \mathbb{E}[m(w_i, \theta)] \). The idea is that if we then maximise \( Q_n(\theta) \), the resulting estimator \( \hat{\theta} \) will converge in probability to the unique ‘true’ value \( \theta_0 \) that maximises \( \mathbb{E}[m(w_i, \theta)] \), assumed to exist courtesy of <a href="#1.5"> 1.5</a>. Conditions <a href="#1.2"> 1.2</a> and <a href="#1.3"> 1.3</a> simply ensure that \( Q_n(\theta) \) has a unique maximum, and <a href="#1.4"> 1.4</a> is a ‘mathematical housekeeping’ condition that just means that \( m(w_i, \theta) \) is in fact a random variable.</p>

<p>Note that despite the fact that the iid assumption is typically used to derive the form of the likelihood function in the first place, it does not appear in <a href="#1.1">1.1</a>-<a href="#1.6">1.6</a>. We only need \( w_i \) to be ergodic and stationary, and \( \mathbb{E}[m(w_i, \theta)] \) to be uniquely maximised at \( \theta_0 \). In other words, the objective function \( Q_n(\theta) \) needn’t be a correctly specified log-likelihood. This opens the doorway to <a href="https://en.wikipedia.org/wiki/Quasi-maximum_likelihood_estimate" target="_blank"> quasi-maximum likelihood estimation</a>, in which the likelihood function is mis-specified but still has the correct maximum.</p>

<p>Conditions <a href="#1.2"> 1.2</a> and <a href="#1.3"> 1.3</a> also ensure that the global maximum of \( Q_n(\theta) \) can be found by taking the first derivative and setting equal to zero. We define the score and Hessian for observation \( i\) as follows</p>

\[\begin{align}
s(w_i, \theta) &amp;:=  \frac{\partial m(w_i, \theta)}{\partial \theta} \\
H(w_i, \theta) &amp;:= \frac{\partial^2 m(w_i, \theta)}{\partial \theta \partial \theta'}.
\end{align}\]

<p>Assume the following,</p>

\[\begin{align}
\frac{1}{n} \sum_i  H(w_i, \theta) &amp;\xrightarrow{p} \mathbb{E}[H(w_i, \theta)] \\
\frac{1}{n} \sum_i  s(w_i, \theta_0) &amp;\xrightarrow{d} N(0, \Sigma).
\end{align}\]

<p>We can then apply the <a href="https://en.wikipedia.org/wiki/Delta_method" target="_blank"> delta method</a> to \( \frac{ \partial Q_n(\hat{\theta})}{\partial \theta} =  \frac{1}{n} \sum_i s(w_i, \theta) = 0 \), obtaining</p>

\[\sqrt{n} (\hat{\theta} - \theta_0) \xrightarrow{d} N(0, \Sigma'),\]

<p>where</p>

<p>\(\begin{align}
\Sigma' = \big( \mathbb{E}[H(w_i, \theta_0)] \big)^{-1} \; \Sigma \; \big( \mathbb{E}[H(w_i, \theta_0)] \big)^{-1} \label{robust} \tag{1}
\end{align}\).</p>

<p>In the case where \( w_i \) are <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" target="_blank"> iid</a> and some <a href="https://en.wikipedia.org/wiki/Fisher_information" target="_blank"> regularity conditions</a> are satisfied, this expression simplifies to</p>

<p>\(\begin{align}
\Sigma' = - \big( \mathbb{E}[H(w_i, \theta_0)] \big)^{-1} \label{non-robust} \tag{2}
\end{align}\).</p>

<p>These results can be found in Chapter 7, sections 7.2 and 7.3 of <a href="#1"> [1]</a>.</p>

<h3> Mis-specification </h3>
<p>Finally we get to the real subject of this post. As indicated above, we may still extract useful results from a model if the likelihood is mis-specified, so long as it has the correct maximum, or the mis-specification is ‘small’ in some appropriate sense. In this case, both \( \hat{\theta} \) and its variance will be consistent.</p>

<p>However, what if there is a more serious mis-specification? This will certainly spoil consistency results for \( \hat{\theta} \). We may still be able to consistently estimate the variance of \( \hat{\theta} \) though, using equation \(\ref{robust}\). But that begs the question, why should we care? What value is there in having a consistent estimator of the variance of a parameter estimator that is itself inconsistent?</p>

<p>This has been pointed out, for example, in <a href="#2"> [2]</a>. In essence, if there is a need to use robust standard errors, then it indicates a serious flaw in the model that demands a new and better model, not a tweak to fix the variance.</p>

<h3>  References </h3>

<p><span id="1"> <b>1. </b> </span> Hayashi, Fumio. Econometrics. Princeton :Princeton University Press, 2000.</p>

<p><span id="2"> <b>2. </b> </span> King G, Roberts ME. <a href="https://gking.harvard.edu/files/gking/files/robust_0.pdf" target="_blank"> How robust standard errors expose methodological problems they do not fix, and what to do about it.</a> Political analysis, Vol. 23, No. 2, 2015.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[17 Jan 2024]]></summary></entry><entry><title type="html">Why do so many statisticians think a normality assumption is required in linear regression?</title><link href="/2024/01/03/Why-do-so-many-statisticians-think-a-normality-assumption-is-required-in-linear-regression.html" rel="alternate" type="text/html" title="Why do so many statisticians think a normality assumption is required in linear regression?" /><published>2024-01-03T00:00:00+00:00</published><updated>2024-01-03T00:00:00+00:00</updated><id>/2024/01/03/Why%20do%20so%20many%20statisticians%20think%20a%20normality%20assumption%20is%20required%20in%20linear%20regression</id><content type="html" xml:base="/2024/01/03/Why-do-so-many-statisticians-think-a-normality-assumption-is-required-in-linear-regression.html"><![CDATA[<p>03 Jan 2024</p>

<p>In my time working in health science, I have been troubled by the number of times I have encountered statisticians and practitioners of statistics who are absolutely sure that either the variables or residuals in a linear regression must be approximately normally distributed, and the model is invalid otherwise.</p>

<p>This idea is completely false. In this post I want to explore why it is nonetheless so widely believed by professionals in the field.</p>

<h3>  Linear regression </h3>

<p>When statisticians say a model is ‘valid’, they usually mean that the model plausibly satisfies some conditions that guarantee ‘nice’ results. By ‘nice’, it is usually meant that parameter estimates converge to the true values from the data generating process, and that the distribution of the parameter estimates is known (usually normal), as the sample size becomes large. There are several slightly different notions of <a href="https://en.wikipedia.org/wiki/Convergence_of_random_variables" target="_blank"> convergence for random variables</a>, but the most commonly used is <a href="https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_probability" target="_blank"> convergence in probability</a>. An estimator \( \hat{\beta} \) that converges in probability to true value \( \beta \) is called <a href="https://en.wikipedia.org/wiki/Consistent_estimator" target="_blank"> consistent</a>.</p>

<p>In this post I will use the following notation: \( y \) is an \( n \)-dimensional vector whose elements are \( y_i \), \( X \) is an \( n \times k \) matrix whose rows are \( x_i \), \( \beta \) is a \( k\)-dimensional vector of parameters to be estimated, and \( \epsilon \) is an \( n \)-dimensional vector of residuals whose elements are \( \epsilon_i \).</p>

<p>In the linear regression setting, there are standard sets of assumptions that are sufficient to guarantee consistency. Note the plural - there are several. The first version you often learn while studying statistics is the following:</p>

<p><span id="1.1"> <b>1.1. Linearity.</b> </span> The data-generating process is of the form \( y = X \beta + \epsilon \).</p>

<p><span id="1.2"> <b>1.2. Strict exogeneity.</b> </span>\( \mathbb{E}[\epsilon \; | \; X] = 0 \).</p>

<p><span id="1.3"> <b>1.3. No multicollinearity.</b> </span>The rank of \( X \) is \( k \) with probability \( 1 \).</p>

<p><span id="1.4"> <b>1.4. Conditional normality of residuals.</b> </span> \(  \epsilon \lvert_X \sim N(0, \sigma^2 I) \), where \( I \) is the \( n \times n \) identity matrix.</p>

<p>Note that an explicit normality assumption appears in <a href="#1.4"> 1.4</a>.</p>

<p>However, several of these conditions can be weakened. For example, the following set of conditions is also sufficient:</p>

<p><span id="2.1"> <b>2.1. Linearity.</b> The data-generating process is of the form \( y = X \beta + \epsilon \).</span></p>

<p><span id="2.2"> <b>2.2. Ergodic stationarity.</b> The <a href="https://en.wikipedia.org/wiki/Stochastic_process" target="_blank"> stochastic pocess</a> \( (y_i, x_i) \) is <a href="https://en.wikipedia.org/wiki/Stationary_process" target="_blank"> stationary</a> and <a href="https://en.wikipedia.org/wiki/Ergodic_process" target="_blank"> ergodic</a>.</span></p>

<p><span id="2.3"> <b>2.3. Weak exogeneity.</b> \( \mathbb{E}[\epsilon_i x_i] = 0 \; \forall i \).</span></p>

<p><span id="2.4"> <b>2.4. Invertibility.</b> \( \mathbb{E}[x_i x_i^T ] \) is invertible.</span></p>

<p><span id="2.5"> <b>2.5. Central limit theorem (CLT).</b> \(  \sqrt{n} \left( \frac{1}{n} \sum_{i} x_i \epsilon_i\right) \xrightarrow{d} N(0, \Sigma) \), where \( \xrightarrow{d} \) denotes <a href="https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_distribution" target="_blank"> convergence in distribution</a>.</span></p>

<p>Note here that the explicit normality assumption <a href="#1.4"> 1.4</a> is replaced by <a href="#2.5"> 2.5</a>, which is much weaker and only requires \( \frac{1}{n} \sum_{i} x_i \epsilon_i \) to obey a <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank"> central limit theorem</a>. The most basic version of the CLT applies to <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" target="_blank"> independent and identically distributed</a> random variables, but there are many other versions with weaker conditions.</p>

<p>In other words, there is no need to assume that the dependent variable, the independent variables or the residuals are normally distributed in linear regression. Why, then, do so many statisticians believe it to be so?</p>

<p>The first reason I can see is that it’s just easier. Conditions <a href="#1.1">1.1</a>-<a href="#1.4">1.4</a>  are simpler to understand and require less mathematical knowledge than <a href="#2.1">2.1</a>-<a href="#2.5">2.5</a>. In particular, you needn’t know anything about the central limit theorem, ergodic and stationary processes, and different notions of convergence for random variables.</p>

<p>The other reason I can see is that if you assume the residuals are normally distributed, then linear regression falls into the <a href="https://en.wikipedia.org/wiki/Generalized_linear_model" target="_blank"> generalised linear class of models</a>. This is an extremely useful set of statistical models that includes workhorses such as logistic and Poisson regression. Non-linear models in this family are fitted using maximum likelihood methods - usually  the <a href="https://en.wikipedia.org/wiki/Newton%27s_method" target="_blank"> Newton-Raphson method</a>, which results in an <a href="https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares" target="_blank"> iteratively re-weighted least squares</a> algorithm. It is conceptually and pedagogically neat to include all of these models under the same banner, because they have a great deal of commonality. However, because they are maximum likelihood models, this does require explicit distributional assumptions to be made - normality in the case of linear regression. Bundling linear regression into the generalised linear class obscures the fact that there are alternate, weaker assumptions that are sufficient for the model to be valid.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[03 Jan 2024]]></summary></entry><entry><title type="html">Eric Weinstein is the Tenacious D of high energy physics</title><link href="/2023/10/29/Eric-Weinstein-is-the-Tenacious-D-of-high-energy-physics.html" rel="alternate" type="text/html" title="Eric Weinstein is the Tenacious D of high energy physics" /><published>2023-10-29T00:00:00+01:00</published><updated>2023-10-29T00:00:00+01:00</updated><id>/2023/10/29/Eric%20Weinstein%20is%20the%20Tenacious%20D%20of%20high%20energy%20physics</id><content type="html" xml:base="/2023/10/29/Eric-Weinstein-is-the-Tenacious-D-of-high-energy-physics.html"><![CDATA[<p>29 Oct 2023</p>

<h3>  Who is Eric Weinstein? </h3>

<p>Eric is a podcaster who has most notably made appearances on the Joe Rogan experience (JRE). He received a PhD in mathematical physics from Harvard in 1992, and between 2013 and 2022 he was a managing director of Thiel Capital - <a href="https://en.wikipedia.org/wiki/Peter_Thiel" target="_blank"> Peter Thiel</a>’s investment firm.</p>

<p>Eric came to some public prominence after his brother Bret Weinstein was involved in an incident at Evergeen State college. The college had a decades-old tradition called ‘Day of absence’, where ethnic minorities students would not attend campus to highlight their contributions. In 2017, a twist on this was put forward where white people would instead stay off campus for the day. Bret was a professor at the college at the time and refused to participate, circulating an email detailing his reasons. A group of students marched on one of his lectures in protest, angrily demanding he leave campus. The protestors later met with Evergreen staff and subjected them to what I can best describe as a struggle session. The whole incident was quite extraordinary, and I recommend watching raw footage, for example <a href="https://www.youtube.com/watch?v=a7s8IAOuumQ&amp;list=PLq9jO8fmlPecnlR18OOm98bETOojth_NL&amp;index=6" target="_blank"> here</a> and <a href="https://www.youtube.com/watch?v=3q_YIGrsODc&amp;list=PLq9jO8fmlPecnlR18OOm98bETOojth_NL&amp;index=31" target="_blank"> here</a>.</p>

<p>After the incident, Bret was invited to discuss his experiences on JRE, and went on to host a podcast himself. Eric managed to get a slot on JRE following his brother’s successful appearances. He subsequently coined the term ‘Intellectual dark web’, to refer to a group of iconoclastic thinkers and commentators, including such figures as Joe Rogan, Jordan Peterson, Sam harris and of course himself.</p>

<h3>  Who are Tenacious D? </h3>

<p>A comedy rock band consisting of Jack Black and Kyle Gass. You may remember them from their timeless classic <a href="https://www.youtube.com/watch?v=_lK4cX5xGiQ" target="_blank"> Tribute</a>, which tells a tale of their encounter with a daemon who demands that they play the ‘best song in the world’. As the now immortal lyrics go, they played the first thing that came to their heads, which just so happened be the best song in the world. However, they aren’t able to remember or reproduce the song after the fact, so we have to rely on their word that it was, in fact, the best song in the world.</p>

<h3>  What do Eric Weinstein and Tenacious D have in common? </h3>

<p>Eric fancies himself as an unrecognised mathematical genius who holds the key to the elusive <a href="https://en.wikipedia.org/wiki/Theory_of_everything" target="_blank"> theory of everything</a> - the holy grail of theoretical physics that many brilliant but lesser minds have been struggling to find for decades. He calls his theory ‘Geometric unity’. If you sign up to his mailing list <a href="https://geometricunity.org/" target="_blank"> here</a>, you can read a draft of this once in a lifetime work of brilliance.</p>

<p>Alternately, if you, like me, are puzzled by the idea of having to sign up to a mailing list to read a theoretical physics paper, you can find a draft that was released in 2021 <a href="https://geometricunity.nyc3.digitaloceanspaces.com/Geometric_Unity-Draft-April-1st-2021.pdf" target="_blank"> here</a>. You can also find a lecture he gave at Oxford in 2013 on his theory <a href="https://www.youtube.com/watch?v=Z7rd04KzLcg" target="_blank"> here</a>.</p>

<p>Spoiler alert: I would not recommend grinding through either of these, even if you have the requisite physics background. The high level summary is that Eric has definitely solved every major outstanding problem in high energy physics, but he just can’t quite remember most or all of the details.</p>

<p>It is definitely the best theory in the world though.</p>

<h3>  I'm confused by how that could happen </h3>

<p>You and me both.</p>

<p>Eric claims to have figured out the major detail of the theory back in 1983-1984, when he would have been just 18-19 years old and in the middle of a combined Bachelor’s/Master’s degree. If you weren’t already suspicious, this should definitely set your bullshit detector screaming. Modern high energy physics is ming-bogglingly complex, to the extent that even world recognised geniuses only started to make serious contributions at earliest in their mid to late twenties. The vast majority of 18 years olds doing physics degrees haven’t even begun to acquire an understanding of general relativity or <a href="https://en.wikipedia.org/wiki/Standard_Model" target="_blank"> the standard model</a>, both extremely complex subjects in their own right that demand mastery as a minimal price of entry before you might even begin making modest contributions towards a ‘theory of everything’.</p>

<p>But Weinstein, a man who admits to not having any spectcalar technical ability, as an 18 year old back in the 1980s, managed not only to make a contribution, but to blow the whole thing sky high? And has kept the lid on this stunning development for several decades, for, um, reasons?</p>

<p>Right.</p>

<p>Eric claims to have pieced together his theory of everything from various documents he produced in that period. From his <a href="https://geometricunity.nyc3.digitaloceanspaces.com/Geometric_Unity-Draft-April-1st-2021.pdf" target="_blank"> draft</a>:</p>

<blockquote> As such this document is an attempt to begin recovering a rather more complete theory which is at this point only partially remembered and stiched(sic) together from old computer files, notebooks, recordings and the like dating back as far as 1983-4 when the author began the present line of investigation. This is the first time the author has attempted to assemble the major components of the story and has discovered in the process how much variation there has been across matters of notation, convention, and methodology. 
</blockquote>
<p><br />
Well, if that doesn’t fill you with confidence, I don’t know what will.</p>

<p>Eric also wants you to know that he has very carefully taken into consideration the immediate objection you might have; that in his own words he has been ‘working in near total isolation from the community for over 25 years, does not know the current state of the literature, and has few, if any, colleagues to regularly consult.’ On this, he has the following to say</p>

<blockquote> Every effort has been made to standardize notation but what you are reading is stitched together from entirely heterogeneous sources and inaccuracies and discrepancies are regularly encountered as well as missing components when old work is located. The author notes many academicians find this unprofessional and therefore irritating. This is quite literally unprofessional as the author is not employed within the profession and has not worked professionally on such material since the fall of 1994. If you find this disagreeable, please feel free to take your professional assumptions elsewhere. This document comes from a context totally different from the world of grants, citations, research metrics, lectures, awards and positions. In fact, the author claims that if there is any merit to be found here, it is unlikely that it could be worked out in such a context due to the author’s direct experience of the political economy of modern academic research. This work stands apart from that context and does so proudly, intentionally, and without apology.
</blockquote>
<p><br />
Well, seems legit to me. If Eric’s work contains errors or just plain doesn’t make sense, the theory is not to blame; rather it’s all just part and parcel of him being a rogue that operates outside of the traditional, stifling environment of academia. If you really think about it, that’s actually a strength.</p>

<p>Ok let’s delve into the abyss of his ideas.</p>

<h3>  Not even wrong </h3>

<p>I have watched <a href="https://youtu.be/Z7rd04KzLcg" target="_blank"> Eric’s 2013 lecture at Oxford</a>. I found it completely incoherent, obscure and lacking in crucial detail. Eric himself admits that he made errors in presentation that may have rendered the talk largely unintelligible. An appropriately trained physicist cannot, for example, sit down with pen and paper and check the maths. You can’t even extract much qualitative detail about the theory. It’s just a vague mess of poorly-presented, half-baked ideas.</p>

<p>So I was very excited when, in 2021, after almost a decade of prevaricating, Eric announced that he was releasing his <a href="https://geometricunity.nyc3.digitaloceanspaces.com/Geometric_Unity-Draft-April-1st-2021.pdf" target="_blank"> paper</a> properly detailing the theory. I wanted to see some actual maths.</p>

<p>I confess, however, that I haven’t made it through the document. It’s the sort of thing you could spend a very long time reading and be none the wiser at the end. It’s still vague, it’s still incoherent, and it’s still the sort of thing that does not remotely show enough promise to spend whatever stupid amount of time it would take to get through it and still not understand at the end because, as it turns out, it’s all nonsense.</p>

<p>What I do understand is that he wishes to consider a gauge theory defined on a 14-dimensional manifold (his so-called ‘Observerse’) consisting of the four known spacetime dimensions, together with a bundle of symmetric  \( 4 \times 4 \) matrices. The gauge group on this total space is \( U(128) \).</p>

<p>The physical motivation for doing any of this is completely unclear. But ignoring that, there are already big problems. In 1967, Sidney Coleman and Jeffrey Mandula proved the 
<a href="https://en.wikipedia.org/wiki/Coleman%E2%80%93Mandula_theorem" target="_blank"> Coleman-Mandula theorem</a>, which forbids combining internal symmetries with spacetime symmetries. There are some exceptions in the theorem, the most notable of which is supersymmetry. It’s not clear to me what Eric Weinstein’s feelings are on supersymmetry. However, his theory, to the extent that it is intelligible (it isn’t), is not formulated with supersymmetry. He offers only this cryptic line on the subject:</p>

<blockquote> The author finds supersymmetry unnecessarily confusing as an as-if symmetry and is uncomfortable saying much more about it. </blockquote>
<p><br />
I don’t know what an ‘as-if’ symmetry is, but if Weinstein finds supersymmetry confusing then he is really up shit creek, since his theory needs supersymmetry if it is to have any hope at all.</p>

<p>Then there’s the problem that most all attempts to include the symmetries of general relativity into a gauge group have. Namely, general relativity has diffemorphism symmetry, which is much larger than the local Lorentz symmetry these theories typically try to combine with internal symmetries. Efforts like Weinstein’s don’t even try to address that issue.</p>

<p>The theory is constructed on a 14-dimensional spacetime manifold. Zero explanation is offered for how our four dimensional spacetime is to be recovered.</p>

<p>Finally, Eric also hasn’t even attempted to quantise his theory. This is a major hurdle any theory must overcome in order to be consistent with well known physics. I guess Eric hasn’t gotten round to it in the last 40 years.</p>

<p>The most enlightening resources I have found on Weinstein’s own work actually come from other people - in particular, critiques <a href="https://timothynguyen.org/geometric-unity/" target="_blank"> here</a> given by Timothy Nguyen, who has a PhD in mathematics from MIT, was formerly a visiting assistant professor at Michigan State, and now works on machine learning at DeepMind. His <a href="https://timothynguyen.files.wordpress.com/2021/02/geometric_unity.pdf" target="_blank"> paper</a> is, in contrast to Weinstein’s, intelligible, and points out that the theory is a complete non-starter. The gauge theory it is based on leads to an anomaly, meaning there is no sensible quantisation. He also notes that</p>

<blockquote> Essential technical details of GU are omitted, leaving many of the central claims unverifiable. </blockquote>
<p><br />
I, for one, am shocked.</p>

<h3>  Some more metaphors </h3>
<p>It can be hard to fully convey how awful all of this to people outside the physics profession. I think the Tenacious D metaphor is pretty good. However, if it didn’t hit home for you, I have some others you might like.</p>

<p>The first is based on a South park episode in which the main characters find that an army of gnomes are stealing everyone’s underwear. When interrogated, the gnomes respond with their master plan illustrated thusly:</p>

<p><img src="../../../images/profit_meme.png" width="900" height="500" class="center" /></p>

<p>You can watch the sequence <a href="https://youtu.be/a5ih_TQWqCA" target="_blank"> here</a>; it’s very amusing. It’s also wonderfully apt as a metaphor for Weinstein’s theory, which is not so much a theory as it is a wish-list of magical things that do not exist but are required in order for his theory to work.</p>

<p>The other draws inspiration from the <a href="https://en.wikipedia.org/wiki/Bogdanov_affair" target="_blank"> Bogdanov affair</a>. The Bogdanovs were French twin brothers who found some success as television presenters and science popularisers. They obtained PhDs in physics from the Universiy of Burgundy around 2002. However, it soon transpired that their PhD theses were complete and utter nonsense. They contained some coherent mathematics copied from other papers, combined with what physicist John Baez described as</p>

<blockquote> a mishmash of superficially plausible sentences containing the right buzzwords in approximately the right order. There is no logic or cohesion in what they write.</blockquote>
<p><br />
Nonetheless, the brothers managed to fool some very smart people, getting five papers publish in peer-reviewed journals and even getting their theses approved by Dirac medallist <a href="https://en.wikipedia.org/wiki/Roman_Jackiw" target="_blank"> Roman Jackiw</a>.</p>

<p>Eric Weinstein is like a live-action Bogdanov affair. He knows enough about physics to lace his speech with technically correct, jargon-laden statements designed to give the appearance of deep knowledge, while combining with poetic statements about his grand ‘theory of everything’ that are ultimately either unintelligible or unverifiable.</p>

<h3>  How can this happen? </h3>

<p>Eric Weinstein has managed to get himself a voice on some very impressive platforms, including the Joe Rogan Experience, as well as stage time with some extremely accomplished physicists such as <a href="https://en.wikipedia.org/wiki/Roger_Penrose" target="_blank"> Roger Penrose</a> and <a href="https://en.wikipedia.org/wiki/Brian_Greene" target="_blank"> Brian Greene</a>. Which is really puzzling on the face of it, given that he has achieved next to nothing in the field. How does he do it?</p>

<p>Eric Weinstein’s main talent in life, it seems to me, is storytelling. Just watch him action when he speaks about a subject that isn’t physics. He puts together sentences that sound unusual, clever and interesting. There is a structure that can be followed, and you feel rewarded for following it. He speaks with confidence. And yet, when you break down what he’s actually saying into simple language, it’s typically vacuous or unintelligible or silly.</p>

<p>He also really likes to present himself as a maverick genius. A wonderful and cringe-worthy example of this came around the 20 minute mark on episdoe 1628 of the Joe Rogan experience.  They have the following exchange where a clip is pulled up of Weinstein playing guitar, which he clearly has some talent at.</p>

<blockquote>
 Eric: "I put out a brief clip of myself playing on instagram and I got contacted by some of the best guitarists in the f-ing world. When Tosin Abasi and Joe Robinson and Ryan Roxy contact you."<br />
<br />
Joe: "Give me some of this Jamie" (Eric's clip plays)<br />
<br />
Joe: "That's pretty good and you are doing that without a pick"<br />
<br />
Eric: "Yeah I didn't know you were supposed to use a pick.... apparently you are supposed to use a pick, but I didn't know! Basically I'm playing air guitar with a real guitar"<br />
<br />
Joe: "Wow that's really good. What do you mean you didn't know you weren't supposed to play with a pick?"<br />
<br />
Eric: "Dude I don't know what I'm doing. I don't know what I'm doing!<br />
<br />
Joe: "How did you learn how to do this?"<br />
<br />
Eric: "Ahhhh I hung out in a room, alone, sort of dark, and lonely"<br />
<br />
Joe: "Really?"<br />
<br />
Eric: "Yeah"<br />
<br />
Joe: "When did you learn this?"<br />
<br />
Eric: "This is part of the thing, I do a bunch of things that I don't do with other people, right? I just learn shit on my own"<br />
<br />
Joe: "But when did you learn this? How long ago?"<br />
<br />
Eric: "I don't even know, some of it in the last year"<br />
<br />
Joe: "But but when did you start playing guitar? You're pretending to be coy, and I don't like it, I'm calling you out on this."<br />
<br />
Eric: "Ok. I've had a guitar since I was 15"<br />
<br />
Joe: "Oh, ok you have been playing forever but you are self taught." 
</blockquote>
<p><br />
Eric is doing everything he can here to let the listener come to the conclusion that he picked up guitar a very short time ago, and now plays like a pro - a true autodidactic genius. Except Joe knows it’s just not possible and on this occasion nails him on it.</p>

<p>Eric pulls similar tricks when he talks about physics with Joe, but gets away with it because Joe doesn’t know how to call him on his bullshit. Eric knows that the vast majority of the audience have no idea what he is talking about, but frequently uses esoteric technical language liberally and unnecessarily anyway. There is only one reason to do this; to craft a particular image that elevates himself above others.</p>

<h3> High energy confidence tricks </h3>

<p>Geometric unity has been summarily ignored by more or less the entirety of the high energy physics community. That’s because it’s a whole lot of hot air. I nonetheless can’t help but think that Eric would prefer if the theory was being attacked by mainstream physicists rather than being ignored. At least that would imply he is a threat, which is more aligned with his self-styling as a heretic and renegade. He’d rather be hated than be irrelevant.</p>

<p>I actually have a real soft spot for unorthodox outsiders who come out of nowhere and prove everyone wrong. A big part of me wants an Eric Weinstein style story to be true. Unfortunately the ultimate truth is that Eric Weinstein is a very sophisticated social illusionist who appears to have bought into his own bullshit. And to an extent, I get it; I don’t think there is a person who has gone into physics research without privately harbouring the small glimmer of hope that they one day might discover something incredible and be declared the next Einstein. Every one of us had a crazy idea or two at some point that we thought might just turn out to be mad genius. Then we all figured out we were really, really wrong and had to eat a sizeable portion of humble pie. Most of us learnt the lesson that needed to be learnt from that. Eric Weinstein didn’t. He doubled, then tripled down. He is a man whose real talent is projecting an image of an unheralded, iconoclastic genius to people who have no way of knowing better. Unfortunately it’s much easier to create that illusion than to be the real thing.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[29 Oct 2023]]></summary></entry></feed>