<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2025-05-29T21:26:36+01:00</updated><id>/feed.xml</id><entry><title type="html">Adolescence is part of the problem</title><link href="/2025/05/29/Adolescence-is-part-of-the-problem.html" rel="alternate" type="text/html" title="Adolescence is part of the problem" /><published>2025-05-29T00:00:00+01:00</published><updated>2025-05-29T00:00:00+01:00</updated><id>/2025/05/29/Adolescence%20is%20part%20of%20the%20problem</id><content type="html" xml:base="/2025/05/29/Adolescence-is-part-of-the-problem.html"><![CDATA[<p>29 May 2025</p>

<p>Adolescence is a Netflix drama that premiered in March of this year and quickly went viral. It depicts events centring on a 13 year old school boy who stabs a female classmate to death. The main themes the shows deals with are toxic masculinity, incels, the manosphere and social media. Its rapid ascent into the cultural spotlight is perhaps best illustrated by the fact that UK prime minister Keir Starmer called for it to be shown in high schools all over the country, and the UK parliament’s women and equalities committee held a hearing with the show’s producers. It is the series that seemingly launched a million conversations about men, boys and masculinity.</p>

<p>Let me start with a quick episode-by-episode synopsis:</p>

<ul>
  <li><strong>Episode 1</strong> focuses on Jamie’s arrest, processing, and questioning at the police station.</li>
  <li><strong>Episode 2</strong> follows the detectives as they visit Jamie’s school.</li>
  <li><strong>Episode 3</strong> centres on a session between Jamie and a forensic psychologist.</li>
  <li><strong>Episode 4</strong> shows a day in the life of Jamie’s family, a year after the murder.</li>
</ul>

<h3> Technical choices </h3>

<p>Before diving into the thematic content, it’s worth touching on one of the show’s most distinctive creative choices: each episode is filmed as a single, continuous shot. This approach works brilliantly in Episode 1— it amplifies the tensions and creates a sense of hyper-realism. It’s also fine for episode 3, which consists almost entirely of a dialogue between Jamie and the psychologist.</p>

<p>However, it’s a serious handicap in Episodes 2 and 4. With each instalment running about an hour and unfolding in real time, the single-take constraint puts serious limits on pacing and setting. We only get to see a relatively short, continuous period of time in roughly the same physical location, and there just isn’t enough interesting content to fill these episodes.</p>

<p>In Episode 2, for instance, the only major revelation is that Jamie was being cyber-bullied by the girl he stabbed. She rejected romantic advances he made towards her and taunted him afterwards by calling him an incel. None of the adults knew about the bullying because the messages were coded using emojis. The lead detective couldn’t even figure it out until told as much by his son, a pupil at Jamie’s school. It’s a mildly interesting development, but feels a little implausible, and isn’t nearly enough to carry a full hour.</p>

<p>Episode 4, meanwhile, shows us a day in the life of the Miller family a year after the murder. It’s clearly meant to emphasise their ordinariness, but the effect wears thin pretty quickly. There are only so many conversations about what they are having for breakfast and other minutiae of their everyday lives that you can tolerate before getting bored. The point—that they’re just a normal family—is made within the first minute. We don’t need to spend half an hour watching them engage in the humdrum of daily life.</p>

<p>If not for the single shot constraint, we would have been able to skip back and forth in time and space to see myriad interesting events related to the story, and answer all sorts of burning questions we have. We might have seen how exactly Jamie became radicalised – what kind of content did he access online, what personal experiences shaped him, what social environment did he navigate at school. We could have been shown his interactions with girls at his school, including the one he ultimately killed. But doing so would have required the writers to chart a believable path from ordinary teenager to cold-blooded killer, which would have undermined the message they were determined to smash down our throats. This brings me nicely to the thematic content.</p>

<h3> Themes</h3>

<p>Episode 1 sets the tone with a gritty, hyper-real style that is tense and compelling. It falters greatly thereafter under the weight of self-contradictory demands placed upon the characters and the plot by the writers’ insistence on suffusing the show with their completely incoherent political and cultural agenda. After episode 1, it is no longer a drama; it’s a propaganda piece.</p>

<p>The writers clearly made a deliberate choice to portray Jamie as an entirely ordinary boy: no history of violence, no involvement in gangs, no signs of delinquency. He comes from a stable, loving home. There’s no trauma in his past, and no prior indication of any violent predispositions. His favourite subject at school is art. Yet we’re asked to believe that this boy becomes a killer simply through exposure to online manosphere content.</p>

<p>The message the writers are sending is that any young boy, no matter how normal, can become a killer, so long as he has an internet connection. That is not how the real world works. Violent criminals are disproportionately</p>
<ul>
  <li>from broken, single-parent homes</li>
  <li>victims of childhood abuse or trauma</li>
  <li>involved in gangs or other criminal behaviour.</li>
</ul>

<p>Homicide and knife crime overwhelmingly involves young men attacking each other and not women or girls. And rarely, if ever, are such acts triggered by romantic rejection.</p>

<p>In fact, no crime even vaguely resembling that depicted in Adolescence has ever happened, and its unlikely one ever will. Even if you look at the only incident of incel homicide that has ever happened in the UK - the Plymouth shooting carried out by Jake Davison in 2022 - you will not find any commonality. Davison killed his own mother, and then seemingly indiscriminately shot at and killed 5 other people, including men, women and children. His actions were horrific and tragic, but bear no relation to this series.</p>

<p>The glaring oxymoron at the centre of the show is most clearly on display in episode 3. The  message requires that Jamie simultaneously inhabits a number of mutually contradictory personalities that he transitions between with whiplash-inducing speed:</p>
<ul>
  <li><strong>The affable teenager</strong>. Jamie starts off the interview as a charming, playful, endearing young boy.</li>
  <li><strong>The aggressive delinquent</strong>. He flies into bouts of rage over minor provocations, yelling and make violent gestures towards the psychologist.</li>
  <li><strong>The psychopath</strong>. At a few very obvious, jarring moments, Jamie abruptly becomes a psychopath. The expression on his face darkens, his words become calculated and cruel, and with eerie composure he shows his ability to manipulate and get inside the head of the psychologist.</li>
  <li><strong>The vulnerable child</strong>. Just as suddenly, he becomes a vulnerable, confused, defenceless lamb. Fragile, overwhelmed and unable to control his inner emotional turmoil, he all but confesses to the murder.</li>
</ul>

<p>The crowning idiocy of this episode comes in the final moments, when the psychologist appears to only realise Jamie’s guilt after the quasi-confession. That’s hard to reconcile with what we already know. In episode 1, the physical evidence is presented to Jamie and it is damning – there is CCTV footage of him committing the murder. There is no way that anyone could have any doubt that it was him. By the end of the first episode, even his father, initially stalwart in his conviction that Jamie is innocent, has given up and accepted his guilt.</p>

<p>As Jamie is forcefully escorted from the interview room, seemingly having sealed his own fate, what is his parting concern? Remorse? Fear? Actually, it’s whether the psychologist is romantically interested in him. Presumably this is meant to be the mask coming off fully, showing Jamie’s violent entitlement toward women. 
After he leaves the room, the psychologist breaks down in tears. This is a trained expert that we can safely assume is exposed to far more horrific, pathological and distributing behaviour on a daily basis. Yet here she is, reduced to a puddle of tears by a barely pubescent boy raising his cracking voice at her.</p>

<p>None of this makes any sense. Fiction must to be consistent with the level of suspension of disbelief it demands from us. The events depicted in adolescence are far more believable than what you will find in e.g. your average 80s action flick – but at least those films retain a coherent internal logic. If you’re going to turn the level of realism up to 11 in the episode 1, you can’t be adrift in a fantasy of your own making by episode 3. It is incongruent and stupid.</p>

<h3> Conclusion </h3>

<p>None of this would be noteworthy if this series was being received as it deserves to be: A mildly interesting piece of fiction that in the end has little or nothing useful to say about reality. Unfortunately, that’s not what the makers are pitching, and seemingly it’s not what much of the audience is catching. The show is not called <strong><em>“A completely implausible fictional story that has little or nothing to do with reality, that shouldn’t be mistaken for meaningful social commentary, and definitely shouldn’t be used as a basis for policy-making – that would be completely fucking mad!”</em></strong>. It’s called Adolescence – we’re meant to believe this is the new, modern reality of boys and young men growing up.</p>

<p>As it is, the show is so desperately trying to virtue signal itself as a part of the solution that it lands squarely on the other side of the ledger. It just gave everyone a little bit more social permission to treat boys and young men as if they are one YouTube video away from murdering someone. If you’re looking for an explanation for the rise of Andrew Tate and his ilk, look no further. As reprehensible as Tate is, at least he doesn’t treat young men as if they are the scum of the Earth. At least he gives them something to aspire to.</p>

<p>By contrast this show, and our broader culture, is hell bent on telling young men that they evil patriarchal oppressors, would-be rapists and murderers in the making. That’s an ideology that used to be confined to the fringes of radical feminism; it’s now mainstream and embedded throughout our culture and institutions. And it’s devastating. For young men to have that relentlessly drilled into them, when their default setting is to absolutely cherish and adore women, is soul-crushing. 60 years on from second wave feminism, and the most aspirational message feminism has for young men is “If you try really hard, maybe you won’t be a complete piece of shit”. Keep treating boys and young men like they’re scum, see if that fixes the problem.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[29 May 2025]]></summary></entry><entry><title type="html">The trouble with economics part 2</title><link href="/2025/04/02/The-trouble-with-economics-part-2.html" rel="alternate" type="text/html" title="The trouble with economics part 2" /><published>2025-04-02T00:00:00+01:00</published><updated>2025-04-02T00:00:00+01:00</updated><id>/2025/04/02/The%20trouble%20with%20economics%20part%202</id><content type="html" xml:base="/2025/04/02/The-trouble-with-economics-part-2.html"><![CDATA[<p>02 Apr 2025</p>

<p>In a <a href="https://www.drstevenkerr.com/2024/10/02/The-trouble-with-economics.html" target="_blank"> previous post</a>, I had a look at some major problems in the theoretical foundations of pretty much all macroeconomic models. To recap, they were as follows:</p>

<ul>
  <li> <b>No money </b>
    <p>Hard as it is to believe, the models that underpin modern macroeconomics do not have money. They are actually models of frictionless bartering. </p>
  </li>
  <li> <b>No unemployment </b>
    <p>Markets always clear, so there is no such thing as unemployment. People waiting in line at the dole office are choosing to do this as a leisure activity.  </p>
  </li>
  <li> <b>No price setting </b>
    <p>No one sets or influences prices. They are sent down as manna from heaven. </p>
  </li>
  <li> <b>No strategy </b>
   <p>Everyone operates in a strategic vaccuum where no one else's behaviour affects their own.
</p>
  </li>
    <li> <b>One consumer and one firm </b>
   <p>For the sake of simplicity, macroeconomic models often treat an entire country's economy as if it consists of exactly one consumer and one firm. 
</p>
  </li>
</ul>

<p>Of course this all looks pretty bad. But it turns out it’s actually not so easy as you might think to improve the situtation.</p>

<p>It’s one of the tasks I set about during my PhD. I believe I had a lot of success, coming up with a new class of models that addresses all of the above problems.</p>

<p>You would think the economics academy would be very welcoming of this.</p>

<p>And you would be wrong.</p>

<h3> Who hurt you Steven </h3>

<p>So as a young, bright-eyed economist who worked very dilligently trying to solve major outstanding problems in the field, I was expecting at least some interest in my ideas. I mean, sure, maybe there wouldn’t be a parade <em>straight away</em>, maybe it would have to wait a little, but at least a teensy acknowledgement of what was, if nothing else, a refreshing new take.</p>

<p>In reality, I was met with incredible hostility. I submitted my paper to multiple journals, generally receiving either a desk rejection or replies that were scathing and incoherent in roughly equal measure.</p>

<p>During this time, I was a visiting researcher at the Institute for New Economic Thinking (INET) at the University of Oxford. INET was set up in the wake of the 2008 financial crisis, in response to the complete failure of the academic economics profession to anticipate or understand the crash. The idea was to bring in outsiders and hitherto unorthodox thinking in order to shake things up.</p>

<p>I visited the group headed by <a href="https://en.wikipedia.org/wiki/J._Doyne_Farmer" target="_blank"> Doyne Farmer</a> on complexity economics. Doyne is a physicist by training, who has had incredible success across multiple fields over the course of his career including complex systems, theoretical biology and econophysics. An incredibly smart and accomplished academic, and by many accounts, a genius.</p>

<p>At the time I was trying to figure out why my papers were getting such a frosty reception. As I spoke more with Doyne and the people in his group I found out, to my shock, that they rarely if ever published in economics journals. Strange on the surface, given that they were all academic economists.</p>

<p>The reason? It turns out that they found it difficult or impossible to do so. They were working on things that were not in the mainstream, like agent-based models and chaotic systems. Not because they were crackpots or these were bad ideas, but because the mainstream had arbitarily decided to close ranks against them.</p>

<p>Learning this information caused a sort of relief in me - finally I understood that there was not some cosmic conspiracy to keep me out of economics. I was not experiencing anything new. It was simply the nature of the beast. Extreme hostility to innovation is the order of the day. Stray even a little from the accepted path, and you will be swiftly rebuked. If Doyne Farmer can’t publish in mainstream economics journals, what hope could I possibly have?</p>

<p>Not long after, I exited from economics. It was clear there was no future for me in the field.</p>

<h3> Why is economics the way it is </h3>

<p>It is useful, however, to understand why exactly I had the experience that I did, and to appreciate the full extent of the rot in economics.</p>

<h4> Economics is very insular </h4>

<p>On the face of it, you wouldn’t think economics would be an insular subject. It is so closely bound up with a multitude of neighbouring disciplines, like psychology, sociology, political science, history, computer science, even physics. Whereas, say, for someone working in mathematics, the opportunities for citing researchers from other fields are definitely naturally limited.</p>

<p>And yet, it turns out that economics researchers are <a href="https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.29.1.89" target="_blank"> far less likely to cite reseach from other fields</a>. There is absolutely no reason for this to be the case - economists should be drawing inspiration and knowledge from many other disciplines. Why aren’t they?</p>

<p>One major reason is that academic economics is something of a racket that is controlled by a relatively small number of universities in the USA. Compared to other disciplines, these universities have a strangehold on all the most prestigious journals, and <a href="https://conference.nber.org/conf_papers/f204525.pdf" target="_blank"> production of Nobel prize winning economists</a>. In other disciplines, the most celebrated researchers are much more evenly distributed across the planet. A clear signal of the institutional capture that has happened across the field.</p>

<h4> Academia is very conservative </h4>

<p>Everything in academia is reputation based. No one has the time or motivation to go over all of your work with a fine-tooth comb. There may only be a handful of people in the world who are sufficiently qualified to understand it anyway, and they’re all pretty busy. At the same time, there is no shortage of crackpots overselling shoddy work. The solution that the academy has more or less settled on to this situation relies heavily on reputation. Generally what this means is that you have to convince an expanding circle of people in your orbit that you can be trusted.</p>

<p>In many ways this is about as good a solution as we can hope to have. The people close to you are the ones who will naturally have more understanding of your work - your PhD supervisor, your collaborators, etc. First you have to convince them you’re on to something. Once you’ve done that, maybe they will be impressed and start to tell more people they know. The more impressive you are, the more people you should be able to win over to your side. If you can pull this off, it means that when you submit a paper to a journal, you greatly improve your chances of getting a reviewer who is familiar with you, or who is only a few degrees of separation away. They may be more willing to accept some things you say on faith because you have built a reputation for trutworthiness.</p>

<p>That’s how it works when things are working well. When they aren’t, the above mechanisms become channels through which nepotism and favouritism run absolutely rampant, and access to publishing in journals is monopolised by a small group of people/institutions. This brings me to my next point.</p>

<h4> The economics journal system is broken </h4>

<p>As someone who has worked in multiple academic fields, I can say without hesitation that the journal system in economics is hands down the most labyrinthine, kafkaesque and academically stifling I have ever experienced.</p>

<p>For example, it is not unusual for it to take three or more years between the first time you submit an article to a journal, to it actually being published. Why does it take so long? What is happening in that time? It can often take more than a year just to hear back from the reviewers, which is a ludicrous turnaround time. And remember you will likely be rejected multiple times before eventually being accepted for publication.</p>

<p>There seems to be little or no correlation between quality of work and its likelihood of being accepted in a high end journal. I have heard several economists express this <em>about their own papers</em>. Genuine bafflement at the papers that have ended going to better journals versus the ones that have been relegated to the lower leagues.</p>

<p>I also know of multiple individuals who have permanent or tenure track positions at universities who have published somewhere between zero and two papers. That’s right - people on track for professorial positions <em>without ever having published anything</em>.</p>

<p>There is absolutely no reason for this to happen. It stifles the production of knowledge and strangles innovation. The only reason it does happen is because the field has been captured by special interests and stripped of its academic integrity.</p>

<h3> Conclusion </h3>

<p>In my view, academic economics is probably beyond being salvaged. A tremendous amount of time, effort and brainpower goes into producing remarkably little genuine or useful knowledge about the world. Nepotism runs amok, and the whole field is run more like a cartel than a free market. This is not how a discipline that aspires to be called a science can or should ever work.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[02 Apr 2025]]></summary></entry><entry><title type="html">Does citation count measure how many friends you have?</title><link href="/2025/02/16/Does-citation-count-measure-how-many-friends-you-have.html" rel="alternate" type="text/html" title="Does citation count measure how many friends you have?" /><published>2025-02-16T00:00:00+00:00</published><updated>2025-02-16T00:00:00+00:00</updated><id>/2025/02/16/Does%20citation%20count%20measure%20how%20many%20friends%20you%20have</id><content type="html" xml:base="/2025/02/16/Does-citation-count-measure-how-many-friends-you-have.html"><![CDATA[<p>16 Feb 2025</p>

<p>Alright folks, I’ll fess up right off the bat; I may have fallen face first into <a href="https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines" target="_blank"> Betteridge’s law</a></p>

<blockquote>
Any headline that ends in a question mark can be answered by the word no.
</blockquote>
<p><br />
Leaving aside the empirical support for this adage, it is true that the answer to the eponymous question is ‘no’. But, I’m trying to provoke discussion here alright. Gimme a break.</p>

<p>The lackadaisicalness of paper authorship policies varies quite a bit across different fields of academia. I’ve worked in several, and I will at least say this. Disciplines that are 
 a little further along the spectrum tend to be much more solitary affairs. The number of authors on your typical physics paper is pretty low; on your typical life sciences
 paper, much higher. There’s a clear positive relationship between the number of authors on a paper and how long they will spend making eye contact with you in a 
 conversation.</p>

<p>It is, however, not uncommon across academia for authorship standards to be pretty much non-existent. For example, a small number of people will do all the work, and then
  send the paper to dozens of others for ‘comments and feedback’. It’s possible to get your name on a paper with just a few mins of light exertion. That’s less than your average 
  <a href="https://www.youtube.com/watch?v=o-50GjySwew" target="_blank"> prancercise workout</a>.</p>

<p>Closely related is the `reciprocal co-authorship’ arrangement: Person A lets person B be a co-author on their paper with minimal contributions, with an unspoken
understanding that the favour will be returned at some date in the future.</p>

<p>One consequence of this is that there are a non-trivial number of researchers whose paper and citation count is, at first sight, utterly baffling. In the field 
I currently work in - health data science - it typically takes at least a year and often several to write a paper. This includes going through the extremely tedious process of 
acquiring permissions to access data, writing protocols, obtaining ethics approvals, etc. Then you have to do the analysis, get feedback, tweak it, etc for multiple rounds. 
After that comes actually writing the paper, followed by more rounds of tweaking. Submit it, get rejected a few times before it finally gets accepted somewhere, but not before at least 
one round of peer review where you often have to basically write another paper defending the first paper.</p>

<p>To put it straightforwardly, there is simply no way one person can make really signficant contributions to this workload for a large number of papers. And yet, there are researchers 
whose yearly paper count is in the high 10s or even 100s.</p>

<p>There is only really one way that this can happen, excepting outright academic fraud. These are people who have lots of friends, and lots of mutual back-scratching arrangements.</p>

<p>It only makes sense. Academics live and die by their publications. If it is possible to get your name on a paper with a tiny fraction of the effort it would require to actually do 
the bulk of the work yourself, then people are going to take advantage of that, some in a really quite cynical way. Practices like those above are so commonplace, I would say that 
in some fields it is pretty much the norm for everyone except the first and last authors on a paper to be ignored. It is fairly widely accepted that anyone in the middle is a tag-along.</p>

<p>This brings me to my next adage, known as <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law" target="_blank"> Goodhart’s law</a>,</p>

<blockquote>
When a measure becomes a target, it ceases to be a good measure.
</blockquote>
<p><br />
This one will be familiar to many people in academia. People whose citation count, in the 10s of thousands, or even in excess of 100,000, whose main expertise seems to be ‘playing the game’. The ones who talk a lot during meetings without actually saying anything, who have a penchant for inserting themselves into other people’s projects. Hierarchy inversions that border on comical, where the lowly code monkeys and technical staff have pitiful citation counts, but their competence seems to eclipse someone else with a monumental citation count.</p>

<p>It’s a little depressing, but I don’t have a good solution. Just a word of caution to be wary of citation count as a metric of academic contribution. Often it is measuring something
else entirely.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[16 Feb 2025]]></summary></entry><entry><title type="html">Academia fails the smell test</title><link href="/2025/01/07/Academia-fails-the-smell-test.html" rel="alternate" type="text/html" title="Academia fails the smell test" /><published>2025-01-07T00:00:00+00:00</published><updated>2025-01-07T00:00:00+00:00</updated><id>/2025/01/07/Academia%20fails%20the%20smell%20test</id><content type="html" xml:base="/2025/01/07/Academia-fails-the-smell-test.html"><![CDATA[<p>07 Jan 2025</p>

<p>I’m a bit behind the outrage news cycle here, but In November last year, a student at the University of Cambridge by the name of Amelia (Ally) Louks unintentionally created quite the stink over her PhD thesis entitled “The Politics of Smell in Modern and Contemporary Prose”. It all started with a seemingly innocuous  <a href="https://x.com/DrAllyLouks/status/1861872149373297078" target="_blank"> twitter post</a> celebrating successful completion of her degree, and quickly spiralled into a furore that “broke the internet”.</p>

<p>Why? To put it gently, her thesis looks like garbage. For example, the second sentence of the abstract lays out her aim to</p>

<blockquote>
...offer an intersectional and wide-ranging study of olfactory oppression by establishing the underlying logics that facilitate smell's application in creating and subverting gender, class, sexual, racial and species power structures.
</blockquote>
<p><br /></p>

<p>For most people that fails the sniff test. The stench of pseudointellectual equine fecal matter is frankly overpowering. Surely only a particularly niche brand of masochist would be interested in subjecting themselves to what is no doubt the ordeal of hundreds of pages of this.</p>

<p>Which leaves many people wondering; how on Earth can someone get a PhD from one of the pre-eminent higher education institutes in the world by producing what appears to be an elaborately curated, heaping monument to obscurity and the unmistakable scent of pasture?</p>

<h3> Fashionable nonsense </h3>

<p>I share the disheartenment of the internet. In a sane world, this document would have been ignored by everyone as the esoteric and pointless ramblings of someone who has the comfort and privilege of being able to waste their time on such trivialities. It should not result in a doctoral degree from one of the most prestigious institutions on the planet.</p>

<p>Unfortunately, Louks’ thesis is not some isolated incident. It is just the tip of the iceberg. There are mountains of garbage just like it in academia. Entire industries are dedicated to producing this garbage.</p>

<p>How did this happen? Part of it is an insistence in many quarters on treating humanities subjects e.g. English literature, anthropology, gender studies, art etc as if they are on a par with the natural sciences - physics, chemistry, biology etc. The problem is that they are fundamentally different. In the natural sciences, there are real discoveries to be made, and real research to be done. There is knowledge that require serious time and effort to master, and it can take the better part of a lifetime to make a notable contribution to these fields. And no one can argue about the results; the achievements of science are absolutely extraordinary.</p>

<p>In the humanities, there is not much of any of that. Nonetheless there is a hierarchy to be constructed - somebody must, after all, be the leading gender studies scholar in the world. In the absence of the potential for real scientific competence or truth-seeking, that hierarchy becomes predicated on something else entirely - one’s ability to dress up mundane or silly ideas in a way that superficially makes them sound just as impressive as actual science. Noam Chomsky nails it in <a href="https://youtu.be/OzrHwDOlTt8?t=343" target="_blank"> this clip</a>,</p>

<blockquote>
Suppose you're a literary scholar at some elite university, or anthropologist or whatever. If you do your work seriously, that's fine. But you don't get any prizes for it. On the other hand, you take a look at the rest of the university, and you've got these guys in the physics department, and the math department, and they have all kind of complicated theories, which of course we can't understand, but they seem to understand them, and they have principles and they deduce complicated things from the principles, and they do experiments and find either they work or they don't work. Thats really impressive stuff, so I want to be like that too. I want to have a theory. 
<br />
<br />
In the humanities, literary criticism, anthropology and so on, theres a field called theory. We're just like the physicists. They talk incomprehensively, we can talk incomprehensively. They have big words, we'll have big words. They draw far-reaching conclusions, we'll draw far-reaching conclusions. We're just as prestigious as they are. Now if they say look well look we're doing real science and you guys aren't, that's white, male, sexist, bourgeois, whatever the answer is, how are we any different from them? Ok that's appealing.
</blockquote>
<p><br /></p>

<p>It really is that simple. And depressing.</p>

<p>All of this was exposed quite brilliantly by Alan Sokal in 1996, who has been a professor of physics at New York University and University College London. He submitted a fake article to the journal Social Text entitled “Transgressing the Boundaries: Towards a Transformative Hermeneutics of Quantum Gravity”, arguing that quantum gravity is a social construct. Needless to say, the paper was filled with nonsense. It nonetheless got published. In 1998, he wrote the book 
<a href="https://en.wikipedia.org/wiki/Fashionable_Nonsense" target="_blank"> Fashionable nonsense</a> criticising the postmodern academic tradition that resulted in the unfortunate institutional circumstances that allow this sort of thing to happen.</p>

<p>This was followed up by the <a href="https://en.wikipedia.org/wiki/Grievance_studies_affair" target="_blank"> Grievance studies affair</a> in 2017 and 2018, by Peter Boghossian, James Lindsay and Helen Pluckrose. They wrote 20 articles filled with patently absurd nonsense and submitted them to various peer-reviewed journals. By the time the hoax was revealed in October 2018, 4 papers had been published, 3 were accepted but not published, 6 were rejected and 7 were under review. The published papers included claims that dogs participate in rape culture and that men can reduce their transphobia by using anal sex toys on themselves.</p>

<p>Further evidence of the rot comes from <a href="https://en.wikipedia.org/wiki/Judith_Butler" target="_blank"> Judith Butler</a>, who is the Maxine Elliot professor in the department of comparative literature and the program of critical theory at the University of California, Berkeley. That is a named professorship at one of the most elite universities in the world. In her most famous work, <a href="https://en.wikipedia.org/wiki/Gender_Trouble" target="_blank"> Gender trouble</a> she argues that gender is not innate, but a performance. Which, depending on how you define gender, is either a truism or an absurdity. Butler is famous for her crimes against the English language, illustrated nicely with this literal award-winningly awful sentence</p>

<blockquote>
The move from a structuralist account in which capital is understood to structure social relations in relatively homologous ways to a view of hegemony in which power relations are subject to repetition, convergence, and rearticulation brought the question of temporality into the thinking of structure, and marked a shift from a form of Althusserian theory that takes structural totalities as theoretical objects to one in which the insights into the contingent possibility of structure inaugurate a renewed conception of hegemony as bound up with the contingent sites and strategies of the rearticulation of power.
</blockquote>
<p><br /></p>

<p>This leads me to what I call <strong>The Butler Challenge</strong>™: Choose <strong>any</strong> book by Judith Butler, flip to <strong>any</strong> page, choose <strong>any</strong> paragraph, and find me a sentence that isn’t complete gibberish. I offer a handsome bounty to anyone who succeeds in this daring endeavour.</p>

<h3> State-sponsored ideological nonsense </h3>

<p>If this was all just blather done in the name of light entertainment that would be one thing. It is altogether another thing when the humanities departments at universities are mass indoctrinating students with a particular ideology and sending them out in the world with revolutionary intentions. And on taxpayers’ dime to boot. Tens of millions of pounds are spent funding anti-scientific “research” in univerities. This is stuff that is on a par with flat earth theory, except far more socially destructive. That is unacceptable.</p>

<p>Unfortunately this shows no signs of abating anytime soon.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[07 Jan 2025]]></summary></entry><entry><title type="html">The trouble with economics</title><link href="/2024/10/02/The-trouble-with-economics.html" rel="alternate" type="text/html" title="The trouble with economics" /><published>2024-10-02T00:00:00+01:00</published><updated>2024-10-02T00:00:00+01:00</updated><id>/2024/10/02/The%20trouble%20with%20economics</id><content type="html" xml:base="/2024/10/02/The-trouble-with-economics.html"><![CDATA[<p>02 Oct 2024</p>

<p>In this post, I’m going to give an overview of some major theoretical issues in modern economics.</p>

<h3> Supply and demand </h3>
<p>In a typical undergraduate economics degree, you spend a lot of time drawing pictures like this:</p>

<div style="text-align: center;">
    <img src="../../../images/supply_demand.png" width="500" height="500" class="center" />
</div>

<p>I don’t think its an exaggeration to say that a major chunk of the syllabus is pretending these diagrams are deeper and have far more explanatory power than they really do. It is, after all, just a graph with two lines.</p>

<p>The basic idea is that when prices go up, demand goes down and supply goes up. The two lines have to meet somewhere, and this is the price that clears the market. Under ‘normal’ circumstances, this is the price that actually prevails (or so the story goes).</p>

<h3> General equilibrium theory </h3>

<p>By the time you get round to doing a masters in economics, you learn the grown up version of supply and demand diagrams - it goes by the name general equilibrium theory. Here there are many goods that are traded concurrently. Each person has preferences and a budget. A major theoretical result in economics is the proof, under fairly general assumptions, that there is always at least one set of prices that clears the market.</p>

<p>General equilibrium theory is the theoretical foundation for modern macroeconomic modelling. Pretty much every mainstream macroeconomic model is either an example of a general equilibrium model, or it is derived from one. This includes models that are routinely used by major financial institutions like central and commercial banks, government treasuries etc for economic forecasting. For example, the ubiquitous <a href="https://en.wikipedia.org/wiki/Dynamic_stochastic_general_equilibrium" target="_blank"> ‘dynamic stochastic general equilibrium’</a> models.</p>

<p>The trouble with general equilibrium theory is that it has features baked in that are in spectacular disagreement with reality.</p>

<h4> No money </h4>

<p>In general equilibrium theory, there is no money. You might be puzzled by this - how can it be that the foundational macroeconomic modelling strategy of major financial institutions throughout the world does not have money? Is money not a central focus of the entire subject of economics?</p>

<p>Well yes, it is, but turns out it’s a lot easier to make models without it. First consider the useful role that money serves</p>

<ul>
  <li> <b>Medium of exchange </b>
    <p>This is perhaps the primary and most important function of money - facilitating the exchange of goods. Without money, we would have a bartering economy in which a great deal of time and effort would be spent engaging in chains of trades that have the desired outcome. For example, if I have a cow and I would like to get a sack of potatoes, I would either have to find someone who has potatoes and wants a cow, or find a series of similar trades that eventually ends with my desired result. </p>
  </li>
  <li> <b>Store of value </b>
    <p>Money provides a means through which you can e.g. provide labour now in exchange for consumption later. For example, you can go to work tilling the fields, get paid into your bank account, then later use that money to buy a four-slice toaster. </p>
  </li>
  <li> <b>Standard of deferred payment </b>
    <p>This is closely related to the function of money as a store of value, but in reverse. That is, you can get consumption now in exchange for work later. This is achieved by borrowing money to fuel today's consumption, and paying back at a future date.</p>
  </li>
  <li> <b>Unit of account </b>
   <p>Finally, money provides a measuring stick for value. If a good can be sold for £1, we have a numerical measure of its value relative to other goods.
</p>
  </li>
</ul>

<p>Any model that includes money must capture all of these functions.</p>

<p>In general equilibrium theory, there is a set of market-clearing prices that certainly serve as a unit of account. But none of the other much more important functions of money are captured at all.</p>

<p>In particular, money does not serve as a medium of exchange. Prices that clear the market are found, then all the goods are magically redistributed in such a way that the value of the bundle each person receives is exactly equal to the value of the bundle they give up. In the real world, this would obviously be accomplished by carrying out all transactions with bits of paper we call money, where the number of pieces of paper required to buy a good is  equal to its equilibrium price. This can happen because we have a society-wide agreement to treat the bits of paper as if they are valuable. This can never happen in general equilibrium theory - there is no room in the model for social constructs or government fiat. And in their absence, why would anyone accept useless bits of paper in exchange for real, intrinsically useful goods? This also implies the equilibrium price of money is zero, rendering it useless as a store of value or a standard of deferred payment.
The result is that general equilibrium theory is really a theory of frictionless, money-free bartering. Which, you may have noticed, is somewhat lacking in realism.</p>

<h4> No unemployment </h4>

<p>In general equilibrium theory, market-clearing prices always prevail. Excess supply or demand is assumed away - it simply cannot happen. That is rather unfortunate because unemployment, perhaps the central quantity of interest in macroeconomic forecasting and policymaking, is a failure of labour markets to clear. In general equilibrium theory, unemployment simply does not exist.</p>

<h4> No price-setting </h4>
<p>In general equilibrium theory, no one sets prices. They are simply sent down as manna from heaven. Everyone calculates what bundle they would buy at every possible set of prices, and divine intervention selects a set of prices that clears the market. At no point does it occur to anyone that they might be able to influence prices.</p>

<h4> No strategy</h4>
<p>General equilibrium theory is not a game. Agents in the model behave as if choosing from a menu of options in complete isolation from everyone else. There is no notion that anyone else’s behaviour might affect your payoff. This is obviously completely at odds with the real world, where firms ruthlessly strategise against each other to maximise their profits.</p>

<p>These are just the failings of general equilibrium theory. However, standard macroeconomic models are in many ways even worse, because they make simplifying assumptions that even more thoroughly divorce the model from reality. In particular, they typically model the economy of an entire country as if it contains exactly one person, one firm, and a handful of goods that are traded. In case that didn’t strike you as obviously doomed to failure, there are theorems indicating you just can’t do this (see the <a href="https://en.wikipedia.org/wiki/Gorman_polar_form" target="_blank"> Gorman aggregation theorem</a>, the <a href="https://en.wikipedia.org/wiki/Cambridge_capital_controversy" target="_blank"> Cambridge capital controversy</a>).</p>

<h3> To summarise</h3>
<p>Standard macroeconomic models that are routinely used by major financial institutions to model entire nations are typically based on frameworks that do not have money and operate through frictionless bartering, where there is no unemployment, sellers cannot choose what price they offer to sell at, there is no strategic considerations, there is precisely one consumer and one firm, and only a handful of goods are traded. And many economists seem to be pretty satisfied with this state of affairs.</p>

<p>Yes, I find it hard to believe too.</p>

<p>When I worked in economics, I was most certainly not happy with this state of affairs. I came up with an alternative framework that in principle fixes many or all of these problems. What I quickly discovered, though, is that for various reasons my work was virtually unpublishable in mainstream economics journals. That is, however, a story for another day.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[02 Oct 2024]]></summary></entry><entry><title type="html">I for one welcome our AI overboard</title><link href="/2024/07/11/I-for-one-welcome-our-AI-overboard.html" rel="alternate" type="text/html" title="I for one welcome our AI overboard" /><published>2024-07-11T00:00:00+01:00</published><updated>2024-07-11T00:00:00+01:00</updated><id>/2024/07/11/I%20for%20one%20welcome%20our%20AI%20overboard</id><content type="html" xml:base="/2024/07/11/I-for-one-welcome-our-AI-overboard.html"><![CDATA[<p>11 Jul 2024</p>

<p>I’ve been very reticent to comment on the current AI hype train, mostly because it’s very easy to be wrong in a bull market. But you will be relieved to know I am now ready to put my hat in the ring.</p>

<p>First I would like to say that, like everyone else, I’m deeply impressed by recent advances in AI, particularly large language models (LLMs). They can give you flawless explanations of topological quantum field theory in one breath, then seamlesly switch to a nuanced discussion of the history of cartography. Tailor-made answers to your burning questions on heraldry, hermeneutics, object-oriented programming, epistemology, and just about any other domain of expert human knowledge your heart desires. They’re really quite amazing. I would be extraordinarily proud if I had been part of the teams that brought LLMs to the world.</p>

<p>But in the wake of the current wave of exuberance, I feel compelled to point out the manifold ways in which LLMs are extremely dumb. Here is but a partial list of tasks that state-of-the-art LLMs have failed at spectacularly:</p>

<ul>
  <li>Basic arithmetic, e.g. multiplying together two 3-digit numbers.</li>
  <li>Understanding relations, e.g. if X is Y, then Y is X. If you ask ‘Who is Olaf Shhulz?’, you can correctly be told that he is the 9th Chancellor of Germany, and then immediately be given a different answer to the question ‘Who is the 9th Chancellor of Germany?’.</li>
  <li>Creating a 3x3 word search containing specified 3-letter words.</li>
  <li>Name three famous people who all share the exact same birth date and year.</li>
  <li>Counting e.g. the numbers of letters in a string, number of paragraphs in a document, etc.</li>
  <li>Simple logical problems, with uncommon variations. For example, <a href="https://en.wikipedia.org/wiki/River_crossing_puzzle" target="_blank"> river-crossing problems</a>, but with no restrictions on which which items can be taken or left together.</li>
  <li>Spatial reasoning. For example, ‘Four children - Alex, Bella, Charlie, and Dana - are sitting around a picnic table. Alex is facing Bella. Charlie is sitting to the right of Bella. Who is sitting to the left of Alex?’.</li>
</ul>

<p>You can find a more in depth exploration of various LLM fails in the appendix of <a href="https://arxiv.org/pdf/2405.19616" target="_blank"> this paper</a>.</p>

<p>What these examples indicate is that LLMs don’t have any understanding of the world that is in any way similar to what you or I have. For example, if you give me the word ‘chair’, it brings to my mind a lifetime’s worth of relevant physical, sensory information. I know what they look and feel like. I know the precise purpose they serve, having experienced the discomfort of having to stand for prolonged periods of time. I know that chairs can be used in all sorts of ways that have little or nothing to do with their intended purpose, because I have actually done it before (wink wink). LLMs have none of this. What they have is, loosely speaking, a bunch of words that they associate with chairs from being exposed to internet text in which the word chair appears.</p>

<p>What this means is that I understand the concept of a chair and its manifestation in the real world in a way that guarantees I’m not going to make obvious, stupid errors that LLMs sometimes make. I can also exhibit chair-related creativity, like finding novel uses that you won’t find written about anywhere on the internet (nudge nudge), because I have an understanding of the physical reality of a chair that is more than just a cloud of associated words.</p>

<p>In short, LLMs may very well be <a href="https://en.wikipedia.org/wiki/Stochastic_parrot" target="_blank"> stochastic parrots</a>. To the extent they give good answers, it is only because human experts have provided good answers to a similar question somewhere on the internet that the LLMs have cobbled together. In a very real sense, you might think of LLMs as souped-up versions of search engines that actually construct a tailor-made, friendly prose answer to your particular query instead of making you spend hours piecing it all together from a collection of top hits from stack exchange, academic papers and obscure blog posts. Or, as Noam Chomsky bluntly put it, ‘high-tech plagiarism’.</p>

<p>Another way to think of an LLM is a smart 10 year old that has read and memorised a textbook on quantum mechanics. If you ask them typical questions about the subject, they may be able to give you impressive-sounding answers. Hell, they might even be able to convince you they are a child prodigy. But they have zero understanding of the subject, a fact which will manifest itself as soon as you venture even a little bit outside of what can be constructed by placing roughly the right words in roughly the right order in a superficial act of mimicry.</p>

<p>In light of this, I find it very difficult to take seriously any notion that we are blindly speeding into an AI-driven apocalypse. I recall seeing an online exchange where one person was taking as a sure harbringer of our impending doom the idea that LLMs were on the brink of being able to consistently perform arithmetic. To which they received the rather acerbic reply <em>‘So can my fu**ing calculator’</em>. Well, quite. And the calculator doesn’t need billions of parameters, hundreds of talented young minds, the entire text of the internet and the power supply of a small country to create.</p>

<p>The most popular version of the AI apocalypse scenario goes something like: we will soon create <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" target="_blank"> artificial general intelligence (AGI)</a> that exceeds human capabilities across a wide range of cognitive tasks. This will kick-off a chain of recursively improving AIs, ultimately leading to a superintelligence that will be able to outwit and outperform all of humanity across the board, leaving us utterly at its mercy. If this superintelligence happens to have an objective that is not carefully aligned with the interests of humans, it could all go seriously tits up. Take the so-called paperclip maximiser, dreamt up by philosopher Nick Bostrom,</p>

<blockquote>
Suppose we have an AI whose only goal is to make as many paper clips as possible. The AI will realize quickly that it would be much better if there were no humans because humans might decide to switch it off. Because if humans do so, there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into paper clips. The future that the AI would be trying to gear towards would be one in which there were a lot of paper clips but no humans.
</blockquote>

<p><br />
Are you scared yet folks?</p>

<p>Well no, not really. To me, this stuff just sounds like the fevered, masturbatory fantasies of sex-starved sci-fi fanboi man-children. In fact, that description might be eerily close to the truth. Exhibit A: Bostrom himself. Exhibit B: Elizier Yudkowsky, leading AI-doomer, and a man who thinks it’s a positively spiffing idea for superpowers to <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/" target="_blank"> conduct pre-emptive military strikes on each others’ AI labs, running the risk of precipitating a nuclear holocaust, in order to prevent the development of AGI</a>. I suppose the hundreds of millions of lives that would be lost is a small sacrifice at the altar of his very serious sci-fi fantasies. Incidentally, he’s the author of <a href="https://en.wikipedia.org/wiki/Harry_Potter_and_the_Methods_of_Rationality" target="_blank"> Harry Potter and the Methods of Rationality</a>, which is basically a sci-fi reimagining of Harry Potter. These people are clearly drawing heavily on a literary tradition that has enthralled us for many decades to weave their apocalyptic tales.</p>

<p>Given that cutting edge LLMs are recommending people <a href="https://news.sky.com/story/glue-cheese-to-pizza-and-eat-rocks-says-googles-new-ai-feature-as-mistakes-flood-social-media-13142528" target="_blank"> eat rocks as part of a healthy diet and glue cheese to pizzas to prevent slippage</a>, the latter based purely on the shitposting of a presumed teenage internet edgelord, I think we can confidently say we are safe for the time being. More generally though, the AI technology singularity scenario requires a long chain of reasoning, each step of which is, to put it gently, <em>extremely</em> speculative. Maybe it will take 50 more quantum leaps, each of a similar magnitude to that which brought us LLMs, to get to AGI. And there is nothing inevitable about each step of the putative self-improving AI explosion. Who knows if any of this is even possible, or if it is, whether it will happen. Maybe there are hard limits to what can be done, not least of which are the tremendous amount of energy and data that appear to be required to train these AIs.</p>

<p>The whole thing sounds like a big <a href="https://en.wikipedia.org/wiki/Conjunction_fallacy" target="_blank"> conjunction fallacy</a> to me. That’s when a specific proposition is thought to be more likely than a more general proposition of which it is a special case. For example, consider the following experiment carried out by Amos Tversky and Daniel Kahnemann where they asked a group of policy experts to rate the probability of the following events:</p>

<ol>
  <li>Russia invades Poland</li>
  <li>Russia invades Poland and the US breaks diplomatic relations with them the following year.</li>
</ol>

<p>They collectively assigned a 1% chance to option 1, and a 4% chance to option 2. This, of course, cannot be the case since 2 is a special case of 1. What might be going on here is that the participants find it difficult to imagine a scenario in which Russia invades Poland and thus assign it lower probability than a more detailed scenario that is more representative of what a real scenario would look like.</p>

<p>And so it is with AI-doomer fantasies. Most people didn’t spend the requisite amount of time in their parents basement during adolescence to be able to magick up the paperclip maximiser from the depths of their imagination. But given a fantastical sci-fi inspired story explaining one way it <em>could</em> happen they start to wildly overestimate its probability.</p>

<p>I find this fixation all particularly hard to swallow given that at this particular moment in history, we may well be on the brink of global nuclear war. Nuclear weapons pose a far bigger and proven threat to humanity that we have basically been ignoring for decades. Anyone who thinks otherwise should read the story of <a href="https://en.wikipedia.org/wiki/Vasily_Arkhipov" target="_blank"> Vasily Arkhipov</a>, the razor-thin, one-man margin that stood between us and nuclear annihilation back in 1962. There have been several more close calls since then too. Let’s be clear about this: <strong>The only reason the nuclear armageddon hasn’t happened yet is pure luck</strong>. The USA may be about to lose its position as global hegemon, and shows no indication of going quietly or peacefully. Countries around the world are increasing military spending and reintroducing the draft. Sorry folks, but I have bigger fish to fry than a damn paperclip maximiser.</p>

<p>My prediction for AI is that it will lead to disruption in some industries, and some handsome payoffs in increased efficiency. The AI doomers will continue their fearmongering, and for them AGI will be like fruit above Tantalus - always within reach, never in our hands.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[11 Jul 2024]]></summary></entry><entry><title type="html">Understanding case control studies</title><link href="/2024/06/04/Understanding-case-control-studies.html" rel="alternate" type="text/html" title="Understanding case control studies" /><published>2024-06-04T00:00:00+01:00</published><updated>2024-06-04T00:00:00+01:00</updated><id>/2024/06/04/Understanding%20case%20control%20studies</id><content type="html" xml:base="/2024/06/04/Understanding-case-control-studies.html"><![CDATA[<p>04 Jun 2024</p>

<p>Case control studies are a staple in the epidemiologist’s toolbox. The basic idea idea is that instead of carrying out an analysis on a random sample of a population, you instead sample from the population conditional on occurrence of an outcome of interest. Typically this will involve selecting all individuals who have the outcome (the cases), and only some people who don’t have the outcome (the controls). The motivation for doing this is that when the outcome is rare, most of the precision in estimates is driven by the cases, and adding a large number of controls often has little effect. Case control studies can thus be more efficient in terms of data collection/processing compared to e.g. a cohort study.</p>

<p>Case control studies have been instrumental in establishing many useful research findings, such as the link between smoking tobacco and lung cancer. Despite this, the exact workings of case control studies are poorly understood by many professionals who use them routinely. I recently co-authored an article in the Journal of Global Health with world-leading statistician and epidemiologist <a href="https://en.wikipedia.org/wiki/Sander_Greenland" target="_blank"> Sander Greenland</a> that attempts to clear up the confusion that persists over this study design.<a href="#1">[1]</a></p>

<h3> Outcome and exposure odds ratios </h3>

<p>Consider the following table that is basically a stock character in epidemiology text books.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: center"><u><strong>Exposed </strong></u></th>
      <th style="text-align: center"><u><strong> Unexposed </strong></u></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right"><u><strong>Outcome event </strong></u></td>
      <td style="text-align: center">a</td>
      <td style="text-align: center">b</td>
    </tr>
    <tr>
      <td style="text-align: right"><u><strong>No outcome event </strong></u></td>
      <td style="text-align: center">c</td>
      <td style="text-align: center">d</td>
    </tr>
  </tbody>
</table>

<p>The entry in each cell denotes the number of people in each exposure/outcome category in a closed cohort. Using this table, we get</p>

\[\begin{align}
\mathrm{Odds}(O=1 | E=1) &amp;= \frac{ \left(\frac{a}{a+c} \right) }{ \left(\frac{c}{a+c} \right) } = \frac{a}{c} \label{odds_e} \tag{1}\\
&amp; \\
\mathrm{Odds}(O=1 | E=0) &amp;= \frac{ \left( \frac{b}{b+d} \right) }{ \left( \frac{d}{b+d}  \right) } = \frac{b}{d}, \label{odds_u} \tag{2}
\end{align}\]

<p>where \( O \) and \( E \) are binary outcome and exposure indicator variables respectively. This gives the outcome odds ratio as</p>

\[\begin{align}
OR  = \frac{\mathrm{Odds} (O=1 \| E=1)}{\mathrm{Odds} (O=1 \| E=0) } = \frac{ad}{bc} \tag{3}.
\end{align}\]

<p>A fact that is often made much of in epidemiology text books is that if you repeat this calculation but instead looking at the odds ratio of <em>exposure</em> in cases compared to controls, you get exactly the same answer, \( \frac{ad}{bc} \). In fact, I have sometimes seen a stronger claim that case control studies <em>cannot</em> directly estimate the outcome odds ratio, but only the exposure odds ratio.</p>

<p>The idea behind this claim appears to be that in a case-control study, the dataset typically contains all the cases, but only some of the controls. Hence we do not know \( c \) or \( d \), and cannot estimate \( \ref{odds_e} \) or \( \ref{odds_u} \). On the other hand, when calculating the odds ratio of exposure, the probability of exposure among those who had the event and those who did not have the event can be estimated.</p>

<p>This is, however, completely immaterial. The ratio \( \frac{d}{c} \) can be estimated by dividing the number of people who are unexposed by the number exposed among the controls. Therefore the quantity \( \frac{ad}{bc} \) can also be estimated from the case control sample even though we don’t know \( c \) or \( d \). It’s irrelevant whether you want to call it an outcome odds ratio or an exposure odds ratio.</p>

<p>Unfortunately, this has generated some confusion. While the outcome and exposure odds ratios are theoretically equal, one would typically estimate them using logistic models for \(  P(O = o \; | \; E=e, X=x) \) and \(  P(E = e \; | \; O=o, X=x) \). These are related by Bayes theorem,</p>

\[\begin{align}
P(O = o \; | \; E=e, X=x) = \frac{  P(E = e \; | \; O=o, X=x) P(O=o, X=x) }{ P(E=e, X=x) }.  \label{bayes} \tag{4}
\end{align}\]

<p>It is extremely unlikely that logistic models will be consistent with this relationship because, as we all know, <a href="https://en.wikipedia.org/wiki/All_models_are_wrong" target="_blank"> all models are wrong</a>. Thus outcome and exposure odds ratios estimated in this way will in general not be the same. The choice of whether to take outcome or exposure to be the dependent variable in principle should be done on the basic of which model is better. In practice, there is an established tradition of the outcome being the dependent variable.</p>

<h3> Odds ratios or rate ratios? </h3>

<p>The second major source of confusion in case control studies is whether one should report an odds ratio or a rate ratio. On the face of it, you would think an odds ratio since case control studies typically use logistic regression. However, this odds ratio is an estimator for the parameter of a data generating process that includes the sampling we carried out for cases and controls. In other words, it is not an estimator for the odds ratio in the population.</p>

<p>On the other hand, there are some circumstances under which the odds ratio in a case control study is equal to the rate ratio in the population. These conditions are laid out in our paper <a href="#1"> [1]</a>, and also in Sander’s paper from the early 80s <a href="#2"> [2]</a>. In particular, this conclusion holds if the analysis uses risk set sampling to match cases with controls, the model includes strata terms for matched cases and controls, and the instantaneous rate ratio is constant over the study period. Risk set sampling means controls are randomly sampled from everyone who has not had an event at the time the of outcome for the index case. Proof of this can be found in <a href="#2"> [2]</a>.</p>

<p>Despite the fact that this has been known for over 50 years, there is still a great deal of confusion. In particular, there are a few papers out there that incorrectly claim that risk set sampling alone is sufficent for an odds ratio in the data generating process of a case control study to be equal to a rate ratio in the population.</p>

<p>Ultimately, it is a matter of ‘taste’ whether to report an odds ratio or rate ratio. Typically the entire motivation for estimating an odds ratio in a case control study is because it is also an estimator of a population parameter of interest - the rate ratio. On the other hand, it is not incorrect to report an odds ratio. What must be understood is that the odds ratio applies to a data generating process that includes the case control sampling, whereas the rate ratio applies to a data generating process for the whole population.</p>

<h3> Re-using data points? </h3>

<p>A final confusion that arises is that in risk set sampling, you can end up having the same individual as both a case and a control. This can happen if they are matched to another case at a time before they have had the event themselves. Surely that’s not allowed?</p>

<p>This can be resolved by understanding that the unit of observation in a case-control study is not an indivdual, but an individual <em>at a particular instant in time</em>. In other words, it’s as if each individual has a row in the data for every instant of follow-up time. That will generally result in an infinite number of observations, but the case control study only ever uses a finite subset of them. If an individual ends up having several of their observations included in the case control study, these should be understood as independent data points - or at least data points that are ‘sufficiently independent’ to satisfy the modelling assumptions that are made.</p>

<h3>  References </h3>

<p><span id="1"> <b>1. </b> </span> Kerr S, Greenland S, Rudan I et al. Understanding and reporting odds ratios as rate ratio estimates in case-control studies. Hournal of global health, 2023.</p>

<p><span id="2"> <b>2. </b> </span> Greenland S, Thomas DC. On the need for the rare disease assumption in case-control studies. Am J Epidemiol. 1982;116:547-53. Erratum in: Am J Epidemiol 1990 Jun;131(6): 1102.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[04 Jun 2024]]></summary></entry><entry><title type="html">The St Petersburg paradox</title><link href="/2024/04/24/The-St-Petersburg-paradox.html" rel="alternate" type="text/html" title="The St Petersburg paradox" /><published>2024-04-24T00:00:00+01:00</published><updated>2024-04-24T00:00:00+01:00</updated><id>/2024/04/24/The%20St%20Petersburg%20paradox</id><content type="html" xml:base="/2024/04/24/The-St-Petersburg-paradox.html"><![CDATA[<p>24 Apr 2024</p>

<h3> The St Petersburg game </h3>
<p>Imagine I offer you the chance to play the following game. The pot starts at \( $ 2\). I flip a coin repeatedly. Each time it comes up tails, the pot doubles. The first time a heads appears, the game ends and you leave with the pot. How much would you be willing to pay to play this game?</p>

<p>If you’re anything like a reasonable person, or <a href="https://en.wikipedia.org/wiki/Daniel_Bernoulli" target="_blank"> Daniel Bernoulli</a>, you’d be unlikely to pay more than \( $ 20\). At that price, you’d need at least 4 consecutive tails to make a profit.</p>

<p>According to some people, this would put you at odds with 300 years of conventional wisdom regarding uncertain outcomes. ‘Rational’ individuals are supposed to maximise expected value, and the expected value of this bet goes to infinity as the number of potential coin flips becomes large,</p>

\[\sum_{i=1}^n = \frac{1}{2} 2 + \frac{1}{4} 4 + \frac{1}{8} 8 + ... \frac{1}{2^n} 2^n \label{st petersburg}\\
= n.\]

<p>You should therefore be willing to hand over all your earthly possessions for a single shot at this game.</p>

<p>The tension between these two prescriptions is known as the <a href="https://en.wikipedia.org/wiki/ADM_formalism](https://en.wikipedia.org/wiki/St._Petersburg_paradox)" target="_blank"> St Petersburg paradox</a>.</p>

<h3> What is rational, anyway? </h3>

<p>Perhaps the main reason why it is nowadays believed that a ‘rational’ decision maker must maximise expected value or utility is the von Neumann Morgenstern (hereafter vNM) utility theorem, which I have written about <a href="https://drstevenkerr.com/2023/10/03/The-church-of-expected-utility-theory.html" target="_blank"> before</a>. To recap; the theorem says that anyone who behaves in accordance with four seemingly quite reasonable axioms must be an expected utility maximiser. They are as follows:</p>

<p><b>1. Completeness.</b> For any two lotteries \( L \) and \( M \), we have either \( L \succeq M \) or \( M \succeq L \)</p>

<p><b>2. Transitivity.</b> If there are three lotteries \( L \), \( M \) and \( N \), with \( L \succeq M \succeq N \), then \( L \succeq N \).</p>

<p><b>3. Continuity.</b> For any three lotteries with \( L \succeq M \succeq N\), there is some probability \( p \) such that \( pL + (1-p)N \sim M \).</p>

<p><b>4. Independence.</b> For any lottery \( M \) and any probability \( p \), \( L \succeq N \) if and only if \( pL + (1-p)M \succeq  pN + (1-p)M \).</p>

<p>Inquiring minds might wonder what exactly a lottery is. In von Neumann and Morgenstern’s original formulation, it’s actually a little bit vague. Subsequent work however clarified that lotteries are elements of some set of probability measures over a measurable space.</p>

<p>Another important but subtle point to note is that it is only proved that there exists a function over lotteries with the following property</p>

\[U(pL + (1-p) M) = p U(L) + (1-p)U(M).\]

<p>This implies that, for example, if you start off with an assignment of utilities to sure outcomes, then this can only be extended to lotteries where a finite number of outcomes have non-zero probability (density). These are sometimes called simple probability measures/lotteries. Later work extended the original vNM theorem to more general spaces of lotteries, although additional axioms need to be introduced.</p>

<p>The final subtlety to notice is that the vNM theorem, as well as various extensions, only demonstrate that for any individual who behaves in accordance with the above axioms, there is a <em>bounded</em> utility function over sure outcomes such that the expected utility represents their preferences over lotteries. Notice my emphasis on the word bounded.</p>

<h3> Resolving the paradox </h3>

<p>Daniel Bernoulli resolved the original form of the paradox shortly after it was first posed by his brother Nicolaus in 1713, and long before anyone knew about the vNM theorem. He noticed that the unbounded quantities of money you might receive in the St Petersburg game do not give unbounded ‘happiness’, or utility. If you restrict to bounded utilities, the paradox evaporates.</p>

<p>In the early 1900s, a newer form of the paradox appeared in which the payoffs are instead taken to be ‘utilities’ rather than money. The idea is that ‘pleasure’ can be measured in units in much the same way as physical quantities like distance can be measured in metres, and we can simply take a modified version of the original St Petersburg game in which the payoffs are units of pleasure that can become arbitrarily large. In some circles, this newer form of the paradox is still considered unsolved.</p>

<h3> The St Petersburg paradox 2.0 </h3>

<p>I have to say I’m fairly unimpressed with the modern incaration of the St Petersburg paradox. A careful reading of the vNM theorem and extensions indicates that there is absolutely no obligation for an individual who acts in accordance with its axioms to assign infinite utility to the St Petersburg game. The theorems simply guarantee that there is a <em>bounded</em> utility function that represents their preferences over sure outcomes. That utility function needn’t have an interpretation as measuring units of pleasure - it simply represents an ordering of outcomes. So, for example, if outcome \( A \) is assigned the value \( 1 \) and outcome \( B \) is assigned the value \( 2 \), it does not mean that \( B \) gives ‘twice as much pleasure’ as \( A \). It simply means that \( B \) is preferred to \( A \).</p>

<p>If you insist on an unbounded utility function that has an interpretation in terms of ‘units of pleasure’, then you are outwith the scope of the vNM theorem. I would suggest you’re probably outwith the scope of reality too. If it is even possible to sensibly measure units of pleasure, I would hazard that it’s not possible for a person, or a finite collection of people, to experience an infinite number of them. In just the same way that it’s not possible for anyone to travel an infinite distance or live for an infinite time. Infinities like this don’t occur in nature. The revamped St Petersburg paradox is asking us to make sense of something that isn’t physically possible. At which point we are completely justified in ignoring it, for the same reason that I don’t spend much time working out how to consistently value fairy dust or unicorn horns.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[24 Apr 2024]]></summary></entry><entry><title type="html">Loop quantum gravity, or lattice quantum gravity?</title><link href="/2024/03/26/Loops-quantum-gravity,-or-lattice-quantum-gravity.html" rel="alternate" type="text/html" title="Loop quantum gravity, or lattice quantum gravity?" /><published>2024-03-26T00:00:00+00:00</published><updated>2024-03-26T00:00:00+00:00</updated><id>/2024/03/26/Loops%20quantum%20gravity,%20or%20lattice%20quantum%20gravity</id><content type="html" xml:base="/2024/03/26/Loops-quantum-gravity,-or-lattice-quantum-gravity.html"><![CDATA[<p>26 Mar 2024</p>

<p>In this post, I’m going to give a brief critical overview of loop quantum gravity, the field that I worked in during my time in physics. A fairly strong understanding of physics is assumed.</p>

<h3> The ADM formalism </h3>

<p>Loop quantum gravity is an attempt at canonical quantisation of Einstein’s general relativity. Thus the starting point is constructing a Hamiltonian. In a flat spacetime that’s easy, but on a general spacetime manifold it’s not so easy because it requires one to define a time coordinate. The <a href="https://en.wikipedia.org/wiki/ADM_formalism" target="_blank"> ADM formalism</a> addresses this by assuming that spacetime is <a href="https://en.wikipedia.org/wiki/Foliation" target="_blank"> foliated</a>. Roughly this means that our spacetime manifold \( M \) is decomposed into a family of spacelike hypersurfaces \( \Sigma_t \) labelled by a timelike coordinate \( t\). Spacelike means that any pair of points on \( \Sigma_t \) are spacelike separated - their spatial and temporal separation is such that light could not travel between them. There are in general many possible foliations of \( M \) that should all be ‘equally as good’ as each other.</p>

<p>With a time coordinate in hand, we can now define the Hamiltonian in terms of the metric \( g_{ij} \) on the 3-dimensional spacelike hypersurfaces and its conjugate momentum \( \pi_{ij} \). The Hamiltonian thus constructed consists of two parts: the Hamiltonian constraint, and the (spatial) diffeomorphism constraint.</p>

<h3> The Wheeler-DeWitt equation </h3>

<p>The next step in canonical quantisation is to replace the canonical variables with operators acting on a suitable space of functionals. Because we are working with a field theory, \( \hat{\pi}_{ij} \) will be a <a href="https://en.wikipedia.org/wiki/Functional_derivative" target="_blank"> functional derivative</a>. If \( F \) is a functional with argument \( f \), the functional differential can be defined as follows</p>

<p>\(\delta F[f][\phi] = \left[ \frac{d}{d \epsilon} F[f + \epsilon \phi]  \right]_{\epsilon = 0}\).</p>

<p>This is the functional analysis equivalent of \( dx \) in vanilla calculus. From here we define the functional derivative as</p>

<p>\(\frac{\delta F[f]}{f(y)} = \delta F[f][\delta_y]\),</p>

<p>where \( \delta_y(x) = \delta(x-y) \) is the <a href="https://en.wikipedia.org/wiki/Dirac_delta_function" target="_blank"> Dirac delta function</a>.</p>

<p>You will notice that every time we take a functional derivative, we get a factor of a delta function. This is a problem. The Hamiltonian is second order in the canonical momenta \( \hat{\pi}_{ij} \), which leads to singularities arising from the accumulated delta functions.</p>

<p>Really what’s happening here is that we started off doing something that isn’t mathematically kosher. When we promote fields to operators, really they are operator-valued <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)" target="_blank"> distributions</a>. It turns out that it is <a href="https://en.wikipedia.org/wiki/Distribution_(mathematics)#Operations_on_distributions" target="_blank"> not at all clear how to sensibly define multiplication for such objects</a>. And yet our Hamiltonian multiplies them footloose and fancy-free.</p>

<p>These are problems that are generically encountered when attempting to canonically quantise any non-trivial quantum field theory. It is the reason that <a href="https://en.wikipedia.org/wiki/Wightman_axioms#Existence_of_theories_that_satisfy_the_axioms" target="_blank"> there is no known example of a mathematically well-defined interacting quantum field theory in 4 dimensions</a>.</p>

<p>If one is willing to work on a spacetime lattice, then this issue can be avoided. Lattice quantum field theory is in fact a completely consistent, mathematically well-defined, non-perturbative quantisation method. But we’ll come back to that later.</p>

<h3> The Ashtekar variables </h3>

<p>Ok so let’s say we are really determined to ignore the fact that we are not doing meaningful maths anymore. It turns out that you can do some manipulations on the Wheeler-DeWitt equation that maybe looks like progress.</p>

<p>The <a href="https://en.wikipedia.org/wiki/Ashtekar_variables" target="_blank"> Ashtekar variables</a> consist of a complex-valued field that looks like an \( SU(2) \) connection. It turns out that in terms of these new variables, the Hamiltonian becomes (up to a factor we can ignore) polynomial in the canonical variables. Now, as per the previous section, polynomials involve multiplication which is still not generally well-defined for operator-valued distributions. However, at least the most egregiously non-polynomial factors like square roots of deteminants no longer make an appearance. This development sparked a wave of excitement which really marks the start of the entire loop quantum gravity research programme.</p>

<h3> Thiemann's trick </h3>

<p>Unfortunately, because the canonical variables are now complex-valued, the theory is a complex version of general relativity. Classically it is easy to impose reality conditions that recover real general relativity. However, in the quantum theory it’s not so easy and so far no one has been able to do it. This led to people giving up on the Ashtekar variables after a few years.</p>

<p>Enter Thomas Thiemann. He figured out a way of writing the Hamiltonian in terms of Poisson brackets involving the connection and the volume. With this trick, he did not have to resort to the problematic complex-valued variables, and once again it looked like maybe progress could be made.</p>

<h3> The loop representation </h3>

<p>Normally the operators defined from the canonical variables are taken to act on a space of wavefunctions that are functionals of the fields. It turns out that no matter if you use the Ashtekar variables or Thiemann’s trick, sensibly defining the Hamiltonian constraint on this space remains difficult and perhaps impossible. That’s probably because we chose to ignore the fact that we’re not really doing maths some time ago and ploughed ahead anyway.</p>

<p>In order to overcome this difficulty, the next ingredient in the loop quantum gravity recipe is the loop representation. Instead of working with connections, we work with <a href="https://en.wikipedia.org/wiki/Holonomy" target="_blank"> holonomies</a> associated with the connection, and their corresponding conjugate variable called ‘flux vectors’. Because the connection lives in \( SU(2) \), it turns out that the wavefunctions coincide with Penrose’s old idea of <a href="https://en.wikipedia.org/wiki/Spin_network" target="_blank"> spin networks</a>.</p>

<p>Now we need to define an inner product to get a proper Hilbert space. Spin networks are defined by graphs, and it turns out that in order to preserve diffeomorphism invariance, the inner product more or less has to be defined so that its action on two spin networks is zero unless their underlying graphs coincide. The Hilbert space thus constructed does not have a countable basis, which is a big departure from standard quantum theory. It is relatively straightforward to define the actions of holonomy and flux vector operators on states in this space.</p>

<h3> Imposing constraints </h3>

<p>We have to impose the spatial diffeomorphism constraint and the Hamiltonian constraint to get to the physical Hilbert space. It turns out that in our candidate Hilbert space above, the diffeomorphism constraint operator can’t be defined, and even if it could, it wouldn’t contain any diffeomorphism invariant state aside from the trivial wavefunction. This is because a diffeomorphism is equivalent to moving the spin network around on the manifold \( M \).</p>

<p>In order to get around this difficulty, the candidate Hilbert space has to be drastically enlarged to allow states to be constructed that are something like ‘averages’ of spin networks over states related by diffeomorphisms. If that sounds like it’s probably not well-defined, that’s because it probably isn’t. It appears that there is no real consensus on how to construct this space properly. It is also not known whether it will have a countable basis.</p>

<p>Ok so that’s the diffeomorphism constraint, how about the Hamiltonian constraint? First you have to define a suitable quantum operator, and it turns out that is a complete nightmare. I won’t go into the details, partly because I don’t entirely know them, but mostly because I don’t think it’s that interesting. If you’re up for it, <a href="#1"> [1]</a> gets into more of the nitty gritty. The upshot is that a large number of contrived choices need to be made in order to even make symbolic process. And even then, a large number of ambiguities remain. In particular, it appears necessary to restrict the space that the Hamiltonian constraint acts on to a particular ‘habitat’, for which there may be an infinite number of candidate choices. It turns out too that the Hamiltonian and diffeomorphism constraints don’t play well together. If you proceed ‘naively’ the action of the Hamiltonian constraint can map a diffeomorphism invariant state onto one that isn’t.</p>

<p>The whole thing ends in such a huge mess that not a single physical eigenstate of the Hamiltonian constraint is known. In addition, because of the extraordinary complexity of the Hamiltonian and diffeomorphism operators, it is not known whether the quantum constraint algebra closes. This is a basic requirement for any theory to be consistent.</p>

<h3> Spin foams </h3>

<p>As long ago as the early 2000’s, the situation on the canonical quantisation side seemed hopeless enough that many people in the loop quantum gravity community had instead turned to path integral approaches, hoping they might provide a way out. To this end, various attempts were made to define a partition function for quantum gravity, which all ultimately rest on introducing a discretisation of spacetime - usually a <a href="https://en.wikipedia.org/wiki/Triangulation_(geometry)" target="_blank"> triangulation</a>. If you start out with topological quantum field theory (TQFT), the partition function is naturally independent of the triangulation you choose, and people like that because it has diffeomorphism symmetry as required by general relativity. However, TQFTs inevitably do not have local degrees of freedom, because those degrees of freedom would certainly be sensitive to the local geometry of spacetime. On the other hand, general relativity does have local degrees of freedom. As soon as you allow for local degrees of freedom, the triangulation independence is lost, and you are left in a situation that is basically identical to lattice quantum field theory. That is, the theory depends on the particular spacetime discretisation you choose.</p>

<h3> Conclusion </h3>

<p>Let me try and sum up the history of loop quantum gravity. We started out with the Ashtekar variables, which seemed to make progress on making sense of the Wheeler-DeWitt equation. However, they were abandoned because reality conditions could not sensibly be imposed in the quantum theory. Our next attempt at making sense of the Wheeler-DeWitt equation was to use Thiemann’s trick. Seemingly the only way to make ‘progress’ in the resulting quantum theory is to switch to the loop representation. This leads to hideously complicated and perhaps ill-defined forms for the Hamiltonian and diffeomorphism constraints and the space that they are meant to act on. The theory is riddled with a large number of ambiguities and artifical choices, and thus far it has not yielded anything physically intelligible. From there, we moved on to the spin foam approach, which is functionally indistinguishable from lattice quantum field theory.</p>

<p>In the process, the following issues remain unsolved</p>
<ul>
  <li>It is not known whether general relativity is recovered in the classical limit of loop quantum gravity.</li>
  <li>It is not known whether a suitable quantum constraint algebra can be defined that is closed, and therefore whether the quantum theory will be independent of arbitrary choices of coordinates.</li>
  <li>The candidate operators and Hilbert spaces that do exist may not be well-defined, and even if they are, their definition is plagued by a large number of ad-hoc choices and ambiguities.</li>
  <li>It is not known whether there is any connection between loop quantum gravity and spin foam approaches.</li>
</ul>

<p>To me, loop quantum gravity looks like a long series of mashing square-shaped pegs into round holes and ignoring the wreckage that ensues. The entire thing is ugly, contrived and fails to make any tangible progress on the subject of quantum gravity. After 40 years of hammering away at this, it appears that the best the field has to offer is something that is really just a variant of lattice quantum field theory. But the whole motivation for doing any of this in the first place was to avoid perceived shortcomings of lattice quanum field theory!</p>

<p>Ok, it’s easy for me to get on my historically retrospective high horse. I will say this about loop quantum gravity; I think it was worth a shot. But I do more or less consider it a failed research programme. It seems like most of its biggest proponents have quietly moved on to other things. I found that the field also has an unfortunate tendency to oversell itself. You would think the above laundry list of outstanding issues would call for a touch of humility and self-awareness. I didn’t see much of that when I worked in the field. I get that the people who dedicate their professional lives to a particular theory are inevitably going to be its biggest cheerleaders. But for young graduate students, the mind-boggling complexity of high energy theoretical physics means that it’s almost impossible to judge the merits of a field before having worked in it for several years. And by that time you’ve probably got intellectual Stockholm syndrome anyway. Maybe the self-aggrandisement of loop quantum gravity is a counter-reaction to the ideological hegemony of string theory and the revolutionary fervour of its adherents. But if that’s the case, I just wish everyone would get a grip. I guess that woudn’t leave them with much content to write papers about though.</p>

<h3>  References </h3>

<p><span id="1"> <b>1. </b> </span> Nicolai H, Peeters K, Zamalkar M. <a href="https://arxiv.org/abs/hep-th/0501114" target="_blank"> Loop quantum gravity: an outside view</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[26 Mar 2024]]></summary></entry><entry><title type="html">Robust standard errors</title><link href="/2024/01/17/Robust-standard-errors.html" rel="alternate" type="text/html" title="Robust standard errors" /><published>2024-01-17T00:00:00+00:00</published><updated>2024-01-17T00:00:00+00:00</updated><id>/2024/01/17/Robust%20standard%20errors</id><content type="html" xml:base="/2024/01/17/Robust-standard-errors.html"><![CDATA[<p>17 Jan 2024</p>

<p>Robust standard errors are frequently used in statistics, often in an unthinking way. They are certainly valuable and useful in the context of linear regression; however, they do not serve a similar purpose in maximum likelihood estimation of non-linear models. The basic issue is that the kind of mis-specification that robust standard errors can address in linear regresion does not spoil consistency results for the parameter estimates. On the other hand, it does spoil those results in the context of maximum likelihood estimation of non-linear models. Thus at best you end up with a consistent estimator for the variance of a parameter estimate that is itself inconsistent, which isn’t really of interest except as a diagnostic tool for detecting bad models. I’ll explore this in more detail below.</p>

<h3>  Linear regression </h3>

<p>In my <a href="https://drstevenkerr.com/2024/01/03/Why-do-so-many-statisticians-think-a-normality-assumption-is-required-in-linear-regression.html" target="_blank"> last post</a>, I discussed some sufficient conditions for ‘model validity’ in linear regression. I will use the same notation defined there. Namely: \( y \) is an \( n \)-dimensional vector whose elements are \( y_i \), \( X \) is an \( n \times k \) matrix whose rows are \( x_i \), \( \beta \) is a \( k\)-dimensional vector of parameters to be estimated, and \( \epsilon \) is an \( n \)-dimensional vector of residuals whose elements are \( \epsilon_i \). One of the conditions was as follows:</p>

<p><b>Central limit theorem (CLT).</b> \(  \sqrt{n} \left( \frac{1}{n} \sum_{i} x_i \epsilon_i\right) \xrightarrow{d} N(0, \Sigma) \), where \( \xrightarrow{d} \) denotes <a href="https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_distribution" target="_blank"> convergence in distribution</a>.</p>

<p>If \( \Sigma \) is of the form \( \sigma^2 I \) where \( I \) is the \( n \times n \) identity matrix, then the errors are said to ‘spherical’. In this case, the conditional variance of  the ordinary least squares (OLS) estimator is</p>

\[\mathrm{Var}(\hat{\beta} \; \lvert \; X) = \frac{\epsilon^T \epsilon}{n-k} (X^T X)^{-1},\]

<p>where \( \hat{\epsilon} = y - X \hat{\beta} \).</p>

<p>It turns out that the spherical error requirement can be significantly weakened. In particular, there are several ‘robust’ variance estimators that are consistent even in the presence of heteroskedasticity. For example, the following estimator</p>

\[HC_0 :=  \frac{n}{n-k} (X^T X)^{-1} X^T \mathrm{diag}(\hat{\epsilon}) X (X^T X)^{-1},\]

<p>where \( \mathrm{diag}(\hat{\epsilon}) \) is the diagonal matrix with non-zero elements given by \( \hat{\epsilon}\).</p>

<p>Crucially, the spherical error assumption is <b>not needed</b> for the OLS estimator \( \hat{\beta} = (X^T X)^{-1} X^T y \) to be consistent. This means there is no conflict between consistency of \( \hat{\beta}\) and presence of heteroscedasticity. Heteroscedasticity robust standard errors are therefore very useful as a quantification of uncertainty of parameter estimates in many scenarios.</p>

<p>As we shall see, the analogous statement is not so much the case in maximum likelihood estimation.</p>

<h3>  Maximum likelihood estimation </h3>

<p>In the following, I will use \( \theta_0 \) to denote the ‘true’ parameter values of a data-generating process, \( \theta \) for a set of parameter values, and \( w_i \) for a vector  that is a single sample from the data-generating process. I will consider the particular case of M-estimators, which are estimators that maximise an objective function that takes the form of a sample average,</p>

\[Q_n(\theta) :=  \frac{1}{n} \sum_i m(w_i, \theta).\]

<p>Here, \( m \) is just some function of the data and the parameters. The typical case in maximum likelihood estimation is \( m(w_i, \theta) = \mathrm{ln}(f(w_i, \theta)) \), where \( f(w_i, \theta) \) is a probability density parametrised by \( \theta \) and evaluated at \( w_i \). In constructing this expression, it is typically assumed that the \( w_i \) are <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" target="_blank"> iid</a>.</p>

<p>Let’s look at a typical set of sufficient conditions for consistency of maximum likelihood parameter estimates.</p>

<p><span id="1.1"> <b>1.1.</b> The <a href="https://en.wikipedia.org/wiki/Stochastic_process" target="_blank"> stochastic pocess</a> \( w_i \) is <a href="https://en.wikipedia.org/wiki/Stationary_process" target="_blank"> stationary</a> and <a href="https://en.wikipedia.org/wiki/Ergodic_process" target="_blank"> ergodic</a>.</span></p>

<p><span id="1.2"> <b>1.2.</b> \( \theta_0 \in \Theta \) where \( \Theta \) is a convex parameter space.</span></p>

<p><span id="1.3"> <b>1.3.</b> \( m(w_i, \theta) \) is concave in \( \theta \; \forall \; w_i \).</span></p>

<p><span id="1.4"> <b>1.4.</b> \( m(w_i, \theta) \) is a <a href="https://en.wikipedia.org/wiki/Measurable_function" target="_blank"> measurable function</a> of \( w_i \; \forall \theta \in \Theta \).</span></p>

<p><span id="1.5"> <b>1.5.</b> \( \mathbb{E}[m(w_i, \theta)] \) is uniquely maximised on \( \Theta \) at \( \theta_0 \).</span></p>

<p><span id="1.6"> <b>1.6.</b> \( \mathbb{E}[m(w_i, \theta)] \) is finite \(\forall \; \theta \in \Theta \).</span></p>

<p>The logic behind these conditions is as follows. <a href="#1.1"> 1.1</a> and <a href="#1.6"> 1.6</a> guarantee that the objective function \( Q_n(\theta) \) converges in probability to \( \mathbb{E}[m(w_i, \theta)] \). The idea is that if we then maximise \( Q_n(\theta) \), the resulting estimator \( \hat{\theta} \) will converge in probability to the unique ‘true’ value \( \theta_0 \) that maximises \( \mathbb{E}[m(w_i, \theta)] \), assumed to exist courtesy of <a href="#1.5"> 1.5</a>. Conditions <a href="#1.2"> 1.2</a> and <a href="#1.3"> 1.3</a> simply ensure that \( Q_n(\theta) \) has a unique maximum, and <a href="#1.4"> 1.4</a> is a ‘mathematical housekeeping’ condition that just means that \( m(w_i, \theta) \) is in fact a random variable.</p>

<p>Note that despite the fact that the iid assumption is typically used to derive the form of the likelihood function in the first place, it does not appear in <a href="#1.1">1.1</a>-<a href="#1.6">1.6</a>. We only need \( w_i \) to be ergodic and stationary, and \( \mathbb{E}[m(w_i, \theta)] \) to be uniquely maximised at \( \theta_0 \). In other words, the objective function \( Q_n(\theta) \) needn’t be a correctly specified log-likelihood. This opens the doorway to <a href="https://en.wikipedia.org/wiki/Quasi-maximum_likelihood_estimate" target="_blank"> quasi-maximum likelihood estimation</a>, in which the likelihood function is mis-specified but still has the correct maximum.</p>

<p>Conditions <a href="#1.2"> 1.2</a> and <a href="#1.3"> 1.3</a> also ensure that the global maximum of \( Q_n(\theta) \) can be found by taking the first derivative and setting equal to zero. We define the score and Hessian for observation \( i\) as follows</p>

\[\begin{align}
s(w_i, \theta) &amp;:=  \frac{\partial m(w_i, \theta)}{\partial \theta} \\
H(w_i, \theta) &amp;:= \frac{\partial^2 m(w_i, \theta)}{\partial \theta \partial \theta'}.
\end{align}\]

<p>Assume the following,</p>

\[\begin{align}
\frac{1}{n} \sum_i  H(w_i, \theta) &amp;\xrightarrow{p} \mathbb{E}[H(w_i, \theta)] \\
\frac{1}{n} \sum_i  s(w_i, \theta_0) &amp;\xrightarrow{d} N(0, \Sigma).
\end{align}\]

<p>We can then apply the <a href="https://en.wikipedia.org/wiki/Delta_method" target="_blank"> delta method</a> to \( \frac{ \partial Q_n(\hat{\theta})}{\partial \theta} =  \frac{1}{n} \sum_i s(w_i, \theta) = 0 \), obtaining</p>

\[\sqrt{n} (\hat{\theta} - \theta_0) \xrightarrow{d} N(0, \Sigma'),\]

<p>where</p>

<p>\(\begin{align}
\Sigma' = \big( \mathbb{E}[H(w_i, \theta_0)] \big)^{-1} \; \Sigma \; \big( \mathbb{E}[H(w_i, \theta_0)] \big)^{-1} \label{robust} \tag{1}
\end{align}\).</p>

<p>In the case where \( w_i \) are <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" target="_blank"> iid</a> and some <a href="https://en.wikipedia.org/wiki/Fisher_information" target="_blank"> regularity conditions</a> are satisfied, this expression simplifies to</p>

<p>\(\begin{align}
\Sigma' = - \big( \mathbb{E}[H(w_i, \theta_0)] \big)^{-1} \label{non-robust} \tag{2}
\end{align}\).</p>

<p>These results can be found in Chapter 7, sections 7.2 and 7.3 of <a href="#1"> [1]</a>.</p>

<h3> Mis-specification </h3>
<p>Finally we get to the real subject of this post. As indicated above, we may still extract useful results from a model if the likelihood is mis-specified, so long as it has the correct maximum, or the mis-specification is ‘small’ in some appropriate sense. In this case, both \( \hat{\theta} \) and its variance will be consistent.</p>

<p>However, what if there is a more serious mis-specification? This will certainly spoil consistency results for \( \hat{\theta} \). We may still be able to consistently estimate the variance of \( \hat{\theta} \) though, using equation \(\ref{robust}\). But that begs the question, why should we care? What value is there in having a consistent estimator of the variance of a parameter estimator that is itself inconsistent?</p>

<p>This has been pointed out, for example, in <a href="#2"> [2]</a>. In essence, if there is a need to use robust standard errors, then it indicates a serious flaw in the model that demands a new and better model, not a tweak to fix the variance.</p>

<h3>  References </h3>

<p><span id="1"> <b>1. </b> </span> Hayashi, Fumio. Econometrics. Princeton :Princeton University Press, 2000.</p>

<p><span id="2"> <b>2. </b> </span> King G, Roberts ME. <a href="https://gking.harvard.edu/files/gking/files/robust_0.pdf" target="_blank"> How robust standard errors expose methodological problems they do not fix, and what to do about it.</a> Political analysis, Vol. 23, No. 2, 2015.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[17 Jan 2024]]></summary></entry></feed>