<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-06-29T14:39:10+01:00</updated><id>/feed.xml</id><entry><title type="html">Population ethics</title><link href="/2023/03/21/Population-ethics.html" rel="alternate" type="text/html" title="Population ethics" /><published>2023-03-21T00:00:00+00:00</published><updated>2023-03-21T00:00:00+00:00</updated><id>/2023/03/21/Population%20ethics</id><content type="html" xml:base="/2023/03/21/Population-ethics.html"><![CDATA[<p>21 Mar 2023</p>

<p>Population ethics is a subfield of philosophy that attempts to sensibly compare two populations that may differ in size and material circumstances, and decide which state of the world is better. That is, if we had the choice between one of these states being a reality, which should we choose?</p>

<p>The way in which one answers these questions can have profound consequences. For example, if life is sacrosanct, then is there a moral obligation to have children? Is not having children when you could ethically tantamount to murder? Or, given the various risks that humanity faces, e.g. climate change, would it in fact be deeply immoral to bring more children into the world?</p>

<p>It also turns out that if you approach this topic in a naive way, you can get very confused very fast. People can end up accepting a series of reasonable-sounding logical steps that lead to conculsions that are, to say the least, extremely dubious.</p>

<h3> Exhibit A: The repugnant conclusion </h3>

<p>Let’s start in a simple-minded way. We’re given two states of the world, and we want to be able to decide which is better. We like numbers, so why don’t we try to assign a number to how ‘good’ each state is, where high numbers means better.</p>

<p>Presumably different people have different ideas about how good each state is, and will each assign their own number. We might imagine that the numbers represent some common unit of goodness - let’s call it utility. So for example, just as we all agree to measure distances in metres so that we can readily compare without having to worry about unit conversions, perhaps we can in principle do the same thing with utility.</p>

<p>So far, so simple.</p>

<p>Ok since we have a common unit of utility, maybe the natural way to aggregate is to add together. Let’s say in a given state of the world, person \( i \) has utility \( u_i \). Then the total utility is</p>

\[U = \sum_{i=1}^{N} u_i. \label{utility} \tag{1}\]

<p>Great, now we can begin to think about how we might use this utility function to make decisions about which states are better than others. This is where the fun starts.</p>

<p>Let’s imagine we have states of the world \( A \) and \( B \). State \( A \) has 100 people who have wonderful lives. State \( B \) has 100 people who have lives that are just a tiny bit better than if they had never come into existence at all.</p>

<p>We don’t need to do any moral calculus to think that \( A \) is clearly better than \( B \).
Ok, but now let’s add more people whose lives are just marginally worth living to state \( B \). If there were \( 10,000 \) such people, would \( B \) be better than \( A \)? How about \( 1,000,000 \)?</p>

<p>Most people’s intuition will tell them there is no number of additional people in state \( B \) that will make it better than \( A \). But that clashes with our equation \(\ref{utility}\). If we multiply any small, positive quantity up by a sufficiently large number, we can get a result that is as big as we want. That is to say, there is some number of people living lives that are just barely worth living, that is preferable to a smaller number of people living wonderful lives.</p>

<p>This is called the repugnant conclusion, and a lot of people don’t like it.</p>

<h3> Ok, so what did we do wrong? </h3>

<p>Maybe we shouldn’t be adding together everyone’s utility. Does it really make sense to do that?</p>

<p>The issue you have if you want to get rid of additive utilities is that you end up in situations where adding another perfectly happy person to the world isn’t any better than if you didn’t. Or adding a completely miserable person to the world isn’t any worse than if you didn’t. That seems like a non-starter.</p>

<p>How about this: maybe some states are not comparable to others. There is just no sensible way of doing it.</p>

<p>The issue with this is that it isn’t so much a solution to our problem as it is giving up. Remember we want to be able to decide which course of action is the best. There is no way of doing this if there are possibilities that are simply not comparable to others.</p>

<p>Ok then what’s left. Accept the repugnant conclusion? But now you can be led down some very strange roads. For example, you could justify inflicting extraordinary, undeserved suffering on people now in order to bring about a future that has trillions of people living lives that are marginally worth living. Maybe you’re willing to bite that bullet. I’m not.</p>

<h3> Hidden option C: We are idiots who don't know how to do maths </h3>

<p>Or, maybe the real place we went wrong was when we allowed philosophers to do maths. Ok, I jest. But more seriously, why should we use <i>numbers</i> to represent how good a state of the world is. Who says it should have anything to do with numbers?</p>

<p>Indeed, every decent economics graduate learns about the canonical example of an ordering that cannot be represented using real numbers: <a href="https://en.wikipedia.org/wiki/Lexicographic_preferences" target="_blank">lexicographic preferences</a>. In short, imagine I like both tea and spending time with my partner. However, there is no amount of tea in the world (or China) that would convince me to spend any less time with her. Those preferences simply cannot be represented using numbers. Turns out there just aren’t enough real numbers to accommodate/fathom my depths.</p>

<p>Ok, so why can’t we have lexicographic preferences or similar when it comes to population ethics? I.e. there is no number of lives that are barely worth living that would be morally preferable to 100 people having wonderful lives. Seems easy enough. We don’t have to come to any of the utterly bizarre conclusions that philosophers think are forced upon us. We just can’t use numbers to represent our preferences - we’re not such simple creatures.</p>

<p>It really is that easy. Nonetheless, you would be truly shocked by the sheer volume of commentary this completely trivial ‘problem’ has generated in academic philosophy circles. And if you’re anything like me, absolutely appalled by the moral positions that various esteemed thinkers take seriously and have adopted as a result of this thinking.</p>

<p>Never do philosophy kids.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[21 Mar 2023]]></summary></entry><entry><title type="html">Effective altruism, longtermism and Pascal’s mugging</title><link href="/2023/01/04/Effective-altruism,-longtermism,-and-Pascal's-mugging.html" rel="alternate" type="text/html" title="Effective altruism, longtermism and Pascal’s mugging" /><published>2023-01-04T00:00:00+00:00</published><updated>2023-01-04T00:00:00+00:00</updated><id>/2023/01/04/Effective%20altruism,%20longtermism,%20and%20Pascal&apos;s%20mugging</id><content type="html" xml:base="/2023/01/04/Effective-altruism,-longtermism,-and-Pascal&apos;s-mugging.html"><![CDATA[<p>04 Jan 2023</p>

<h3> What is effective altruism? </h3>

<p>It is a social movement that broadly aims to do the ‘most good’ in the world with limited resources. They advocate for making career choices and charitable donations that maximise altruistic impact. Their activities include writing research papers on ethics, analysing effectiveness of charitable organisations, giving career advice, and promoting pledges to donate a proportion of one’s income to charity. The main individuals who are credited with starting the movement are Will MacAskill and Toby Ord, both at the University of Oxford, and Australian moral philospher Peter Singer, at Princeton university.</p>

<h3> What is longtermism? </h3>

<p>Longtermism has become a signature idea within the effective altruism community. It broadly argues for greater emphasis to be placed on the long-term future in ethical decisions. The basic gist is that future humans/sentient beings who have not yet come into existence should be given the same ‘moral weight’ as those alive now. This tends to be coupled with the idea that the number of beings alive right now could be completely dwarfed by the number of future humans, especially if humanity survives for millions/billions of years into the future and colonises other planets.</p>

<h3> Sounds dandy. Ok, and what is the last thing you mentioned? </h3>

<p>Pascal’s mugging.</p>

<p>Let’s say you’re walking along the street one day and you’re accosted by a mugger. However, he is an extremely incompetent mugger who has forgotten his weapon. So, he instead makes the following proposal: give me all the money in your wallet, and tomorrow I will give you back twice that amount of money. You, being sensible, immediately dismiss his offer - what are the chances that he would actually follow through? The mugger, sensing he is losing you, continues to increase his offer. He says that if there is even the tiniest non-zero probability that he will honour the deal, then you should accept. Eventually he wins you over by offering a bajillion dollars on the morrow sharp if you agree, but otherwise he will kick a gazillion puppies right in the face.</p>

<h3> Ok, you have a problem, don't you Steven. Why don't you tell me about it. </h3>

<p>Yeah I do have a problem. Well-intuited, me.</p>

<p>The notion that we are not giving sufficient ‘moral weight’ to future individuals compared to individuals alive now, comes from a little mish-mash of undergraduate economics and philosophy. In economics 101, you learn that the default setting for economic models is to discount future utility. That means that we prefer a bowl of ice cream now compared to a bowl of ice cream tomorrow, or a year from now. This is expressed mathematically as follows:</p>

<p>\begin{align} U = \sum_{t=1}^{T} \beta^t u(x_t).\end{align}</p>

<p>Here, \( x_t \) is some amount of a good received at time \( t \) (e.g. a bowl of ice cream), \( u \) is a utility function which measures the benefit that you get from said good at the moment of consumption, and \( \beta \) is a number between 0 and 1 that we call the utility discount factor. The utility discount factor captures how much we prefer benefits now compared to benefits at a later date. For example, let’s say units of time are measured in days and \( \beta = 0.5 \). That would mean that a bowl of ice cream tomorrow is ‘worth’ half as much as a bowl of ice cream today.</p>

<p>Now, the fine upstanding philospher sees this and is frankly outraged. So outraged, he almost smashes his monocle. What does this presume to say - that the you of tomorrow has only half the moral worth of today’s you? What egregious claptrap is this!</p>

<p>I jest, but only a little. But that’s basically the longtermists; they think that \( \beta \) should be equal to 1, for reasons.</p>

<p>Of course what they completely ignore is that the utility discount factor is really just a parsimonious way of capturing the fact that everything is a lottery, without actually having to model everything as a lottery, because frankly that’s a pain in the arse. The choice is not between a bowl of ice cream now or tomorrow. It is a choice between a bowl of ice cream now, and an almost-certain bowl of ice cream tomorrow plus a small probability that you meet your tragic and untimely demise in an unforseeable bovine trebuchet incident before tomorrow, thus depriving you of said bowl of ice cream.</p>

<p>Ok I jest again, maybe a little more this time.</p>

<p>The point is there is a chance you may not be around to get that bowl of ice cream tomorrow, or you might lose your taste buds, or whatever else. Of course there’s also a chance that a magical fairy will wave her wand and make it so that tomorrow you get twice the enjoyment from that bowl of ice cream that you otherwise would have. But basically, there’s a lottery over events that could happen, and the end result of that lottery is that humans tend to prefer sure things today rather than near-sure things tomorrow, or not-so-sure things a year from now. As a quick fix to model this, we set \( \beta \) a little less than 1.</p>

<p>It is not difficult to figure this out. Many people know it, and have pointed it out. This has not stopped the longtermists from building a utopian (read: probably dystopian) moral calculus though.</p>

<p>So yeah, let’s say we take the longtermist idea seriously - there will be no utility discounting, no siree. On top of that, let’s presume that there is a non-zero chance that there will be an arbitrarily large number of human beings with wonderful lives in the future. Does this remind you of a certain unarmed robbery of a certain historical Frenchman yet?</p>

<p>Armed with this ideology, you could come to some rather dubious conclusions, to say the least. For example, you could argue that instead of using our resources to provide food for millions of people on the brink of famine today, we should instead give that money to our well-to-do academics and intellectual superiors at Oxbridge and the Ivy league so they can do urgently-needed research on where to best expend further resources.</p>

<p>Yeah, right.</p>

<p>In fact, you could use this ideology to justify inflicting any amount of suffering on people who are alive today, in order to serve the interests of hypothetical future people. After all, what’s worse: the suffering of a few million people, or the suffering of <b>ONE ZILLION SQUILLION</b> future people, who let’s face it are, for all ethical intents and purposes, really alive today.</p>

<p>This thinking is the product of ordinary academic minds furiously scurrying to justify their existence. They need some new, counterintuitive take that they can write papers about, use to position themselves as the thought-leaders of a burgeoning social movement and get tenured positions at universities.</p>

<p>It’s easy not to take this too seriously. Surely they are on the fringe.</p>

<p>Then you realise that that the effective altruism movement may have control over several billions of dollars.</p>

<p>Then you find out that a prominent effective altruist, by the name of Sam Bankman-Fried, was (allegedly!) running a cryptocurrency Ponzi scheme, which basically amounted to stealing people’s life savings and donating it to his preferred political party.</p>

<p>Very effective. Much altruism.</p>

<p>Really what this is is some kids with a god complex and a utopian ideology that, like so many before it, would almost certainly lead to misery on the grandest scale imaginable if ever fully realised. The only thing they need to make it a reality is a little more power.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[04 Jan 2023]]></summary></entry><entry><title type="html">Negative vaccine effectiveness</title><link href="/2022/11/20/Negative-vaccine-effectiveness.html" rel="alternate" type="text/html" title="Negative vaccine effectiveness" /><published>2022-11-20T00:00:00+00:00</published><updated>2022-11-20T00:00:00+00:00</updated><id>/2022/11/20/Negative%20vaccine%20effectiveness</id><content type="html" xml:base="/2022/11/20/Negative-vaccine-effectiveness.html"><![CDATA[<p>20 Nov 2022</p>

<p>Myself and a team of collaborators including Stuart Bedston at the University of Swansea, Declan Bradley at Queen’s University Belfast and Mark Joy at the University of Oxford recently published a <a href="https://doi.org/10.1093/ije/dyac199" target="_blank"> paper</a> in the International Journal of Epidemiology studying waning of COVID-19 vaccine effectiveness. One of the major findings of the paper is that for first and second dose of AstraZeneca and first dose of Pfizer, effectiveness of the vaccine against COVID-19 hospitalisation or death reached zero by approximately day 60-80 after receiving the vaccine, and then went negative.</p>

<p>That is quite a striking result. This post is going to dig into possible explanations of this phenomenon and its implications.</p>

<h3> First of all, what did you actually do? </h3>

<p>We carried out a type of study called a ‘target trial’. This attempts to find a ‘naturally occurring’ clinical trial, by matching people in a population who received a given treatment to similar people who didn’t receive the treatment. You can then compare the rate of adverse events in these retrospectively generated, artificial treatment and control groups.</p>

<p>The data that we used to do this study are highly confidential, and stored securely on different servers in each nation of the UK. It is not permitted to have any statistically disclosive, individual-level information leave those servers. However, we introduced an interesting methodological novelty that allowed us to do a pooled study despite these restrictions. This means that the final result that we ended up with is identical to what we would have got if the data all started out in the same location. We were able to do this because of the target trial study design that we used. So, amazingly, it is possible to do a pooled analysis across multiple separate datasets that controls very carefully for individual level characteristics, without actually having to share any individual-level information!</p>

<h3> What could be causing negative vaccine effectiveness? </h3>

<p>One possibilty is that it is behavioural. So people who get vaccinated might be more likely to behave in ways that put them at greater risk of infection. For example, at various points during the pandemic, vaccinated people have been subject to fewer restrictions on movement than unvaccinated people. On the other hand, it could just as well go the other way - unvaccinated people may be less risk averse when it comes to the possibility of being infected, and thus may have behaved in ways that put them at greater danger of contracting COVID-19.</p>

<p>It is also possible that naturally-acquired immunity is more robust than vaccine-induced immunity. A <a href="https://doi.org/10.1093/ije/dyac199" target="_blank"> study</a> from Israel found that vaccinated individuals had a 13-fold increased risk of infection compared to previously infected individuals.</p>

<p>A final possibility is that this is all just a consequence of the fact that the protection offered by the vaccine is relatively short-lasting. For example, imagine that I had two cohorts of similar individuals, and vaccination offered 100% protection against infection for a short period of time. The first cohort receives a dose of vaccine on day 0, while the second cohort receives a dose of vaccine on day 30. One would intuitively expect that the first cohort would initially have fewer severe COVID-19 outcomes, but then at some point the second cohort would do better. Now instead of vaccinating the second cohort, let’s have them contract COVID-19. And instead of it happening on day 30, let’s sprinkle it around randomly in time, but initially occurring at a higher rate than in the vaccinated group. Finally, let’s assume that the vaccine is effective but not perfect for a short period of time, and naturally-acquired innoculation is as immunologically effective (or better) than the vaccine. One would still expect something qualitatively similar; the first cohort would have fewer events intially, then subsequently the second cohort would do better because those people who contract and recover from COVID-19 are naturally innoculated.</p>

<p>By the way I should say that our study is not the only one to find negative vaccine effectiveness - our result is unlikely to be an anomaly.</p>

<h3> What are the implications? </h3>

<p>If negative vaccine effectiveness is largely explained by behavioural factors, then it might be possible to curtail it with a public information campaign aimed at making sure vaccinated individuals continue to observe precautions against COVID-19. On the other hand, I do think that one of the major incentives to get the vaccine is so that one can return to normal behaviour. I personally am not convinced that the explanation for negative vaccine effectiveness is behavioural, and even if it was I am skeptical about our ability to do anything about it.</p>

<p>If negative vaccine effectiveness cannot be significantly curtailed, then it raises some very interesting policy questions. Vaccines still have utility, but what is their proper role in world where COVID-19 is endemic? For example, if vaccine doses could be timed so that the period where negative vaccine effectiveness would kick in happens towards the end of a wave of infections, it may be be possible to provide significant long-term protection by offering a precisley-timed, recurring regimen of COVID-19 vaccines. However, I don’t really have much confidence in our ability to sufficiently accurately forecast future waves of infections in a way that would allow us to usefully implement such a scheme.</p>

<p>The other main utility offered by COVID-19 vaccines is ‘flattening the curve’. That is, spreading the caseload over time so that our healthcare services are not overwhelmed and can offer better treatment. This could be worthwhile, though again it would require forecasting ability that so far has proven elusive to us.</p>

<p>The final question is whether all of this is worth it. Mass COVID-19 vaccinations are expensive, and it is not clear how beneficial it is. It seems quite possible that there are far more cost-effective ways to preserve human life. In particular, I think in our revolutionary fervour for vaccines, we may have neglected a cheaper and easier win in the form of anti-viral and monoclonal antibody treatments.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[20 Nov 2022]]></summary></entry><entry><title type="html">Modern monetary theory is the road to hell</title><link href="/2022/08/28/Modern-monetary-theory-is-the-road-to-hell.html" rel="alternate" type="text/html" title="Modern monetary theory is the road to hell" /><published>2022-08-28T00:00:00+01:00</published><updated>2022-08-28T00:00:00+01:00</updated><id>/2022/08/28/Modern%20monetary%20theory%20is%20the%20road%20to%20hell</id><content type="html" xml:base="/2022/08/28/Modern-monetary-theory-is-the-road-to-hell.html"><![CDATA[<p>28 Aug 2022</p>

<h3> What is modern monetary theory (MMT)? </h3>

<p>It’s an economic theory which, in a nutshell, says that full employment and funding of various other government programmes can be guaranteed by printing money, and the inflation that would normally ensue can be kept under control by taxation.</p>

<h3> Woah. That sounds pretty wild. </h3>

<p>Indeed.</p>

<p>The first thing that should set off alarm bells is that MMT appears to advocate for governmental control over monetary policy. That’s a <i>really</i> bad idea. This is a lesson that we have historically learned the hard way. Governments simply cannot be trusted with the keys to the printing press. The temptation for them to print money in order to deliver on some promise, at least in the short term, is just too great. That’s how you get hyperinflation. There is a reason we insist on independence of central banks and government.</p>

<p>Second is that, of course if you’re willing to fire up the printing press you can achieve full employment. You can pay people to twiddle their thumbs all day if you want. Much more difficult is ensuring that people are employed <i>usefully</i>. Forgive me if I don’t have a great deal of confidence in the ability of governments to do this. It would mean that they would have to step in and effectively deploy a significant and variable fraction of the country’s labour force across different sectors, depending on the prevailing economic conditions. Yeah, right.</p>

<p>Finally, how exactly is it that taxation is supposed to control inflation? I mean, in principle it could be done if the government increased taxes and then threw the revenue thus collected into the sea. And I mean that literally, not figuratively, i.e. if they actually destroyed money, or at least removed it from circulation for an extended period of time. That would effectively make money more valuable which could reduce inflation. The problem is again, do we really think governments can be trusted to do this rather than spend that money? Especially a government that has already demonstrated itself to be quite fond of liberal use of the printing press to meet its obligations?</p>

<p>The whole thing is a nonsense. And a very dangerous nonsense, at that.</p>

<h3> What's really going on here? </h3>

<p>When I see people advocating for ideologies that are so obviously lacking, instead of trying square their circle I ask what purpose does the ideology serve for them.</p>

<p>As an example, think about Marxism. There are many ideas under that umbrella that fail to survive the gentlest scrutiny. Take the <a href="https://en.wikipedia.org/wiki/Labor_theory_of_value" target="_blank"> labour theory of value</a>, which underpins the whole edifice. It holds, roughly speaking, that the economic value of an object is determined by the amount of labour required to produce it. This has the immediate consequence that some extraordinarily useless objects have arbitarily high value. Like a <a href="https://www.youtube.com/watch?v=EZ73Q4DwrGM" target="_blank"> sarcasm detector</a> or <a href="https://youtu.be/dJNd_HtZH2g?t=30" target="_blank"> The Homer</a>. I bet it takes <i>a lot</i> of effort to make them. Ok, my examples may be taken from the Simpsons, but the Simpsons does have a stunning ability to reflect real life.</p>

<p>Anyway, the point is that the entire foundation of Marxism is based on a nonsense. Why then did it become so popular? I would say that no small part of the explanation lies in the fact that it gave the working classes, who at the time genuinely were exploited and treated horrendously, intellectual justification for their revolutionary fervour. It was political discontent in search of an academic seal of approval.</p>

<p>And so it is with MMT.</p>

<p>What MMT advocates ultimately want is to radically increase government spending and have the rich pay for it via progressive taxation. That <i>may</i> be an admirable goal, but what if the process by which it is achieved is far more harmful than the end is good?</p>

<p>The MMT crowd have figured out that increasing taxation is difficult and generally requires a long legislative process that tends to disfavour radical changes. On the other hand, if the government just got its hands on the printing press, it could spend as much as it wants.</p>

<p>The only problem is that causes inflation, and inflation is effectively a tax levied on all those who hold the nation’s currency. As prices rise, the notes in your wallet have less and less purchasing power - you are losing real wealth. And it is a deeply regressive tax at that - poorer people who can’t put their money into assets will be hit harder than the rich.</p>

<p>What the MMT advocates wish to do is to use that to put a gun to government’s head. They fire up the printing press and spend to their heart’s content, and that threatens to cause inflation with  truly terrible consequences. At this point they offer up their snake oil solution. There is only one way out: force rich people to bail us out.</p>

<p>It’s a truly terrible ideology. And there are some indications that it’s becoming more mainstream. For example, here is Joe Biden <a href="https://twitter.com/joebiden/status/1525234935346483210?lang=en" target="_blank"> advocating for a core tenet of MMT</a>.</p>

<p>This is very worrying.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[28 Aug 2022]]></summary></entry><entry><title type="html">The solution to everything</title><link href="/2022/07/22/The-solution-to-everything.html" rel="alternate" type="text/html" title="The solution to everything" /><published>2022-07-22T00:00:00+01:00</published><updated>2022-07-22T00:00:00+01:00</updated><id>/2022/07/22/The%20solution%20to%20everything</id><content type="html" xml:base="/2022/07/22/The-solution-to-everything.html"><![CDATA[<p>22 Jul 2022</p>

<p>What if I told you there was a well-known, 100-year old theory of gravity that reproduces all empirical successes of Einstein’s general relativity, and may solve a who’s who of major oustanding problems in theoretical physics?</p>

<p>Its called <a href="https://en.wikipedia.org/wiki/Einstein%E2%80%93Cartan_theory" target="_blank"> Einstein-Cartan theory</a>, or sometimes Einstein-Cartan-Sciama-Kibble (ECSK) theory. By the way, the eponymous Kibble is my “physics grandfather” - my PhD supervisor’s PhD supervisor.</p>

<p>ECSK theory is a minor generalisation of Einstein’s general relativity that does not require the <a href="https://en.wikipedia.org/wiki/Torsion_tensor" target="_blank"> torsion </a> to be zero. It turns out that the torsion tensor does not have any propagating degrees of freedom, with the consequence that the only place that ECSK theory and general relavitity differ is at extremely high matter densities.</p>

<p>First, let’s do a quick tour of major outstanding problems that ECSK theory might solve.</p>

<h3> Gravitational singularities </h3>

<p>In a nutshell, Einstein’s general relativity posits that space and time are curved, and that their curvature is determined by the distribution of energy/matter within the spacetime. So, if you plonk a mass down somewhere, it causes space and time to bend in way that we experience as gravity.</p>

<p>One of the problems with the theory is that if the mass/energy gets sufficiently dense, the curvature of spacetime becomes infinite. This is known as a gravitational singularity, and in general relativity every black hole comes with one. Many physicists consider it to be an unsatisfactory state of affairs that the theory predicts some physical quantities are infinite, although currently this does not conflict with any experimental evidence because the singularity is inevitably ‘hidden’ behind an <a href="https://en.wikipedia.org/wiki/Event_horizon" target="_blank"> event horizon</a>, which stops us from knowing much about what happens on the other side.</p>

<p>Physicist Nikodem Poplawski has suggested that <a href="https://arxiv.org/abs/0910.1181" target="_blank">gravitational singularities may be avoided in ECSK theory</a>, due to torsion manifesting as a repulsive force that prevents fermions collapsing to infinite density. The big bang is then replaced by a big bounce, and our universe is the other side of a black hole in another universe.</p>

<h3> The horizon problem </h3>

<p>The universe appears to be relatively samey everywhere we look. In particular, the temperature of the <a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background" target="_blank"> cosmic background radiation</a>, a remanant from the early universe, is extremely uniform. Turns out that’s a big problem.</p>

<p>The issue is that when the cosmic background radiation was formed, the universe was about 300,000 years old, and it is possible to show that cosmic background radiation separated by an angle of incidence on Earth of any more than about 2 degrees cannot have been been in causal contact at that time. In other words, there is no reason to think that the radiation coming from any two such patches of the sky were in thermal equilibrium at the time they were produced, and they have not been in thermal contact since - so why is it the same temperature?</p>

<p>To put it more simply, let’s say I have a glass of hot water and a glass of cold water. If they are not in thermal contact with each other, or their surroundings, they will remain at different temperatures. If I pour them both into a container, they rapidly come into thermal equilibrium at an intermediate temperature. Cosmic background radiation from two slightly separated patches of the sky are like separate glasses of water - there’s no reason why they should have been in thermal equilibrium when they were created, and have not been in thermal contact since. So why is it that in every direction we look, the temperature is the same?</p>

<p>In the ECSK theory, the torsion generates <a href="https://en.wikipedia.org/wiki/Inflation_(cosmology)" target="_blank"> cosmic inflation</a>, which<a href="https://arxiv.org/abs/1410.3881" target="_blank"> causes the early universe to be homogeneous</a>, leading to uniformity in the temperature of the cosmic background radiation.</p>

<h3> The black hole information paradox </h3>

<p>In quantum mechanics/quantum field theory, it is taken as an axiom that time evolution is <a href="https://en.wikipedia.org/wiki/Unitarity_(physics)" target="_blank"> unitary</a>. What this means simply is that the value of the wavefunction at one time determines a unique value at all times. The main reason this is important is because it is a sufficient condition for ‘probability to be conserved’. That is, if the probability of finding the system in some state \( \ket{ \psi(0)} \) at time \(0\) is \(p\), and it evolves to \( \ket{ \psi(t)} \) at time \(t\) under the Hamiltonian, then the probability of \( \ket{ \psi(t)} \) is also \(p\).</p>

<p>Note however, as Roger Penrose reminds us, that there are standard interpretations of quantum mechanics in which unitarity is routinely violated (by measurement), and this does not appear to pose any insurmountable difficulties.</p>

<p>In the 1970s, Stephen Hawking found that combining quantum field theory and general relativity could result in a loss of unitarity. In particular, black holes emit <a href="https://en.wikipedia.org/wiki/Hawking_radiation" target="_blank"> Hawking radiation</a> which should eventually cause black holes to evaporate. The resulting thermal radiation would not contain any information about the initial state of the black hole. In other words, multiple different initial states could end in the same final state, and therefore a value for the wavefunction at a later time does not uniquely determine its value at all other times. Information has somehow been destroyed.</p>

<p>Thus it appears that physicists have to give up at least one of their cherished principles. Either time evolution in the Schrodinger equation is not unitary, or general relativity is wrong, or some other aspect of quantum field theory is wrong.</p>

<p>It has been suggested the ECSK theory could resolve the black hole information paradox by avoiding the gravitational singularity in black holes. Then every black hole is actually a one-way membrane leading to a new universe that retains all the information from the initial state.</p>

<h3> Matter-antimatter asymmetry </h3>

<p>In the laws of physics as they are best known today, there is nothing that favours the production of matter over antimatter. On the other hand, the <a href="https://en.wikipedia.org/wiki/Observable_universe" target="_blank"> observable universe</a> is heavily dominated by matter, which immediately raises the question “Why?”.</p>

<p>ECSK theory coupled to fermions generates a term that is cubic in the spinors and thus not invariant under charge conjugation, giving a <a href="https://arxiv.org/abs/1101.4012" target="_blank"> possible explanation for matter-antimatter asymmetry</a>.</p>

<h3> Ultraviolet divergences </h3>

<p>When you naively try to calculate physically measurable quantities in interacting quantum field theories, you tend to get an infinite answer. That’s a problem.</p>

<p>The basic issue is that fields are operator-valued distributions, for which multiplication is not well-defined outside of some trivial cases. If you plough ahead anyway pretending this problem doesn’t exist, aforementioned infinities rear their ugly head. Garbage in, garbage out, as they say.</p>

<p>To deal with this, physicsts play a regularisation game that involves artificially adding terms to the theory to cancel out the infinities, in a process that Richard Feynman famously described as ‘dippy’. This is not mathematically kosher, but it does allow us to extract sensible predictions from the theory, so the technical concerns tend to get brushed under the carpet to a degree.</p>

<p>The infinities that occur are broadly of two kinds: ultraviolet (high energy/short distance) and infrared (low energy/large distance). The infrared divergences are typically regarded as less troubling, and one reason is because they go away if you think the universe is finite because that introduces a maximum possible length scale. The ultravolet divergences tend to cause more consternation because there is less sympathy for the idea of a universal minimum length scale.</p>

<p>It has been suggested by Nikodem Poplawski that Einstein-Cartan theory <a href="https://arxiv.org/abs/1712.09997" target="_blank"> resolves the ultraviolet divergences</a> because torsion causes momentum operators to be non-commutative.</p>

<h3> Ok, if ECSK theory is so awesome, why isn't it more popular? </h3>

<p>Great question, me.</p>

<p>So I should first say that the solutions that ECSK theory proposes to the above problems are mostly only conjecture at the moment. We have suggestive evidence only.</p>

<p>On the other hand, it is a relatively modest generalisation of Einstein’s general relativity that may solve just about every major problem going in physics today. Why isn’t it famous?</p>

<p>One reason is that ECSK theory is more complicated than Einstein’s general relativity. Introducing torsion makes it significantly more difficult to do calculations and extract predictions from the theory. Because there is no currently feasible experiment whose result would distinguish between Einstein’s general relativity and ECSK theory, people have tended to go for the simpler theory.</p>

<p>That said, the additional theoretical complications that ECSK theory brings does not, to me, seem to justify how little attention it receives. It introduces non-linearity in the Dirac equation, but since when did a little non-linearity scare us physicists? General relativity and Quantum Chromodynamics are both non-linear and we’re quite happy with those. On the other hand, there is an entire industry dedicated to the extremely speculative idea of string theory, which is mind-bogglingly complicated and it is not yet clear whether it solves anything despite decades of dedicated work by many very smart people. Surely ECSK theory could use a little love too?</p>

<p>Ultimately I think the disparity has its explanation in historical coincidence and herding behaviour. Which I understand, but I also think we need to #GiveECSKTheoryAChance.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[22 Jul 2022]]></summary></entry><entry><title type="html">On one-use code</title><link href="/2022/06/05/On-one-use-code.html" rel="alternate" type="text/html" title="On one-use code" /><published>2022-06-05T00:00:00+01:00</published><updated>2022-06-05T00:00:00+01:00</updated><id>/2022/06/05/On%20one%20use%20code</id><content type="html" xml:base="/2022/06/05/On-one-use-code.html"><![CDATA[<p>05 Jun 2022</p>

<p>Academics who code are, I think, somewhat known for their less than stellar creations. I have certainly written code that I would prefer didn’t see the light of day. A recent example that attracted a great deal of attention is <a href="https://en.wikipedia.org/wiki/Neil_Ferguson_(epidemiologist)" target="_blank"> Professor Neil Ferguson’s</a> <a href="https://en.wikipedia.org/wiki/CovidSim" target="_blank"> CovidSim</a>. As the name suggests, it is a simulator of COVID-19 transmission that works by creating artificial agents representing people and environments that they interact with, in much the same way as the SimCity series of games but without the funky graphics. It was the basis of a <a href="https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf" target="_blank"> paper</a> that is credited with fundamentally altering the course of the UK’s COVID-19 policy. The paper predicted that even under the optimal ‘mitigation’ strategy that was considered, the peak surge capacity of ICU beds in the UK would be exceeded 8-times over due to the pandemic. As of the date of writing, it has 3,910 citations.</p>

<p>In May 2020, The code behind the simulation was released to GitHub, which is a ubiquitously-used online tool for software development. Before it was released to the public, it apparently consisted of a single 15,000 line file written in C. Having one file of source code that long is already a cardinal sin in coding, and there were many other failures to live up to standard software development practices.</p>

<h3> Ok, but it worked, didn't it? </h3>

<p>An independent research group was able to reproduce the published results of CovidSim by running it themselves. So, while CovidSim may have been less than desirable from a coding point of view, it did its job as intended. Does it really matter that it wasn’t that pretty?</p>

<p>I have a lot of sympathy for the team that worked on this code. Academic coding often doesn’t live up to industry software development standards, for a multitude of reasons. Code that is used to carry out analysis for research is typically ‘one-use’, or close to it. It is not, for example, google.com, which is used by god knows how many millions of people every day and has a large team of people continuously maintaining it. Code written for research is typically intended to be used by a small number of people who authored it, to obtain a specific set of outputs just once.</p>

<p>In addition to that, the PhD students, postdocs and other research staff who typically write the code for such projects are usually themselves not trained in industry standard software development pratices. Many of them will come from disciplines that aren’t primarily coding-centred, and will have to pick it up along the way with little or no oversight. That’s a tough position to be in.</p>

<p>Third, academia is often a race to publish results, that can end up being a very much winner-take-all proposition. Release your paper a week too late and posterity will not look kindly upon you. These are not circumstances that are conducive to producing nice code.</p>

<p>Lastly, an adage comes to mind that goes something like this: “Feel free to break the rules once you know why they exist”. It captures the idea that once you are at a sufficiently high level in a given skill, a lot of what you do consists of knowing when exactly the rules can be bent/broken. Great chess grandmasters, for example, often play with flagrant disregard for well-established principles of the game. They have been at it for long enough that they can often get a competitive edge by going beyond the rules, in a way that is informed by years of study and expertise. Likewise, there are circumstances where it is ok to write fairly ‘terrible’ code, that breaks all the rules - sometimes you just need a result quickly, and you can focus on cleaning it up/optimising later, or not at all. Why, for example, spend hours beautifying a piece of code when you have a deadline looming that depends crucially on the output of said code? Do your due diligence in making sure it is correct, and hit enter.</p>

<p>That said, it is not exactly confidence-inspiring that this simulation that changed the course of a nation was lacking in robustness. One reason we like pretty code is that it tends to minimise the chances of an error. The stakes could not have been higher in this case - an error would have had massive consequences. Also, the fact that the code has been around in one form or another since 2005, and has been adapted repeatedly for modelling various epidemics, makes me a little less sympathetic. That is not exactly what one would call one-use code.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[05 Jun 2022]]></summary></entry><entry><title type="html">The F-twist</title><link href="/2022/05/07/The-F-twist.html" rel="alternate" type="text/html" title="The F-twist" /><published>2022-05-07T00:00:00+01:00</published><updated>2022-05-07T00:00:00+01:00</updated><id>/2022/05/07/The%20F-twist</id><content type="html" xml:base="/2022/05/07/The-F-twist.html"><![CDATA[<p>07 May 2022</p>

<h3> What is the F-twist? </h3>

<p>Milton Friedman was a Nobel prize-winning economist whose ideas became deeply influential both inside and outside academia.
He broadly advocated for free markets with minimal government intervention. For example, he opposed the military draft,
 minimum wage laws and <a href="https://en.wikipedia.org/wiki/Occupational_licensing" target="_blank"> occupational licensing</a>,
and supported <a href="https://en.wikipedia.org/wiki/School_voucher" target="_blank"> school vouchers</a>
and legalisation of drugs and prostitution. He was an advisor to Ronald Reagan
and Margaret Thatcher, and he is widely thought of as the intellectual father of the ideology of 
<a href="https://en.wikipedia.org/wiki/Neoliberalism" target="_blank"> neoliberalism</a>.</p>

<p>In his 1953 book Essays in positive economics, he introduced a principle that has later become known as the “F-twist”, famously 
embodied in the following sentence:</p>

<blockquote>
Truly important and significant hypotheses will be found to have "assumptions" that are wildly inaccurate descriptive 
representations of reality, and, in general, the more significant the theory, the more unrealistic the assumptions (in this sense).
</blockquote>

<p>The name “F-twist” was dubbed by Friedman’s fellow Nobel economist Paul Samuelson, who reportedly chose not to name it after Friedman
directly out of “courtesy”.</p>

<p>It’s a pretty outrageous idea, seemingly suggesting that we should limit our search for new theories to those whose axioms are
“wildly inaccurate”. Nonetheless, it has been widely adopted in mainstream academic economics. That’s a fascinating state of affairs
to me.</p>

<h3> Doesn't it contradict pretty much everything we know about science and the scientific method? </h3>

<p>In a word: yes.</p>

<p>The core of the scientific method is</p>

<ol>
  <li>Generate a hypothesis</li>
  <li>Figure out what observable consequences of the hypothesis are</li>
  <li>Do experiments to see if those observable consequences are in accordance with reality</li>
</ol>

<p>There’s a few thorny issues to take care of there. But the underlying philosophy is that the proper aim of science is to understand/predict
the behaviour of the universe and its various constitutents, and the ultimate arbiter of truth is reality itself - all ideas
must be subject to rigorous empirical testing.</p>

<p>As an example, consider Einstein’s special theory of relativity. It starts with two axioms:</p>

<ol>
  <li>The laws of physics are the same in all inertial frames of reference</li>
  <li>The speed of light in a vacuum is that same for all observers</li>
</ol>

<p>Starting from just this, one can figure out (if you’re a genius) that measurements of length and time are “subjective” - two 
observers with clocks and rulers can disagree on the length of an object, or the number of seconds that elapses between
two events, depending on their state of motion. In particular, an observer travelling at high speed will experience time
passing more slowly, and lengths as shorter, than a stationary observer.</p>

<p>One of the great strengths of this theory is that it made clear, novel predictions, and the axioms can be directly subjected 
to experimental test. As it happened, it passed with flying colours.</p>

<p>Of note here is that the axioms of a theory are trivially also predictions of that theory, that should be tested against reality. 
There is nothing special about the axioms as opposed to observable consequences that are derived from those axioms; they all 
must be scrutinised to determine if they are in concordance with reality.</p>

<p>The extraordinary thing about the F-twist is that it completely upends this bedrock of science. It arbitrarily asks us to give the axioms of 
a theory an easy ride. In fact, further; it asks us to look for axioms that are wildly inconsistent with observed reality, for
this is surely where all the most fruitful and significant ideas lie. If taken seriously, it would revolutionise the 
way we do science. It’s not often in history that paradigm shifts like that occur.</p>

<p>Of course, it needn’t be a revolution that’s actually any good. Imagine physicists and chemists and biologists rejecting new
ideas out of hand because they are simply in excessively good agreement with reality. Everyone knows those theories are a dead-end!</p>

<p>I have to call out the F-twist for what it is: nonsense.</p>

<h3> Oh yeah, if it'so obviously wrong, then why has it been bought wholesale by academic economists? </h3>

<p>Oh that’s easy. It’s because they need to rationalise what they spend their professsional lives doing.</p>

<p>Economists in general are trying to answer extraordinarily complex questions. Questions so difficult that they may be
permanently beyond the ability of humans to answer. The people who end up devoting their professional lives to this pursuit
are inevitably going to be the ones who can convince themselves, by hook or by crook, that what they are doing is useful and important.
 The people who don’t do that drop out of the profession relatively quickly.</p>

<p>What we are left with is a lot of theorising that is ultimately fairly useless. The metric of success is decoupled from 
the normal scientific standards of standing up to empirical scrutiny, and instead focuses on how much other equally 
useless research it inspires/influences. Friedman was quite expicit about this in his essays in positive economics; 
theories are to be judged according to fruitfulness in the precision and scope of their predictions and <i>ability to generate 
additional research lines</i>. Theoretical economics becomes a hierarchy that is predicated more upon ability to convince
one’s colleagues how pleasing one’s ideas are, than ability to understand and predict economic systems.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[07 May 2022]]></summary></entry><entry><title type="html">Model complexity</title><link href="/2022/04/21/Model-complexity.html" rel="alternate" type="text/html" title="Model complexity" /><published>2022-04-21T00:00:00+01:00</published><updated>2022-04-21T00:00:00+01:00</updated><id>/2022/04/21/Model%20complexity</id><content type="html" xml:base="/2022/04/21/Model-complexity.html"><![CDATA[<p>21 Apr 2022</p>

<p>There are models for just about anything you can think of. Fundamental physics, economic markets, epidemics, brain function, climate, click-through rates, animal behavior, election outcomes, to name a few. There’s a universe of possibilities for ways to model any given phenomenon, and only a few small hidden oases in an otherwise barren desert. What’s a model-builder to do?</p>

<h3> How complex should my model be? </h3>

<p>So, a model should certainly not be as “complex” as the pheonomenon that it attempts to describe is, at least at face value. The whole point of a model is to take something that is too complex for our brains to process by brute force, and find some underlying structure that makes it easier for us to understand. ‘The map is not the territory’, as they say.</p>

<p>As an example, <a href="https://en.wikipedia.org/wiki/Standard_Model" target="_blank"> The Standard Model</a> of particle physics contains precisely 19 free parameters that need to be determined by experiment. In exchange for that, we get a description of all known matter, that is accurate all the way down to the subatomic scale. Quite extraordinary.</p>

<p>That still leaves plenty of rope with which to hang oneself though. In particular, I think the academy has a tendency to needlessly over-complicate. For one thing, more free parameters allows more impressive results to be obtained - at least when limiting to the data that the model is trained with. This is known as <a href="https://en.wikipedia.org/wiki/Overfitting" target="_blank"> overfitting</a>, and it’s everywhere. It gives the model-builder more levers and buttons that they can use in order to manufacture the exact results they want. Cranking up model complexity also allows academics to go down an intellectual rabbit hole where they can write lots of papers, as is necessary to advance one’s career. And finally, it allows them to obfuscate their overfitting - the model gets so complex that people mostly can’t be bothered investing weeks/months of their lives in order to figure out the exact trickery that is afoot.</p>

<h3> Model complexity should be proportional to experimental power </h3>

<p>The better the experiments we can do, the more detailed the hypotheses they can distinguish between. So for example, good physics models can be quite complex because we can often do excellent experiments that allow us to declare a clear victor amongst the competitors. We can build big machines like the <a href="https://en.wikipedia.org/wiki/Large_Hadron_Collider" target="_blank"> Large Hadron Collider</a> and smash beams of particles together in precisely controlled ways for years, and see which theory best predicts what comes out the other end.</p>

<p>By way of contrast, in e.g. economics we generally have poor experimental power. We can’t run repeated controlled experiments that involve tweaking the economies of large countries and see what the results are. This is why economic modelling should for the most part be quite humble in its aspirations. It should not, for example, have any illusions about its ability to provide adequate <a href="https://en.wikipedia.org/wiki/Microfoundations" target="_blank"> microfoundations</a> for macroeconomic models. It’s just not going to happen, at least anytime soon.</p>

<h3> Parameters required:Parameters used </h3>

<p>I have a somewhat loosely formulated idea for an initial sweep at sorting the wheat from the chaff.</p>

<p>First, look at the quantities that the model purports to accurately predict. What is the smallest number of parameters that would be required to get a prediction at least as good? For example, if its a macroeconomic model whose only accurate prediction a straight line trend for GDP, this would require precisely two parameters: an intercept and a slope.</p>

<p>Now look at the number of free parameters the model has. If it has at least two, then I suggest the model prima facie has no predictive value beyond a simple line of best fit. It hasn’t simplified anything, and the free parameters  give the model-builder enough space to actually get whatever straight line they wanted.</p>

<p>This is definitely a vague idea at the moment, but maybe it has some legs.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[21 Apr 2022]]></summary></entry><entry><title type="html">God</title><link href="/2022/03/27/God.html" rel="alternate" type="text/html" title="God" /><published>2022-03-27T00:00:00+00:00</published><updated>2022-03-27T00:00:00+00:00</updated><id>/2022/03/27/God</id><content type="html" xml:base="/2022/03/27/God.html"><![CDATA[<p>27 Mar 2022</p>

<p>I have decided to take a philosophical turn with this blog, because why the Dickens not. Philosphers love doing useless crap (just kidding, I heart you guys), so in that spirit I have converged upon the ultimate philosophical waste of time: Does god exist?</p>

<h3> Whence god? </h3>

<p>Right so as a good proto-philsopher, it’s important to start with definitions in order to elucidate the subsequent discussion. What do we mean by ‘god’?</p>

<p>I distinguish between two broad formulations.</p>

<h4> Formulation 1: The man in the sky </h4>

<p>This notion of god would have him as a <em>physical</em> being, residing in a <em>physical</em> place in the universe (in our exemplar, the sky). So basically, a bloke a lot like you or I, except holding some mysterious interest in/sway over mundane human matters.</p>

<p>Now I think we are quite justified in concluding with an <em>extremely high</em> level of confidence that this god does not exist. First of all, you’d think we would have located him by now. And don’t give me that <a href="https://en.wikipedia.org/wiki/Evidence_of_absence" target="_blank"> absence of evidence is not evidence of absence</a> claptrap - if I look extensively in the place where the thing would be if it existed and turn up nothing, then that <em>is</em> evidence that the thing ain’t there. But more than that, this hypothetical being would, according to most ideas about god, have various supernatural powers/properties that are contrary to basically everything that we know and believe about anything. If we ever found him, this lad would have <em>a lot</em> of explaining to do. The physics textbooks would have to be re-written for a start.</p>

<p>Alright so let’s move on to the next door.</p>

<h4> Formulation 2: The whatcha-ma-callit  in the thing-ma-bob </h4>

<p>Admittedly, this one is a bit more abstract. But I think it is the concept of god that most believers have, whether they acknowledge it or not. Namely; god as a mysterious, supernatural and completely intangible entity. There is no experiment that we could do, <em>not even in principle</em>, that would allow us to reasonably adjust our degree of belief in his existence one way or the other. God transcends all that pedestrian stuff.</p>

<p>At this point my reply is pretty simple: I consider this to be the definition of something that does not exist. Like, if someone tells me there’s a gnome on my shoulder, but this gnome does not interact with our universe in any intelligble way and has a presence that is theoretically undetectable, then I am pretty uninterested in talking about the gnome. That is as long as I’m on a fact-finding mission about the universe. This gnome contains no information about the universe, and so frankly can take a running jump. Just another <a href="https://en.wikipedia.org/wiki/Russell%27s_teapot" target="_blank">Russell’s teapot</a>.</p>

<p>I think this is a reasonable definition of what it means for something to not exist, and I’m willing to engage in trial by combat with anyone who thinks otherwise.</p>

<h3> Hang on, but isn't the idea of god interesting/useful? </h3>

<p>For sure. And this is quite separate from the idea of whether god exists. Humans are quite capable of entertaining/believing in all manner of fantastical, self-inconsistent and just plain wrong things.</p>

<p>As a storytelling device god is absolutely unparalleled. I think the most compelling stories that have ever been told have all in one way or another been about god. As an organising idea for human behaviour and by extension a force guiding the course of human history, you’ll struggle to find anything more significant - religious stories have had as deep an impact on humanity as you care to realise. For my money, I think that god and religion provide a sort of familial superstructure to society - in Christianity it is the brotherhood of man under the paternal bond of god. I don’t think we stop having a deep yearning for a father figure as soon as we become adults ourselves. That’s our so-called god-shaped-hole, and it needs filled with something, gosh darnit. Indeed, I am semi-convinced that such superstructures are necessary/responsible for the emergence of humans from the wretched <a href="https://en.wikipedia.org/wiki/State_of_nature" target="_blank">state of nature</a>. And I have had my mind changed in a fairly major way in recent years on the merits of religion and the contributions that it has made to humanity. In short, I think it’s likely a net positive. But that’s a subject for another day.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[27 Mar 2022]]></summary></entry><entry><title type="html">The weakness of the continuum</title><link href="/2022/03/08/The-weakness-of-the-continuum.html" rel="alternate" type="text/html" title="The weakness of the continuum" /><published>2022-03-08T00:00:00+00:00</published><updated>2022-03-08T00:00:00+00:00</updated><id>/2022/03/08/The%20weakness%20of%20the%20continuum</id><content type="html" xml:base="/2022/03/08/The-weakness-of-the-continuum.html"><![CDATA[<p>08 Mar 2022</p>

<p>Ever since Newton/Leibniz invented the infinitesimal calculus in the latter half of the 1600s, it seems like the world has been enthralled by beauty and possibility of the continuum. This event marked the beginning of modern physics and mathematics, and there is virtually no topic in either of these fields that is not deeply interwoven with calculus.</p>

<p>Sometimes, though, I think they may have been a little too successful.</p>

<h3> Before we get started, kindly allow me to explain my super clever title pun </h3>

<p>It turns out that there are different sizes of infinity. So, for example, the entire set of counting numbers \( \{ 1,2,3 …\} \) is smaller than the entire set of real numbers. What this means is that you can pair up every counting number to a real number, and still have lots of real numbers left over. 
Sets that are of the same ‘size’ as the real numbers are sometimes said to have the ‘power of the continuum’ - and this is where we lay the scene of the eponymous wordplay.</p>

<h3> What's weak about the continuum? </h3>

<p>No one can doubt the real numbers are pretty and endlessly fascinating. But have we sometimes been a little too seduced/beguiled by them?</p>

<h4> Exhibit A: Quantum field theory </h4>

<p>Quantum field theory is at the heart of our current best theory of the fundamental workings of the universe, <a href="https://en.wikipedia.org/wiki/Standard_Model" target="_blank">The Standard Model</a>. However, somewhat amusingly, <a href="https://en.wikipedia.org/wiki/Wightman_axioms#Existence_of_theories_which_satisfy_the_axioms" target="_blank">we do not yet know if the standard model ‘exists’ mathematically</a>. When it comes to calculating physical quantities from the standard model, we use various types of fudges/approximations, all of which rely in one way or another on a ‘cutoff’. The cutoff is a distance scale below which, for practical purposes, we declare that we are not interested in what is physically happening - much like when we round off numbers at say, 2 decimal places, because precision beyond that is not useful for the purpose at hand. This cutoff effectively introduces some ‘discreteness’ into the theory, and that allows us to do sensible calculations.</p>

<p>However, the full theory is meant to be set in a continuous spacetime. When you try to define this mathematically, you unfortunately run into all sorts of problems. In particular, the fields are operator-valued distributions, for which multiplication is not well-defined outside of some very restricted subsets. This has the consequence that there are basically no known, interacting quantum field theories that have local degrees of freedom - this is the kind of quantum field theory that is required to describe our universe. Indeed there is a <a href="https://en.wikipedia.org/wiki/Yang%E2%80%93Mills_existence_and_mass_gap" target="_blank">million dollar Millenium prize</a> waiting for anyone who can provide an example. It has also led physicists to spend a huge amount of time and effort on very complex, speculative, and issue-ridden ideas like string theory.</p>

<p>All of this kerfuffle can be avoided if you’re just willing to take the cutoff seriously and put the theory on a discrete spacetime lattice. Everything is mathematically well-defined, and any physical quantity the theory predicts can be calculated to any desired degree of precision by adjusting the cutoff as necessary.</p>

<p>Physicists almost unanimously reject this, though. The continuum is just too pretty to be abandoned, despite all the issues it causes.</p>

<h4> Exhibit B: Financial markets </h4>

<p>Let’s say that asset prices are buffeted by mysterious, difficult-to-comprehend forces that we believe are roughly ‘independent’ from each other at different time steps. Let’s say we also believe that time in financial markets is a continuum. Then the central limit theorem implies that movements in asset prices are normally distributed at all times - it’s inescapable.</p>

<p>Of course, movements in asset prices are known to exhibit ‘fat tails’ - in reality there are far more extreme events than we would expect from models that use normal distributions. Indeed some people have at least partly attributed the 2008 financial crisis, as well as various other financial disasters, to widespread use of modelling strategies in which asset returns are normally distributed.</p>

<p>It is pretty clear that the assumptions above are wrong. An asset price does not undergo an infinite number of random kicks between 09:00:00 AM and 09:00:01 AM on a Monday morning. But the underlying mathematics sure is pretty if you assume it does.</p>

<h3> Conclusion </h3>

<p>I’m sure there are other good examples we could talk about. My point is that I think people often want so much for the maths to be pretty that they get led astray from reality.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[08 Mar 2022]]></summary></entry></feed>