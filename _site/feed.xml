<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-03-27T21:53:57+01:00</updated><id>/feed.xml</id><entry><title type="html">God</title><link href="/2022/03/27/God.html" rel="alternate" type="text/html" title="God" /><published>2022-03-27T00:00:00+00:00</published><updated>2022-03-27T00:00:00+00:00</updated><id>/2022/03/27/God</id><content type="html" xml:base="/2022/03/27/God.html"><![CDATA[<p>27 Mar 2022</p>

<p>I have decided to take a philosophical turn with this blog, because why the Dickens not. Philosphers love doing useless crap (just kidding, I heart you guys), so in that spirit I have converged upon the ultimate philosophical waste of time: Does god exist?</p>

<h3> Whence god? </h3>

<p>Right so as a good proto-philsopher, it’s important to start with definitions in order to elucidate the subsequent discussion. What do we mean by ‘god’?</p>

<p>I distinguish between two broad formulations.</p>

<h4> Formulation 1: The man in the sky </h4>

<p>This notion of god would have him as a <em>physical</em> being, residing in a <em>physical</em> place in the universe (in our exemplar, the sky). So basically, a bloke a lot like you or I, except holding some mysterious interest in/sway over mundane human matters.</p>

<p>Now I think we are quite justified in concluding with an <em>extremely high</em> level of confidence that this god does not exist. First of all, you’d think we would have located him by now. And don’t give me that <a href="https://en.wikipedia.org/wiki/Evidence_of_absence" target="_blank"> absence of evidence is not evidence of absence</a> claptrap - if I look extensively in the place where the thing would be if it existed and turn up nothing, then that <em>is</em> evidence that the thing ain’t there. But more than that, this hypothetical being would, according to most ideas about god, have various supernatural powers/properties that are contrary to basically everything that we know and believe about anything. If we ever found him, this lad would have <em>a lot</em> of explaining to do. The physics textbooks would have to be re-written for a start.</p>

<p>Alright so let’s move on to the next door.</p>

<h4> Formulation 2: The whatcha-ma-callit  in the thing-ma-bob </h4>

<p>Admittedly, this one is a bit more abstract. But I think it is the concept of god that most believers have, whether they acknowledge it or not. Namely; god as a myserious, supernatural and completely intangible entity. There is no experiment that we could do, <em>not even in principle</em>, that would allow us to reasonably adjust our degree of belief in his existence one way or the other. God transcends all that pedestrian stuff.</p>

<p>At this point my reply is pretty simple: I consider this to be the definition of something that does not exist. Like, if someone tells me there’s a gnome on my shoulder, but this gnome does not interact with our universe in any intelligble way and has a presence that is theoretically undetectable, then I am pretty uninterested in talking about the gnome. That is as long as I’m on a fact-finding mission about the universe. This gnome contains no information about the universe, and so frankly can take a running jump. Just another <a href="https://en.wikipedia.org/wiki/Russell%27s_teapot" target="_blank">Russell’s teapot</a>.</p>

<p>I think this is a reasonable definition of what it means for something to not exist, and I’m willing to engage in trial by combat with anyone who thinks otherwise.</p>

<h3> Hang on, but isn't the idea of god intersting/useful </h3>

<p>For sure. And this is quite separate from the idea of whether god exists. Humans are quite capable of entertaining/believing in all manner of fantastical, self-inconsistent and just plain wrong things.</p>

<p>As a storytelling device god is absolutely unparalleled. I think the most compelling stories that have ever been told have all in one way or another been about god. As an organising idea for human behaviour and by extension a force guiding the course of human history, you’ll struggle to find anything more significant - religious stories have had as deep an impact on humanity as you care to realise. For my money, I think that god and religion provide a sort of familial <a href="https://en.wikipedia.org/wiki/Superstructure" target="_blank">superstrucutre</a> to society - in Christianity it is the brotherhood of man under the paternal bond of god. I don’t think we stop having a deep yearning for a father figure as soon as we become adults ourselves. That’s our so-called god-shaped-hole, and it needs filled with something, gosh darnit. Indeed, I am semi-convinced that such superstrucutres are necessary/responsible for the emergence of humans from the wretched <a href="https://en.wikipedia.org/wiki/State_of_nature" target="_blank">state of nature</a>. And I have had my mind changed in a fairly major way in recent years on the merits of religion and the contributions that it has made to humanity. In short, I think it’s likely a net positive. But that’s a subject for another day.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[27 Mar 2022]]></summary></entry><entry><title type="html">The weakness of the continuum</title><link href="/2022/03/08/The-weakness-of-the-continuum.html" rel="alternate" type="text/html" title="The weakness of the continuum" /><published>2022-03-08T00:00:00+00:00</published><updated>2022-03-08T00:00:00+00:00</updated><id>/2022/03/08/The%20weakness%20of%20the%20continuum</id><content type="html" xml:base="/2022/03/08/The-weakness-of-the-continuum.html"><![CDATA[<p>08 Mar 2022</p>

<p>Ever since Newton/Leibniz invented the infinitesimal calculus in the latter half of the 1600s, it seems like the world has been enthralled by beauty and possibility of the continuum. This event marked the beginning of modern physics and mathematics, and there is virtually no topic in either of these fields that is not deeply interwoven with calculus.</p>

<p>Sometimes, though, I think they may have been a little too successful.</p>

<h3> Before we get started, kindly allow me to explain my super clever title pun </h3>

<p>It turns out that there are different sizes of infinity. So, for example, the entire set of counting numbers \( \{ 1,2,3 …\} \) is smaller than the entire set of real numbers. What this means is that you can pair up every counting number to a real number, and still have lots of real numbers left over. 
Sets that are of the same ‘size’ as the real numbers are sometimes said to have the ‘power of the continuum’ - and this is where we lay the scene of the eponymous wordplay.</p>

<h3> What's weak about the continuum? </h3>

<p>No one can doubt the real numbers are pretty and endlessly fascinating. But have we sometimes been a little too seduced/beguiled by them?</p>

<h4> Exhibit A: Quantum field theory </h4>

<p>Quantum field theory is at the heart of our current best theory of the fundamental workings of the universe, <a href="https://en.wikipedia.org/wiki/Standard_Model" target="_blank">The Standard Model</a>. However, somewhat amusingly, <a href="https://en.wikipedia.org/wiki/Wightman_axioms#Existence_of_theories_which_satisfy_the_axioms" target="_blank">we do not yet know if the standard model ‘exists’ mathematically</a>. When it comes to calculating physical quantities from the standard model, we use various types of fudges/approximations, all of which rely in one way or another on a ‘cutoff’. The cutoff is a distance scale below which, for practical purposes, we declare that we are not interested in what is physically happening - much like when we round off numbers at say, 2 decimal places, because precision beyond that is not useful for the purpose at hand. This cutoff effectively introduces some ‘discreteness’ into the theory, and that allows us to do sensible calculations.</p>

<p>However, the full theory is meant to be set in a continuous spacetime. When you try to define this mathematically, you unfortunately run into all sorts of problems. In particular, the fields are operator-valued distributions, for which multiplication is not well-defined outside of some very restricted subsets. This has the consequence that there are basically no known, interacting quantum field theories that have local degrees of freedom - this is the kind of quantum field theory that is required to describe our universe. Indeed there is a <a href="https://en.wikipedia.org/wiki/Yang%E2%80%93Mills_existence_and_mass_gap" target="_blank">million dollar Millenium prize</a> waiting for anyone who can provide an example. It has also led physicists to spend a huge amount of time and effort on very complex, speculative, and issue-ridden ideas like string theory.</p>

<p>All of this kerfuffle can be avoided if you’re just willing to take the cutoff seriously and put the theory on a discrete spacetime lattice. Everything is mathematically well-defined, and any physical quantity the theory predicts can be calculated to any desired degree of precision by adjusting the cutoff as necessary.</p>

<p>Physicists almost unanimously reject this, though. The continuum is just too pretty to be abandoned, despite all the issues it causes.</p>

<h4> Exhibit B: Financial markets </h4>

<p>Let’s say that asset prices are buffeted by mysterious, difficult-to-comprehend forces that we believe are roughly ‘independent’ from each other at different time steps. Let’s say we also believe that time in financial markets is a continuum. Then the central limit theorem implies that movements in asset prices are normally distributed at all times - it’s inescapable.</p>

<p>Of course, movements in asset prices are known to exhibit ‘fat tails’ - in reality there are far more extreme events than we would expect from models that use normal distributions. Indeed some people have at least partly attributed the 2008 financial crisis, as well as various other financial disasters, to widespread use of modelling strategies in which asset returns are normally distributed.</p>

<p>It is pretty clear that the assumptions above are wrong. An asset price does not undergo an infinite number of random kicks between 09:00:00 AM and 09:00:01 AM on a Monday morning. But the underlying mathematics sure is pretty if you assume it does.</p>

<h3> Conclusion </h3>

<p>I’m sure there are other good examples we could talk about. My point is that I think people often want so much for the maths to be pretty that they get led astray from reality.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[08 Mar 2022]]></summary></entry><entry><title type="html">Vaccines and blood clots</title><link href="/2022/02/27/Vaccines-and-blood-clots.html" rel="alternate" type="text/html" title="Vaccines and blood clots" /><published>2022-02-27T00:00:00+00:00</published><updated>2022-02-27T00:00:00+00:00</updated><id>/2022/02/27/Vaccines%20and%20blood%20clots</id><content type="html" xml:base="/2022/02/27/Vaccines-and-blood-clots.html"><![CDATA[<p>27 Feb 2022</p>

<p>On February 22nd, PLOS medicine published a <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003927" target="_blank">paper</a> on cerebral venous sinus thrombosis (CVST) following vaccination on which I was the lead author. CVST is a type of blood clot in the brain that can lead to stroke and death. In this paper, we carried out a pooled analyis of 11.6 million people in England, Scotland and Wales, comparing the rate of CVST events in individuals before and after receiving the vaccine. We found that in the 4 weeks following vaccination with Oxford-AstraZeneca, the risk of CVST events approximately doubled. We did not see any increase in risk of CVST events following Pfizer.</p>

<p>This paper was released at the same time as a <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003926" target="_blank">second paper</a> headed by William Whiteley, also at the University of Edinburgh, that studied 46 million people in England and had strikingly similar findings. It is reassuring when independent research efforts come to similar conclusions.</p>

<p>CVST is a very rare event, occurring at a rate of perhaps 3-4 per million people per year. Assuming this as a background rate, our findings would imply one additional CVST event for every 4 million doses of Oxford-AstraZeneca administered. This has to be weighed up against the benefits of vaccination. Due to concerns over safety, Oxford-AstraZeneca was suspended in those aged under 40, and was not routinely used for booster doses in the UK. The risk/benefit analysis for whom should be vaccinated, with which vaccine, and when, is extremely complicated and depends on background rates of infection, availability of vaccines, vaccine waning, characteristics of future variants and robustness of naturally-acquired immunity versus vaccine-induced immunity, among others.</p>

<p>Our paper included a small methodological novelty that allowed us to pool data across several nations without individual-level data having to be shared between them. Each country stores data in a trusted resesarch environment (TRE), which is just a highly secure server that contains anonymised data. Sharing individual-level data is forbidden, but we worked around this by carrying out a Poisson regression in which only count data is required. We are aiming to use a similar method to carry out further pooled studies in future.</p>

<p>PLOS medicine published an author interview with me, which can be found <a href="https://speakingofmedicine.plos.org/2022/02/22/plos-medicine-author-interview-steven-kerr-phd/" target="_blank">here</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[27 Feb 2022]]></summary></entry><entry><title type="html">Lockdowns and mortality</title><link href="/2022/02/09/Lockdowns-and-mortality.html" rel="alternate" type="text/html" title="Lockdowns and mortality" /><published>2022-02-09T00:00:00+00:00</published><updated>2022-02-09T00:00:00+00:00</updated><id>/2022/02/09/Lockdowns%20and%20mortality</id><content type="html" xml:base="/2022/02/09/Lockdowns-and-mortality.html"><![CDATA[<p>09 Feb 2022</p>

<p>A <a href="https://sites.krieger.jhu.edu/iae/files/2022/01/A-Literature-Review-and-Meta-Analysis-of-the-Effects-of-Lockdowns-on-COVID-19-Mortality.pdf" target="_blank">paper</a> has recently come out claiming that lockdowns had minimal effects on COVID mortality, causing quite a stir. The headline figure is that in Europe and the USA, lockdowns reduced mortality by 0.2%. Fifty words in and the authors are already dropping bombs like this:</p>

<p><em>“While this meta-analysis concludes that lockdowns have had little to no public health effects,
they have imposed enormous economic and social costs where they have been adopted. In
consequence, lockdown policies are ill-founded and should be rejected as a pandemic policy
instrument.”</em></p>

<p>You’ve gotta admire that moxie.</p>

<p>Here’s my review of this paper.</p>

<h3>  What did they do? </h3>

<p>The authors have done a meta-analysis of studies seeking to establish the effect that non-pharmaceutical interventions (NPIs, referred to loosely as `lockdowns’ by the authors) have on COVID mortality.</p>

<ul>
  <li>
    <p>Their initial search turned up 18,590 papers.</p>
  </li>
  <li>
    <p>1,048 remained after screening based on the title of the paper.</p>
  </li>
  <li>
    <p>117 remained after excluding studies that were not empirical, or did not study COVID mortality.</p>
  </li>
  <li>
    <p>34 remained after filtering according to their eligibility criteria.</p>
  </li>
</ul>

<h3>  What eligibility criteria? </h3>

<p>Great question. Most of the substantive criticisms of this paper focus on whether their selection conditions were reasonable. The authors used the following criteria:</p>

<ul>
  <li>
    <p><strong>Studies that used simulations to predict mortality in the counterfactual were excluded</strong> <br />
Counterfactuals are scenarios that didn’t happen, that one typically uses as a basis for comparison. In this context, the counterfactual is what would have unfolded without lockdowns. The authors only admit studies that used real data on mortality from countries that implemented different policies, rather than simulated data. <br /><br />I find myself in agreement with this as an exclusion criterion, given how stunningly bad various predictions for the epidemic have been, and how much freedom simulations give researchers to get the answer that they want.</p>
  </li>
  <li>
    <p><strong>Studies that used synthetic controls were excluded</strong> <br />
Synthetic control methods work as follows. Ideally, what one would like to do is to take two identical populations, existing in identical circumstances, apply a lockdown to one (the ‘treatment’ group) and not the other (the control group), and then measure the difference in mortality. Any difference can then unambiguously be attributed to lockdown, since everything else is identical.<br /><br />However, that’s obviously not practically feasible. Synthetic control studies attempt to get around this by artifiically creating a control group. So, perhaps there exists a population that were not subjected to lockdown that might be used as a control group, except they are quite different to the treatment group. Synthetic control studies statistically re-weight this population so that they look more like the treatment group, and then examine the differences in mortality compared to the treatment group <br /><br />While there is definitely a lot of room for this technique to be abused, I think the authors were perhaps a little too trigger-happy using it as grounds for exclusion.</p>
  </li>
  <li>
    <p><strong>Interrupted time-series studies were excluded</strong> <br />
Interrupted time series studies are fairly straightforward, and in this context consist of fitting a curve to deaths both before and after the imposition of lockdown. One hopes that all other variables that might affect COVID mortality do not change much during the study period, so that causality can be assigned to the lockdown.<br /><br />I think the authors were on solid ground excluding studies of this type. There are typically just too many other variables that are changing in the study period to sensibly assign causation.</p>
  </li>
  <li>
    <p><strong>Both published and working papers were included</strong> <br />
This means that the paper did not have to be published in a peer-reviewed journal to make the final cut.<br /><br />I think this is fair enough. In my experience, peer-review rarely results in major changes to the results of a paper, and there are many high quality papers that are yet to be published.</p>
  </li>
  <li>
    <p><strong>Papers that focus on comparing imposition of lockdowns at different times excluded</strong> <br />
The main rationale behind this is that the authors were seeking to compare the lockdowns that were actually imposed with a counterfactual in which the lockdowns were not imposed, not a counterfactual in which they were imposed at a different time.<br /><br />You might say, however, that it is still worth doing the latter comparison because maybe it will tell us whether lockdowns are worthwhile if we were just able to get the timing right. However, I have pretty much zero confidence in our ability to precisely time the imposition of lockdowns, and I think I the worldwide track record in the last two years backs me up on that.</p>
  </li>
</ul>

<h3>  Quality measures </h3>
<p>Ok, so that’s how the authors narrowed it down to their chosen 34. That’s not the end of the story though. They also rated each of these studies according to the following four measures of quality:</p>

<ul>
  <li>
    <p><strong>Published versus working papers</strong> <br />
You didn’t think they were going to ignore peer-review completely, did you? <br /><br />I do think it is reasonable to put more trust in papers that have been peer-reviewed. So no qualms with this one.</p>
  </li>
  <li>
    <p><strong>Long verus short term</strong> <br />
Papers that have data that extends beyond 31st May 2021 get an extra merit point from the authors. The rationale here is that if lockdowns ‘flattern the curve’ but do not prevent deaths, then studies that are cut short may conclude that lockdowns are effective against death when in fact all they may be doing is slightly prolonging death.<br /><br />I also think this is reasonable.</p>
  </li>
  <li>
    <p><strong>Studies that have an effect on mortality sooner than 14 days after the intervention</strong> <br />
The idea here is that lockdowns presumably affect mortality by curbing transmission. Since it typically takes at least a few weeks between infection and death for those who die from COVID, any study that sees an early effect on mortality is likely to be flawed.<br /><br />I’m on board with this.</p>
  </li>
  <li>
    <p><strong>Social sciences versus other sciences</strong> <br />
Ok this is where it gets a bit sketchy. The authors’ rationale is that social scientists have greater expertise in evaluating policy interventions compared to those in the natural sciences. What this effectively means is that studies carried out by economists get a merit point, and studies carried out by e.g. epidemiologists don’t.<br /><br />I think this is questionable. While I recognise there certainly are biases in different fields, this just doesnt sit well with me.</p>
  </li>
</ul>

<h3>  The results </h3>
<p>These measures of quality were used to do stratified analyses. So, they carried out their analysis on papers that scored 4 out of 4 on the above criteria, then a separate analysis for the papers that scored at least 3, etc.<br /><br />The authors carry out an inverse-variance weighted meta-analysis, which is just a fancy type of weighted average of the results in the selected papers. It means that studies that were able to more precisely estimate the effect of lockdown on mortality get given more weight. Their main results are as follows:</p>

<ul>
  <li>
    <p><strong>Stringency-based studies</strong> <br />
These are studies that used the <a href="https://ourworldindata.org/" target="_blank">our world in data</a> <a href="https://ourworldindata.org/metrics-explained-covid19-stringency-index" target="_blank">stringency index</a> to measure how strict lockdown measures were. In their meta-analysis of these studies, the authors find that lockdowns resulted in a 0.2% reduction in mortality in Europe and the United States. There were only seven papers in this analysis, and one of them receives almost all the weighting.</p>
  </li>
  <li>
    <p><strong>Shelter-in-place-order studies</strong> <br />
The authors found that shelter-in-place orders reduced mortality by 2.9% in Europe and the United States. Again most of the weight goes to the result from one paper.</p>
  </li>
  <li>
    <p><strong>Specific NPIs</strong> <br />
Amongst papers that scored a perfect 4 on the quality criteria, the authors found a 34% reduction in mortality for the use of face masks, and 2.9% reduction for business closures. Once again, most of weight appears to come from single studies.</p>
  </li>
</ul>

<h3>  The verdict </h3>

<p>Once can find a selection of papers in the literature on lockdowns that support pretty much any inclusion one wishes to come to. Thus most criticism of this meta-analysis has rightly focused on whether the selection criteria employed by the authors are reasonable.</p>

<p>I think they are mostly kosher. The one that I take greatest issue with is the exclusion of papers written by people from the natural sciences. It is a little difficult though, because I do think there is a prevailing orthodoxy amongst epidemiologists that errs strongly on the side of interventions that seek to prevent deaths whose proximate cause is COVID, while neglecting any other side effects those interventions may have, no matter how catastrophic. Economists, on the other hand, tend to orient themselves towards broader, utilitarian measures of societal welfare.</p>

<p>At the intutitive level, I would not be surprised if lockdowns had minimal effect on COVID mortality. The mechanism through which they were meant to work was ‘flattening the curve’, thus preventing healthcare services from being overwhelmed. That didn’t happen, and looking at other countries that didn’t have as stringent lockdowns, it seems like we wouldn’t even have come particularly close.</p>

<p>Another interesting point that the furore over this paper illustrates is the huge schism between economists and public health professionals. It’s quite amazing how two well-developed wings of the academy can be so deeply in disagreement over some rather fundamental items. It’s reminiscent of the chasm that exists between biologists and the humanities on ideas like the <a href="https://en.wikipedia.org/wiki/Tabula_rasa#Science" target="_blank">tabula rasa</a> theory of human nature.</p>

<p>One thing is for sure; if the economists were in charge of the governmental response to the pandemic, things would have looked <em>a lot</em> different.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[09 Feb 2022]]></summary></entry><entry><title type="html">What is vaccine effectiveness?</title><link href="/2022/01/17/What-is-vaccine-effectiveness.html" rel="alternate" type="text/html" title="What is vaccine effectiveness?" /><published>2022-01-17T00:00:00+00:00</published><updated>2022-01-17T00:00:00+00:00</updated><id>/2022/01/17/What%20is%20vaccine%20effectiveness</id><content type="html" xml:base="/2022/01/17/What-is-vaccine-effectiveness.html"><![CDATA[<p>17 Jan 2022</p>

<p>There have been interesting reports from Public Health Scotland that COVID infection rates amongst the unvaccinated have been consistently lower compared to those who have had one or two doses of vaccine since early December 2021, though higher compared to those with a booster dose. See a news story <a href="https://www.heraldscotland.com/news/19843315.covid-scotland-case-rates-lowest-unvaccinated-double-jabbed-elderly-drive-rise-hospital-admissions/" target="_blank">here</a>, based on the original report <a href="https://publichealthscotland.scot/media/11089/22-01-12-covid19-winter_publication_report.pdf" target="_blank">here</a>. I believe this has led many people to question vaccine effectiveness.</p>

<h3>  What is vaccine effectiveness? </h3>

<p>If you’re maths averse, you might want to scroll straight down to the “What does it mean?” section. It’s not that bad though, I promise.</p>

<p>There are many quantities that get called “vaccine effectiveness” that are mathematically and conceptually distinct. This is a little bit unfortunate, because it makes it diffucult to compare them. Throughout this article, I will assume we are talking about vaccine effectiveness against infection, but we may also be interested in effectiveness against hospitalistion, death and other outcomes.</p>

<h4>  Risk ratio </h4>

<p>Risk ratios are perhaps the most intuitive way  of defining vaccine effectiveness. It really is just the probability of a vaccinated person being infected with COVID, divided by the probability of an unvaccinated person being infected with COVID. A risk ratio less than one indicates some vaccine effectiveness against infection. This motivates our first definition of vaccine effectiveness,</p>

\[\mathrm{VE} = 1-  \mathrm{Risk \; ratio}.\]

<h4>  Rate ratio </h4>

<p>The rate at which vaccinated/unvaccinated people get infected can be estimated using <a href="https://en.wikipedia.org/wiki/Poisson_regression" target="_blank">Poisson regression</a>. Dividing the rate for vaccinated people by the rate for unvaccinated people gives the rate ratio, which leads to our second definition of vaccine effectiveness,</p>

\[\mathrm{VE} = 1-  \mathrm{Rate \; ratio}.\]

<p>Rate ratios and risk ratios should be approximately equivalent over the same time horizon.</p>

<h4>  Hazard ratio </h4>

<p>The hazard ratio is similar to the rate ratio, except that it takes into account the number of people who are susceptible to an event at any given time. So the rate ratio just compares the event rate in all the vaccinated and unvaccinated, whereas the hazard ratio compares the event rate amongst only those who are susceptible to an event in the vaccinated and unvaccinated. A key difference is that, for example, people who die during the observation period are removed from the calculation of hazard ratios, but not rate ratios. Hazard ratios can be estimated using <a href="https://en.wikipedia.org/wiki/Proportional_hazards_model" target="_blank">proportional hazards models</a>. This gives our third definition of vaccine effectiveness,</p>

\[\mathrm{VE} = 1-  \mathrm{Hazard \; ratio}.\]

<p>Hazard ratios and risk ratios should be aproximately equivalent if there aren’t many people who cease being susceptible to an event in the observation period.</p>

<h4>  Odds ratio </h4>

<p>The odds of an event is the probability of it occurring \( p \), divided by the probability of it not occurring \(1- p \),</p>

\[\mathrm{Odds} = \frac{p}{1-p}.\]

<p>An odds ratio is dividing the odds of one event, by the odds of a second event,</p>

\[\mathrm{OR} = \frac{ \mathrm{Odds}_1 }{ \mathrm{Odds}_2 }.\]

<p>So, we might divide the odds of being infected with COVID if you have recived the vaccine, by the odds of being infected with COVID if you haven’t received the vaccine. If this quantity is less than one, it indicates that those who received the vaccine are less likely to be infected than those who didn’t. Odds ratios can be calculated using <a href="https://en.wikipedia.org/wiki/Logistic_regression" target="_blank">logistic regression</a>. This gives our fourth definition of vaccine effectiveness,</p>

\[\mathrm{VE} = 1- \mathrm{OR}.\]

<p>Odds ratios are approximately equal to risk ratios for low probability events. This follows from the first order Taylor expansion of the odds,</p>

\[\mathrm{Odds} = \frac{p}{1-p} \simeq p.\]

<h3>  Why so many different measures of vaccine effectiveness? </h3>

<p>At first glance, it might seem odd that we use so many different measures of vaccine effectivenss, especially given that under many circumstances they are approximately equal. Why not just one? The answer that there are a number of statistical models/techniques that can be used to try and estimate vaccine effectiveness, and they each have various merits depending on the assumptions one thinks are reasonable to make, and the data that one has available.</p>

<h3>  What does it mean? </h3>

<p>That’s a fine question. I believe that many people, when reading a headline of e.g. 90% effectiveness, will take that to mean something like “there is a 90% chance the vaccine will prevent me from getting COVID”. Which is of course, not at all what it means.</p>

<p>A more sophisticated take would be “The vaccine will reduce my chances of catching COVID by a factor of 20”. This is better, but still not quite right. The main issue is that it lacks a time horizon. What period of time does this statement apply to?</p>

<p>News reports on studies have vaccine effectiveness have tended to report peak effectiveness over a certain unit of time. For example, in our <a href="https://doi.org/10.1016/S0140-6736(21)00677-2 " target="_blank">study of vaccine effectiveness</a>, we calculated vaccine effectiveness by week, and the media widely reported the maximum value. This can cause confusion.</p>

<h3>  An example </h3>

<p>Let’s assume that the probability of an unvaccinated person being infected with COVID in a given week is 0.1 and constant. Let’s assume vaccine effectiveness stays at roughly the peak level that we found in our paper of 90%.  What is the probability of being infected with COVID relative to an unvaccinated person over, say, a 12 week period?</p>

<p>This is like flipping a biased coin each week, and calculating the probability of flipping 12 “not infected” in a row. Doing this calculation (I’ll spare you the details), you get a reduction in risk of \( 84 \% \).</p>

<p>Pretty good. However, we know that COVID vaccines wane in effectiveness relatively quickly. What happens if we take that into account?</p>

<p>Let’s assume that vaccine effectiveness starts out at 90% and decreases by 5% each week (this is a decent approximation to what we actually see with COVID vaccines). Then you get a reduction in risk of \(49 \% \). Doing the same calculation over 16 weeks, you get a reduction in risk of \(33 \% \).</p>

<p>This is very much a ‘back of the envelope’ calculation, and does not take account of the fact that background infection rates are dynamic and dependent upon levels of immunity amongst the population. Nonetheless I do think it hints at some interesting questions about what the proper aim of vaccination is in the midst of a pandemic.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[17 Jan 2022]]></summary></entry><entry><title type="html">The efficient market hypothesis</title><link href="/2022/01/06/The-efficient-market-hypothesis.html" rel="alternate" type="text/html" title="The efficient market hypothesis" /><published>2022-01-06T00:00:00+00:00</published><updated>2022-01-06T00:00:00+00:00</updated><id>/2022/01/06/The%20efficient%20market%20hypothesis</id><content type="html" xml:base="/2022/01/06/The-efficient-market-hypothesis.html"><![CDATA[<p>06 Jan 2022</p>

<h3>  What is the efficient market hypothesis? </h3>

<p>The efficient market hypothesis is, roughly speaking, the idea that asset prices ‘reflect all available information’. This means that the only way to consistently outperform the market is to have access to information that isn’t widely known, or to get lucky.</p>

<p>In particular, it implies that even the fanciest of funds, managed by very smart people with fancy degrees from even fancier universities, will on average fare no better than a monkey throwing darts at a list of stocks.</p>

<h3>  Is it true? </h3>

<p>There’s definitely some interesting evidence in its favour. Burton Makiel’s 1973 book <em>A random walk down Wall street</em> gives a very nice, readable account of the theory and supporting evidence. It appears that a lot of professionally managed funds would have been about as well off as if they had gone for the dart-throwing-monkey strategy.</p>

<p>On the other hand, there does seem to be some interesting counterpoints. Warren Buffet, for example, is a legendary investor who subscribes to <a href="https://en.wikipedia.org/wiki/Value_investing" target="_blank">value investing</a>, which probably should not consistently result in returns greater than the market average if the efficient market hypothesis is true. So, maybe he got lucky? Or maybe he was reliably producing information that was not widely known?</p>

<h3>  Is it falsifiable? </h3>

<p>That brings us to the next question. Isn’t it the case that any set of returns to any set of strategies can be viewed as consistent with the efficient market hypothesis, if we just say the unusually high returns were either due to luck, or we take a suitable definition of ‘all available information’?</p>

<p>On the former point, in principle it should be possible to test the theory by figuring out the pattern of overwhelming success stories/unmitigated failures that is likely to be seen assuming some form of the efficient market hypothesis is true, and compare that to what actually happens. On the latter point, I do believe there are limits on how much one can bend the definition of ‘all available information’ before it starts to look like a nonsense.</p>

<p>If we really wanted to, we could do a controlled experiment with the dart-throwing-monkies in one group, and the maths PhDs/professional fund managers in the other, give them access to only publicly available information, and observe over time so see how they get along. That might be very impractical, but it’s definitely within the realm of possibility.</p>

<h3>  Is it tautologous? </h3>

<p>It’s kind of amusing that there is a theory which a large contingent of people consider unfalsifiable, while simultaneously another large contingent of people consider it tautologous. Like, what’s it going to be guys? Is it \(1 + 1 = 2\), or is it the existence of god?</p>

<p>Anyway, I get ahead of myself. There are people who consider the efficient market hypothesis to be tautologous because they believe it amounts to the statement that “on average, the returns that investors get is equal to the average of investor returns”.</p>

<p>I definitely get what they’re saying. I just don’t think that’s all the efficient market hypothesis amounts to. It is not just a statement about the mean of returns; it is a statement about the whole distribution of returns. For example, if it really was possible to consistently beat the market using publicly availalable information, then you would expect certain individuals who have figured out how to do this to <em> consistently</em> outperform the market, in a way that is unlikely to be a fluke, instead of having their performance randomly drawn from the same hat as everyone else and fluctuating completely unpredictably. This is not a trivial statement.</p>

<h3>  Is it hilarious? </h3>

<p>Definitely. Some people have set up comedy investment strategies to illustrate the point. For example, <a href="https://www.bbc.co.uk/news/technology-58707641" target="_blank">Mr Goxx, the crypto trading hamster</a>, who beats human investors by using his hamster wheel as a wheel of fortune. Or the <a href="https://www.dailymail.co.uk/sciencetech/article-7922601/Cows-match-performance-human-stock-analysts-Norwegian-experiment.html" target="_blank">cows who chose investments by crapping on a grid of stocks</a>, and in doing so matched the performance of the professional stock analysts they were pitted against.</p>

<h3>  My take </h3>

<p>It is a striking fact that on average, the dart-throwing-monkies do about as well as professionally managed funds. I used to be fond of making the following analogy with boxing: Imagine if we randomly chose people off the street to fight the world heavyweight champion, and about half the time the champ lost. Amazingly, something like that seems to be going on in speculative markets.</p>

<p>This seems to suggest that there isn’t really any skill in investing, but I don’t think that’s true. Clearly it is possible to discern information about an asset that can be used to reasonably update one’s beliefs about in a way that more accurately reflects reality. If a company is haemorrhaging cash and no one is interested in buying their product, it’s probably not wise to go long on it. Ok that’s an extreme example, but I definitely do think there are more subtle indicators that take some work/savvy to understand.</p>

<p>Where the analogy fails is as follows: when I perform well at boxing, this does not make anyone else better at boxing. This is not true in the world of speculative investment. If the market participants use their information to buy underpriced assets and sell overpriced ones, they move the price closer to what it ‘should’ be, which in turn ensures that most of the time the dart-throwing-monkies aren’t going to be doing anything particularly egregious or particularly awesome compared to everyone else.</p>

<p>In this sense, the dart-throwing-monkies are essentially free-riding on the genuine work of other people. It’s worth noting that we could get to the point where there are too many free-riders and too few people doing the actual work of acquiring real information. That’s something to bear in mind, especially as index funds and passive investing have had a large surge in popularity in recent years.</p>

<p>On the other hand, I think there is a very hard limit on what it is possible to do. We can glean a little bit of information that can be used to predict prices better, but for the most part it’s just too complex for anyone really to get much of a handle on (except in the case where one has access to some very specific inside information).</p>

<p>I used to find it hard to have much truck with the efficient market hypothesis. Can it really be the case that trillions of dollars of transactions and god knows how many millions of man hours are directed towards activities that are, approximately, a waste of time? When I was a young, bright-eyed lad I wouldn’t have been able to stomach that. A decade or two interlude with human nature has made me change my tune. I mean, imagine for a moment that you’re a high net worth individual, and you have to manage your wealth somehow. Are you going to choose the dart-throwing monkies, or are you going to choose the fancy investment firm that hires all the ivy league maths PhDs? And if you are a maths PhD getting a ridiculous pay check for tinkering with an algorithm all day, are you going to convince yourself that in fact you and your coworkers have the singular ability to beat the market despite all the evidence to the contrary, or are you going to see the world as it truly is?</p>

<p>I more or less believe the efficient market hypothesis to be true. If you want to beat the market, you’re either going to need luck or insider information, and neither are easy to come by.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[06 Jan 2022]]></summary></entry><entry><title type="html">Counting COVID-19 deaths</title><link href="/2022/01/06/Counting-COVID-deaths.html" rel="alternate" type="text/html" title="Counting COVID-19 deaths" /><published>2022-01-06T00:00:00+00:00</published><updated>2022-01-06T00:00:00+00:00</updated><id>/2022/01/06/Counting%20COVID%20deaths</id><content type="html" xml:base="/2022/01/06/Counting-COVID-deaths.html"><![CDATA[<p>06 Jan 2022</p>

<p>There has been a great deal of controversy over the way COVID-19 deaths are calculated. Initially in the UK, the count that was most widely reported consisted of all who died within 60 days of a positive PCR test, or who died more than 60 days after a positive test but had COVID-19 listed as a cause of death on their death certificate.</p>

<p>The main objection to this is that, for example, someone could test positive for COVID-19, recover fully, then die in e.g. a car crash and still be counted as a COVID-19 death.</p>

<h3> What are we trying to do when we count COVID-19 deaths anyway? </h3>

<p>It’s important to be clear what we’re trying to achieve with the COVID-19 death count. I think most people agree that we should be aiming to count the number of deaths caused by COVID-19. But what does that mean exactly? In the Scottish data, the average number of comorbidities in people who died from COVID-19 is upwards of 3. How do we assign causation when there are multiple contributing factors?</p>

<p>Ideally, what we would like to do is a controlled experiment. We would ‘run’ a parallel version of the world that is completely identical, except SARS-CoV-2 is non-existent. And when I say identical, I really mean it. Everyone believes just as much in the reality of SARS-CoV-2 as they do now, people are furloughed from work en masse, governments impose social distancing and lockdown measures etc. Everything the same, it’s just that there ain’t no such thing as SARS-CoV-2.</p>

<p>The point is that we only changed one thing in our experiment. Therefore any difference in outcome can definitively be attributed to that one change.</p>

<p>Of course this wouldn’t really be feasible (not to mention ethical), but at least now we know what we’re aiming for.</p>

<h3> Hasn't someone spent a lot of time thinking about this problem before? </h3>

<p>Yes. There is a large literature on, for example, how to define deaths due to influenza. The proximate cause of death in most people who die as a result of influenza is pneumonia. But they also often have several comorbidities. One might typically see a flu death defined as death after a hospital admission with flu recorded as the reason for admission. However, the question is far from being settled.</p>

<h3> What should we do with COVID-19 then? </h3>

<p>On August 12 2020, <a href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/916035/RA_Technical_Summary_-_PHE_Data_Series_COVID_19_Deaths_20200812.pdf" target="_blank">Public Health England changed their main definition of a COVID-19 death to be a death within 28 days of a PCR positive test.</a> This led to the primary COVID-19 death count dropping by about 13% overnight. Still, it remains possible for car crash victims to be counted as COVID-19 deaths, and there is the unfortunate fact that the rate of COVID-19 deaths increases as we do more testing, even if the rate of deaths actually caused by COVID-19 remains constant.</p>

<p>This is also quite different from how a flu death is usually defined. Perhaps there’s a defensible rationale for that; COVID-19 and flu are different diseases, and there is a world of difference in how we have reacted to them. For example, it might be reasonable not to require a positive PCR test in the definition of a flu death because PCR testing is quite rare in a typical flu season, which would likely lead to a significant under-count. On the other hand, in the UK we have ramped up PCR testing for COVID extraordinarily rapidly, <a href="https://coronavirus.data.gov.uk/details/testing" target="_blank"> from 10s of thousands per day in April 2020, to well over half a million per day at the time of writing</a>. In my eyes, the more pertinent danger with our current definition of a COVID-19 death is over-counting. I think there is a reasonable case for further modifying the defintion of a COVID-19 death, to be death within 28 days of a postive PCR test <strong>and </strong> COVID-19 listed as the main cause of death on the death certificate, or something similar.</p>

<p>I really wouldn’t want to be the PR person who deals with the reaction to that, though.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[06 Jan 2022]]></summary></entry><entry><title type="html">Do we already have the quantum theory of gravity?</title><link href="/2022/01/05/Quantum-gravity.html" rel="alternate" type="text/html" title="Do we already have the quantum theory of gravity?" /><published>2022-01-05T00:00:00+00:00</published><updated>2022-01-05T00:00:00+00:00</updated><id>/2022/01/05/Quantum%20gravity</id><content type="html" xml:base="/2022/01/05/Quantum-gravity.html"><![CDATA[<p>05 Jan 2022</p>

<h3>  The holy grail of theoretical physics </h3>

<p>For many decades now, physicists have been trying to ‘unify’ quantum mechanics and Einstein’s general relativity into a quantum theory of gravity. This has proven to be very difficult, and achieving such a unification is one of the major outstanding problems in theoretical physics. To understand why, let me first give you a whistlestop tour of some topics in physics.</p>

<h3>  The principle of least action </h3>

<p>Classically, modern physics theories tend to be defined in terms of the <a href="https://en.wikipedia.org/wiki/Stationary-action_principles" target="_blank"> principle of least action</a>. What this means is that the laws of nature are derived from the minimisation of a quantity we call the action. It turns out that a wide range of dynamical laws can be derived this way, including for example Newton’s laws.</p>

<p>When various mathematicians/physicists worked all of this out in the 1700s, some were inclined to attach mystical significance to it. After all, if the workings of the entire universe are described by a minimisation principle, then there must be a <em>minimiser</em>, whom some identified as god.</p>

<h3>  Symmetry </h3>

<p>The action is in turn is an integral over spacetime of a quantity called the Lagrangian (or sometimes Lagrangian density),</p>

\[S = \int \mathcal{L} \tag{1}\]

<p>In principle, the Lagrangian can be just about anything we want, and there is a hell of a lot of choice to be had. So how do we pick a Lagrangian that desribes our universe?</p>

<p>Well one thing that helps to narrow down the choice significantly is symmetry, which plays a very important role in modern physics. It turns out that once you know what symmetries the universe has, you are well on your way to having a theory that describes it.</p>

<p>Symmetry is, roughly speaking, when you do something to something else, and the something else has some property that doesn’t change when you do this. As an example, think about coordinate systems. The behaviour of a physical system should not change depending on the system of coordinates we use to describe it. This is a symmetry that puts significant constraints on the space of theories. These sort of considerations were important in leading Einstein to develop special and general relativity, for example.</p>

<h3> The length cutoff </h3>

<p>Ok but we haven’t completely nailed down the Lagrangian yet. In fact, there are typically an infinite number of terms that could appears in the Lagrangian that are consistent with a given set of symmetries. How do we narrow it down further?</p>

<p>If we reckon the Lagrangian is an analytic function of the fields (which I will just call \( \phi \) ), then we can approximate it as accurately as we want with a Taylor series,</p>

\[\mathcal{L} = \alpha_0 + \alpha_1 f_1(\phi) + \alpha_2 f_2(\phi) + ...  \label{lagrangian} \tag{2}\]

<p>The numbers \( \alpha_0, \; \alpha_1, … \) are constants that need to be determined by experiment, and \( f_n(\cdot) \) are some functions.</p>

<p>We can also treat spacetime as if it is discrete. In fact, not only can we, but it seems like it <a href="https://en.wikipedia.org/wiki/Wightman_axioms#Existence_of_theories_which_satisfy_the_axioms" target="_blank">may be forced upon us in order for the theory to be mathematically well-defined</a>. Note that this is not to say that spacetime ‘really is’ discrete; rather treating it as if it is provides an approximation that is good enough for us to calculate whatever quantities we want.</p>

<p>Discrete theories come with a length scale called a cutoff that defines the smallest length it is possible to sensibly ‘talk about’ within the theory. The point here is that we can arrange the above Taylor expansion so that the higher order terms are suppressed by powers of the cutoff. This means that at any desired degree of accuracy, we need only include a finite number of terms in the theory.</p>

<h3>  Why don't quantum mechanics and general relativity play well together? </h3>

<p>Now it seems like we’re all set. We believe the universe has symmetry under certain coordinate transformations, and for any desired degree of accuracy we can do a finite number of experiments to determine the parameters that fully specify the theory. So what’s the problem?</p>

<p>Well, the way I have presented things here is quite contrary to the way things unfolded historically. In the 60s and 70s, there was a belief among many physicists that as the length cutoff goes to zero, it better be the case that only a finite number of terms have a non-zero coefficient in equation \(\ref{lagrangian}\). They were many good reasons to believe this at the time, and many still maintain the belief today. The main argument in favour is that if it weren’t true, then as the cutoff is taken to zero, and barring some ‘special’ circumstances, there would be an infinite number of parameters, each of which has to be determined by experiment. I don’t know about you, but I don’t really have time to do an infinite number of experiments.</p>

<p>When this is the case, the theory is called ‘non-renormalisable’. It turns out that Einstein’s general relativity is non-renormalisable, and this the heart of the problem of ‘unifying’ quantum mechanics and general relativity into a quantum theory of gravity.</p>

<h3>  What's wrong with a never-ending series of increasingly accurate approximations? </h3>

<p>I am, however, not at all convinced that non-renormalisability is that big an issue. We already have a theory that in principle can be used to calculate any quantity we want, to any desired degree of accuracy we want. Am I missing something here?</p>

<p>Fundamentally, I think the desire to have a theory that is determined by a finite set of parameters at all length scales is driven by what the physics community thinks an aesthetically-pleasing theory is. I don’t really think that’s the greatest guide to scientific truth. Physicists are quite fickle in their aesthetic tastes - there was a time when quantum mechanics was considered quite ugly, for example. For what it’s worth, I personally quite enjoy the idea that nature isn’t so simple that it can be described by a finite number of experimentally determinable quantities at all length scales. We have to work harder the more we want to know. There is no cosmic free lunch.</p>

<h3>  Don't expect this point of view to catch on any time soon </h3>

<p>The other problem with this view is that it would kind of leave a whole bunch of high energy theoretical physicists without much to do. If we already have the quantum theory of gravity, then they are out of a job. Thus they collectively have a very strong incentive to convince themselves that non-renormalisability is a big problem, and so of course that’s exactly what they do.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[05 Jan 2022]]></summary></entry><entry><title type="html">Imputation - the respectable face of p-hacking?</title><link href="/2022/01/04/Imputation.html" rel="alternate" type="text/html" title="Imputation - the respectable face of p-hacking?" /><published>2022-01-04T00:00:00+00:00</published><updated>2022-01-04T00:00:00+00:00</updated><id>/2022/01/04/Imputation</id><content type="html" xml:base="/2022/01/04/Imputation.html"><![CDATA[<p>04 Jan 2022</p>

<h3>  What is imputation? </h3>

<p>Imputation is ‘filling in’ missing values in a dataset. So, for example, say I have a dataset consisting of the name, age, sex and height of some individuals as follows:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><u><strong>Name </strong></u></th>
      <th style="text-align: center"><u><strong>Sex </strong></u></th>
      <th style="text-align: center"><u><strong>Age </strong></u></th>
      <th style="text-align: center"><u><strong>Height </strong></u></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Alice</td>
      <td style="text-align: center">Female</td>
      <td style="text-align: center">25</td>
      <td style="text-align: center">165cm</td>
    </tr>
    <tr>
      <td style="text-align: center">Bob</td>
      <td style="text-align: center">Male</td>
      <td style="text-align: center">_</td>
      <td style="text-align: center">180cm</td>
    </tr>
    <tr>
      <td style="text-align: center">Carol</td>
      <td style="text-align: center">_</td>
      <td style="text-align: center">55</td>
      <td style="text-align: center">_</td>
    </tr>
  </tbody>
</table>

<p>Notice however that some of the entries are blanks - their values are not recorded. There are many reasons why this might be the case - maybe the person who collected the data forgot to fill it in, or some information was accidentally deleted, etc.</p>

<p>Imputation consists of filling in the blank spaces, hopefully in a way that is sensible. The main reason to do this is that often when we train a model using data, we can only use rows in the data that are complete, i.e. no blank spaces. It can seem ‘wasteful’ to discard an entire row because it contains a missing value.</p>

<p>Another important reason we may wish to impute is because the values that are missing could be statistically distinct from the values that are not missing. For example, imagine that for whatever reason, younger people are less likely to have their age recorded in the data than older people. Then the missing values are statistically distinct from the non-missing values, and if we do some statistical analysis using this data we risk introducing bias.</p>

<p>This is all well and good as a motivation for imputation, but the waters can quickly get muddy.</p>

<h3> How is imputation done? </h3>

<p>There are a number of methods that are commonly used for imputation:</p>

<ul>
  <li>
    <p><strong>Mean substitution</strong> <br />
Replace missing values with the mean of the non-missing values in that column.</p>
  </li>
  <li>
    <p><strong> Modal substition</strong> <br />
Replace missing values with the mode (most commonly occurring) of the non-mising values in that column.</p>
  </li>
  <li>
    <p><strong>Regression</strong> <br />
Train a model that predicts the missing values from the non-missing values.</p>
  </li>
  <li>
    <p><strong>Imputation by chained equations</strong> <br />
This one is a little more complicated, and is something like applying a regression model iteratively. I won’t go into it in detail here, but <a href="https://stats.stackexchange.com/questions/421545/multiple-imputation-by-chained-equations-mice-explained" target="_blank"> this answer</a> on stack exchange gives a nice, simplified explanation of the procedure.</p>
  </li>
</ul>

<h3> How much imputation is too much? </h3>

<p>Let’s say that now instead of the above dataset, I have a larger dataset that has <em>a lot</em> of missing values. Let’s make it 1 million rows and 20 columns, and there is only one 
non-missing value in each row. Would it be reasonable to impute all of the missing values?</p>

<p>That idea understandably makes many people uneasy. Can we really be confident that our imputed values are sensible, especially since we started out with so little actual information? Surely this cannot always be ‘kosher’.</p>

<p>Taking the rationale behind imputation to its conclusion, we might even impute an arbitrary number of rows for which we have no data at all - effectively magicking new data into existence! If the non-missing values are indeed statistically distinct from the missing values in a way that can be ‘accounted for’ by the non-missing values, then it might be argued that not only is this ‘OK’, it’s thoroughly commendable statistical practice.</p>

<h3> Imputation by chained equations - an example </h3>
<p>Imputation by chained equations has emerged as a sort of ‘best practice’ for imputation in many areas of academia. Here is the result of imputing age in this way on a dummy dataset that I created:</p>

<p><img src="../../../images/age_imputation.jpg" width="600" height="500" class="center" /></p>

<p>Original values are in blue, imputed values in red. This graph shows the distribution of age, so values where the graph is at a peak are more common, and values where the graph dips are less common.</p>

<p>Your first reaction might be “Well there’s a pretty good match between observed values and imputed values. Seems sensible”. And that might be reasonable as a first reaction.</p>

<p>But then if you work with imputation for a little while and start thinking about it a little more, maybe you start to have some doubts.</p>

<h3> Reasons for pause </h3>

<p>Graphs of the type above, showing the distribution of observed and imputed values, are often used as a post-imputation diagnostic tool. We do the imputation, and then use those plots to decide whether the imputation is ‘acceptable’. But how do we tell the difference between ‘acceptable’ and ‘unacceptable’ imputations?</p>

<p>Well, things get a little vague here. My experience of the literature on imputation is that it is a bit cagey on this point. See, the problem is as follows: If the observed and imputed values are obviously statistically distinct, is that because the imputation is bad, or is it because the imputation is doing its job and correctly telling us that the missing values are indeed statistically distinct from the non-missing values?</p>

<h3> How much do you trust the computer? </h3>

<p>In practice, I don’t think statisticians place much trust in imputations that produce values that are clearly statistically distinct from non-missing values in the original dataset. They just don’t really think the computer is <em>that smart</em>, and I’m more or less in agreement. If they put their faith in their computer, they would also have the non-trivial problem of convincing a peer-reviewer why the odd looking imputation results are actually A-OK.</p>

<p>If the imputed values and the observed values are in conflict, then so much the worse for the imputed values, it seems.</p>

<h3> What is the real purpose that imputation serves? </h3>

<p>The algorithm behind imputation by chained equations is sometimes described as containing a pinch of magic. It has a remarkable tendency to produce graphs like the one above, with excellent agreement between the maginal distributions of the observed and imputed values, thus conveniently avoiding the above issues. This is despite the fact that one of the main justifications for imputation is that it is supposed to be used when the non-missing values are statistically distinct from the missing values, and to fill in the missing values appropriately. I have to think that this is not an accident.</p>

<p>Whenever I see humans behaving in ways that appear to contradict their stated principles, I am increasingly inclined to pay little attention to the rationalisations they offer, and instead look at what benefit they derive from the behaviour in question. In this case, the obvious useful purpose imputation by chained equations serves is allowing researchers to jack up their sample size, thus reducing their p-values, narrowing their confidence intervals and increasing their chances of getting a statistically significant, publishable result, without raising the hackles of the reviewers. Normally this sort of thing is called p-hacking.</p>

<h3> What is p-hacking? </h3>

<p>Researchers try to assess how confident they can be that their results are correct and not a ‘fluke’. The way this is typically done is by calculating a p-value, which is essentially the probability of seeing the result the researchers saw assuming their hypothesis is incorrect. We tend to take a low p-value as indicating a low probability that the hypothesis is incorrect. Low p-values are widely coveted by researchers the world over.</p>

<p>p-hacking is a suite of dubious techniques that researchers can use to artifically lower their p-values. While there is a clear personal incentive for them to do this (advancing their career, getting tenured etc), it has the unfortunate effect of <a href="https://journals.plos.org/plosmedicine/article/file?type=printable&amp;id=10.1371/journal.pmed.0020124" target="_blank"> causing the scientific literature to be populated with findings that are false</a>. Then we have the embarrassment that is the <a href="https://en.wikipedia.org/wiki/Replication_crisis" target="_blank"> replication crisis</a> in medicine and the social sciences - a shamefully large fraction of results in the literature fail to be reproduced when someone else repeats the analysis.</p>

<h3> The bottom line </h3>

<p>If I simply took a real dataset and copy-pasted it to create an artifical dataset that was twice the size and with every entry duplicated, I would likely not get very far in publishing the results of any analysis. However, if I instead took a dataset where half the rows have missing values and do an imputation by chained equations, I doubt it would get much critical scrutiny, despite the fact that the end results are probably similar. Imputation by chained equations is complex enough that it is at least within the realm of possibility that it is doing something statistically sound, while not being <em>obviously</em> wrong. I find it hard to understand its primary purpose as anything except putting a respectable face on p-hacking.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[04 Jan 2022]]></summary></entry><entry><title type="html">Vaccine safety webinar with HDRUK</title><link href="/2021/12/13/Vaccine-safety-webinar-with-HDRUK.html" rel="alternate" type="text/html" title="Vaccine safety webinar with HDRUK" /><published>2021-12-13T00:00:00+00:00</published><updated>2021-12-13T00:00:00+00:00</updated><id>/2021/12/13/Vaccine%20safety%20webinar%20with%20HDRUK</id><content type="html" xml:base="/2021/12/13/Vaccine-safety-webinar-with-HDRUK.html"><![CDATA[<p>13 Dec 2021</p>

<p>I was an invited speaker to the HDRUK COVID-19 vaccine safety research webinar that took place on 11th November, 2021. You can see a video of my talk below. In it, I briefly outline the main findings from studies of vaccine safety that I worked on using the <a href="https://www.ed.ac.uk/usher/eave-ii" target="_blank">EAVE II</a> platform, which consists of clinical and demographic information for the entire population of Scotland.</p>

<p>Our main findings were that vaccination with Oxford-AstraZeneca was associated with an elevated risk of cerebral venous sinus thrombosis (a type of blood clot in the brain that can potentially be fatal), and idiopathic thrombocytopenic purpura (low platelet count in blood which can lead to bleeding - sometimes in the brain, which can be very serious).</p>

<p>I also give a quick outline of the main costs and benefits of vaccination, and suggest that given it is likely that SARS-CoV-2 will become endemic, we may have to move away from mass-vaccination towards a strategy that is much smaller in scope and targeted at the elderly and the vulnerable.</p>

<p><br /></p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/gyMNwFQrTOo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>]]></content><author><name></name></author><summary type="html"><![CDATA[13 Dec 2021]]></summary></entry></feed>